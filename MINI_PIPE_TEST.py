import torch
import torch.nn as nn
import time
import psutil
import os
import tracemalloc
from transformers import AutoTokenizer
from modelscope.utils.hub import snapshot_download
from detection.Loader.mymodel_file.gptJ_cloud import gptJ_cloud
from detection.Loader.mymodel_file.gptJ_edge import gptJ_edge
from detection.SVD_model import SVDED_GPTJ_EDGE_Layer
from detection.MINI_PIPE_EVAL import evaluate_minipile_gptj,load_and_tokenize_dataset

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§ç±»ï¼Œç”¨äºè®°å½•CPUæ—¶é—´å’Œå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    def __init__(self):
        self.process = psutil.Process(os.getpid())
        self.reset_stats()
        
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡æ•°æ®"""
        self.cloud_gpu_times = []
        self.edge_cpu_times = []
        self.network_times = []
        self.memory_snapshots = []
        self.token_count = 0
        self.layer_calls = 0
        
        # è¯¦ç»†çš„è®¡æ—¶ç»Ÿè®¡
        self.cloud_total_time = 0.0
        self.edge_total_time = 0.0
        self.network_total_time = 0.0
        
        # å†…å­˜ç»Ÿè®¡
        self.initial_memory = self.get_memory_mb()
        self.peak_memory = self.initial_memory
        
    def get_memory_mb(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        return self.process.memory_info().rss / 1024 / 1024
    
    def get_cpu_percent(self):
        """è·å–CPUä½¿ç”¨ç‡"""
        return self.process.cpu_percent()
    
    def start_memory_tracking(self):
        """å¼€å§‹å†…å­˜è·Ÿè¸ª"""
        tracemalloc.start()
        
    def stop_memory_tracking(self):
        """åœæ­¢å†…å­˜è·Ÿè¸ªå¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯"""
        if tracemalloc.is_tracing():
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            return current / 1024 / 1024, peak / 1024 / 1024  # è½¬æ¢ä¸ºMB
        return 0, 0
    
    def record_cloud_time(self, time_taken):
        """è®°å½•äº‘ç«¯GPUæ—¶é—´"""
        self.cloud_gpu_times.append(time_taken)
        self.cloud_total_time += time_taken
        
    def record_edge_time(self, time_taken):
        """è®°å½•è¾¹ç¼˜CPUæ—¶é—´"""
        self.edge_cpu_times.append(time_taken)
        self.edge_total_time += time_taken
        
    def record_network_time(self, time_taken):
        """è®°å½•ç½‘ç»œä¼ è¾“æ—¶é—´"""
        self.network_times.append(time_taken)
        self.network_total_time += time_taken
        
    def record_memory_snapshot(self, phase=""):
        """è®°å½•å†…å­˜å¿«ç…§"""
        current_memory = self.get_memory_mb()
        self.memory_snapshots.append({
            'phase': phase,
            'memory_mb': current_memory,
            'timestamp': time.time()
        })
        self.peak_memory = max(self.peak_memory, current_memory)
        
    def increment_counters(self):
        """å¢åŠ è®¡æ•°å™¨"""
        self.layer_calls += 1
        
    def increment_token_count(self):
        """å¢åŠ tokenè®¡æ•°"""
        self.token_count += 1
        
    def print_detailed_report(self):
        """æ‰“å°è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print(f"ğŸ” è¯¦ç»†æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print(f"{'='*70}")
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   ğŸ”¢ å¤„ç†çš„Tokenæ•°é‡: {self.token_count}")
        print(f"   ğŸ”¢ æ€»å±‚è°ƒç”¨æ¬¡æ•°: {self.layer_calls}")
        print(f"   ğŸ”¢ å¹³å‡æ¯tokenå±‚è°ƒç”¨: {self.layer_calls/max(1, self.token_count):.1f}")
        
        # æ—¶é—´ç»Ÿè®¡
        print(f"\nâ±ï¸  æ—¶é—´ç»Ÿè®¡ (æ€»è®¡):")
        print(f"   â˜ï¸  GPUäº‘ç«¯æ€»æ—¶é—´: {self.cloud_total_time:.4f}s")
        print(f"   ğŸ–¥ï¸  CPUè¾¹ç¼˜æ€»æ—¶é—´: {self.edge_total_time:.4f}s")
        print(f"   ğŸŒ ç½‘ç»œä¼ è¾“æ€»æ—¶é—´: {self.network_total_time:.4f}s")
        print(f"   ğŸ”„ æ€»å¤„ç†æ—¶é—´: {self.cloud_total_time + self.edge_total_time + self.network_total_time:.4f}s")
        
        # å¹³å‡æ—¶é—´ç»Ÿè®¡
        if self.token_count > 0:
            print(f"\nâ±ï¸  å¹³å‡æ¯Tokenæ—¶é—´:")
            print(f"   â˜ï¸  GPUäº‘ç«¯å¹³å‡: {self.cloud_total_time/self.token_count:.4f}s")
            print(f"   ğŸ–¥ï¸  CPUè¾¹ç¼˜å¹³å‡: {self.edge_total_time/self.token_count:.4f}s")
            print(f"   ğŸŒ ç½‘ç»œä¼ è¾“å¹³å‡: {self.network_total_time/self.token_count:.4f}s")
            print(f"   ğŸ”„ æ€»å¹³å‡: {(self.cloud_total_time + self.edge_total_time + self.network_total_time)/self.token_count:.4f}s")
        
        # å†…å­˜ç»Ÿè®¡
        current_memory = self.get_memory_mb()
        memory_diff = current_memory - self.initial_memory
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨ç»Ÿè®¡:")
        print(f"   ğŸ“ˆ åˆå§‹å†…å­˜: {self.initial_memory:.2f}MB")
        print(f"   ğŸ“Š å½“å‰å†…å­˜: {current_memory:.2f}MB")
        print(f"   ğŸ“ˆ å³°å€¼å†…å­˜: {self.peak_memory:.2f}MB")
        print(f"   ğŸ“Š å†…å­˜å˜åŒ–: {memory_diff:+.2f}MB")
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = self.get_cpu_percent()
        print(f"   ğŸ”¥ å½“å‰CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")
        
        # è·å–å†…å­˜è·Ÿè¸ªä¿¡æ¯
        if hasattr(self, '_tracemalloc_peak'):
            print(f"   ğŸ” å†…å­˜è·Ÿè¸ªå³°å€¼: {self._tracemalloc_peak:.2f}MB")
        
        # æ—¶é—´åˆ†å¸ƒåˆ†æ
        if len(self.cloud_gpu_times) > 0:
            print(f"\nğŸ“ˆ GPUæ—¶é—´åˆ†å¸ƒ:")
            print(f"   æœ€å°: {min(self.cloud_gpu_times):.4f}s")
            print(f"   æœ€å¤§: {max(self.cloud_gpu_times):.4f}s")
            print(f"   å¹³å‡: {sum(self.cloud_gpu_times)/len(self.cloud_gpu_times):.4f}s")
            
        if len(self.edge_cpu_times) > 0:
            print(f"\nğŸ“ˆ CPUæ—¶é—´åˆ†å¸ƒ:")
            print(f"   æœ€å°: {min(self.edge_cpu_times):.4f}s")
            print(f"   æœ€å¤§: {max(self.edge_cpu_times):.4f}s")
            print(f"   å¹³å‡: {sum(self.edge_cpu_times)/len(self.edge_cpu_times):.4f}s")
        
        # æ€§èƒ½æ¯”è¾ƒ
        if self.cloud_total_time > 0 and self.edge_total_time > 0:
            ratio = self.edge_total_time / self.cloud_total_time
            print(f"\nğŸ” æ€§èƒ½æ¯”è¾ƒ:")
            print(f"   CPU/GPUæ—¶é—´æ¯”: {ratio:.2f}x")
            if ratio > 1:
                print(f"   ğŸ’¡ CPUæ¯”GPUæ…¢ {ratio:.1f} å€")
            else:
                print(f"   ğŸ’¡ CPUæ¯”GPUå¿« {1/ratio:.1f} å€")
        
        print(f"{'='*70}")
        
    def print_memory_timeline(self):
        """æ‰“å°å†…å­˜ä½¿ç”¨æ—¶é—´çº¿"""
        if len(self.memory_snapshots) > 0:
            print(f"\nğŸ“Š å†…å­˜ä½¿ç”¨æ—¶é—´çº¿:")
            for i, snapshot in enumerate(self.memory_snapshots):
                print(f"   {i+1}. {snapshot['phase']}: {snapshot['memory_mb']:.2f}MB")
                
    def get_summary_stats(self):
        """è¿”å›æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'cloud_total_time': self.cloud_total_time,
            'edge_total_time': self.edge_total_time,
            'network_total_time': self.network_total_time,
            'token_count': self.token_count,
            'layer_calls': self.layer_calls,
            'memory_usage_mb': self.get_memory_mb(),
            'memory_peak_mb': self.peak_memory,
            'memory_diff_mb': self.get_memory_mb() - self.initial_memory
        }

class SVD_GPTJ_Edge_Model(nn.Module):
    """åŒ…å«æ‰€æœ‰SVDå±‚çš„å®Œæ•´edgeæ¨¡å‹ï¼Œå…¼å®¹åŸå§‹edgeæ¨¡å‹æ¥å£"""
    def __init__(self, original_edge, svd_reduce_rate, device='cpu', svd_device='cpu',No_init=False):
        super().__init__()
        self.device = device
        self.svd_device = svd_device
        self.num_layers = original_edge.num_layers
        self.max_ctx = original_edge.max_ctx
        self.v_cache = [None] * self.num_layers
        
        print(f"ğŸ”„ å¼€å§‹SVDåˆ†è§£å¤„ç†ï¼Œå‹ç¼©ç‡: {svd_reduce_rate}")
        print(f"ğŸ“Š æ€»å…±éœ€è¦å¤„ç† {self.num_layers} å±‚...")
        print(f"âš¡ SVDåˆ†è§£è®¾å¤‡: {svd_device}, è¿è¡Œè®¾å¤‡: {device}")
        
        # ç”¨SVDå‹ç¼©çš„å±‚æ›¿æ¢åŸå§‹edgeå±‚
        self.svd_layers = nn.ModuleList()
        if(not No_init):
            for i in range(self.num_layers):
                print(f"  å¤„ç†ç¬¬ {i+1}/{self.num_layers} å±‚: ", end="")
                original_edge_layer = original_edge.layers[i]
                    # å¥‡æ•°å±‚è·³è¿‡å‹ç¼©
                if isinstance(svd_reduce_rate, list):

                    svd_layer = SVDED_GPTJ_EDGE_Layer(
                        gptj_edge_layer=original_edge_layer,
                        reduce_rate=svd_reduce_rate[i],
                        device=device,
                        svd_device=svd_device
                    )
                else:
                    svd_layer = SVDED_GPTJ_EDGE_Layer(
                        gptj_edge_layer=original_edge_layer,
                        reduce_rate=svd_reduce_rate,
                        device=device,
                        svd_device=svd_device
                    )
                print("è·³è¿‡å‹ç¼© (å¥‡æ•°å±‚)")
                self.svd_layers.append(svd_layer)
        
        print(f"ğŸ‰ æ‰€æœ‰å±‚çš„SVDåˆ†è§£å¤„ç†å®Œæˆï¼")
    
    def forward_no_cache(self,x,layer_idx,attn_weights):
        output=self.svd_layers[layer_idx].forward_no_cache(
            x,  attn_weights
        )
        return output
    
    def forward_cache(self, x, layer_idx, attn_weights):
        """
        å…¼å®¹åŸå§‹edgeæ¨¡å‹çš„forward_cacheæ¥å£
        Args:
            x: è¾“å…¥tensor
            layer_idx: å±‚ç´¢å¼•
            attn_weights: æ³¨æ„åŠ›æƒé‡
        Returns:
            tuple: (v_cache, output_x) - ä¸åŸå§‹edgeæ¨¡å‹ç›¸åŒçš„è¿”å›æ ¼å¼
        """
        # ä½¿ç”¨SVDå‹ç¼©çš„å±‚è¿›è¡Œå‰å‘ä¼ æ’­
        # tim1=time.time()
        self.v_cache[layer_idx], output_x = self.svd_layers[layer_idx].forward_cache(
            x, self.v_cache[layer_idx], attn_weights
        )
        # tim2=time.time()
        # print(f"layer_{layer_idx}_forward_time:",tim2-tim1)
        
        # åº”ç”¨sliding windowåˆ°ç¼“å­˜
        if self.v_cache[layer_idx] is not None and self.v_cache[layer_idx].size(1) > self.max_ctx:
            self.v_cache[layer_idx] = self.v_cache[layer_idx][:, -self.max_ctx:, :]
        # tim3=time.time()
        # print(f"layer_{layer_idx}_memory_time:",tim3-tim2)

        return self.v_cache[layer_idx], output_x

class GPTJPipeline(nn.Module):
    def __init__(self, model_name='AI-ModelScope/gpt-j-6b', device_cloud='cuda:0', device_edge='cpu', svd_reduce_rate=0.5, use_compile=True,edge=None):
        super(GPTJPipeline, self).__init__()
        print(f"ğŸš€ åˆå§‹åŒ–GPTJPipeline...")
        print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
        print(f"   - æ¨¡å‹: {model_name}")
        print(f"   - äº‘ç«¯è®¾å¤‡: {device_cloud}")
        print(f"   - è¾¹ç¼˜è®¾å¤‡: {device_edge}")
        print(f"   - SVDå‹ç¼©ç‡: {svd_reduce_rate}")
        
        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        self.performance_monitor = PerformanceMonitor()
        
        # ä½¿ç”¨ ModelScope ä¸‹è½½æ¨¡å‹
        print(f"ğŸ“¥ ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹ {model_name}...")
        model_dir = snapshot_download(
            repo_id=model_name,
            cache_dir='./gpt-j-6b'
        )
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œè·¯å¾„: {model_dir}")
        
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„åŠ è½½ tokenizer
        print(f"ğŸ”¤ åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        
        # è®¾ç½® pad_token ä¸º eos_tokenï¼ˆGPT-J æ²¡æœ‰ pad_tokenï¼‰
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print(f"â˜ï¸  åŠ è½½äº‘ç«¯æ¨¡å‹åˆ° {device_cloud}...")
        self.cloud = gptJ_cloud(model_name=model_dir).to(device_cloud)
        print(f"ğŸ–¥ï¸  åŠ è½½è¾¹ç¼˜æ¨¡å‹åˆ°CPU...")
        # å¼ºåˆ¶ edge æ”¾åœ¨ CPU
        original_edge = gptJ_edge(model_name=model_dir).to('cpu')
        self.embed = self.cloud.model.transformer.wte
        self.ln_f = self.cloud.model.transformer.ln_f
        self.lm_head = self.cloud.model.lm_head
        self.num_layers = len(self.cloud.q_weights)
        
        print(f"ğŸ“Š æ¨¡å‹å…±æœ‰ {self.num_layers} å±‚")
        
        # SVDå‹ç¼©å‚æ•°
        self.svd_reduce_rate = svd_reduce_rate
        self.use_compile = use_compile
        
        # åˆ›å»ºæ•´ä¸ªSVD edgeæ¨¡å‹
        print(f"ğŸ”§ åˆ›å»ºSVDè¾¹ç¼˜æ¨¡å‹...")
        # å¦‚æœæœ‰GPUï¼Œå…ˆåœ¨GPUä¸Šè¿›è¡ŒSVDåˆ†è§£ï¼Œç„¶åç§»åˆ°CPU
        svd_device = device_cloud if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ”§ SVDåˆ†è§£å°†åœ¨ {svd_device} ä¸Šè¿›è¡Œ...")
        
        if(svd_reduce_rate!=-1):
            self.edge = SVD_GPTJ_Edge_Model(
                original_edge=original_edge,
                svd_reduce_rate=svd_reduce_rate,
                device='cpu',  # æœ€ç»ˆè¿è¡Œåœ¨CPUä¸Š
                svd_device=svd_device  # ä½†SVDåˆ†è§£åœ¨GPUä¸Šè¿›è¡Œ
            )
        else:
            self.edge=self.edge = SVD_GPTJ_Edge_Model(
                original_edge=original_edge,
                svd_reduce_rate=svd_reduce_rate,
                device='cpu',  # æœ€ç»ˆè¿è¡Œåœ¨CPUä¸Š
                svd_device=svd_device,  # ä½†SVDåˆ†è§£åœ¨GPUä¸Šè¿›è¡Œ
                No_init=True
            )
        
        print(f"âœ… GPTJPipelineåˆå§‹åŒ–å®Œæˆï¼")
        print(f"ğŸ¯ å‡†å¤‡å¼€å§‹æ¨ç†ï¼ŒSVDå‹ç¼©ç‡: {self.svd_reduce_rate}")


    def forward(self, input_ids):
        # 1. ç”Ÿæˆ padding mask: pad_token_id ä½ç½®ä¸º 0ï¼Œå…¶å®ƒä¸º 1
        #    å‡è®¾ self.config.pad_token_id å·²ç»è¢«è®¾ç½®
        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()  # [B, T]

        # Reset caches for a fresh generation
        for i in range(self.num_layers):
            self.cloud.k_cache[i] = None
            self.edge.v_cache[i] = None

        # Statistics variables
        cloud_time = edge_time = net_time = 0.0
        layer_calls = 0
        bandwidth = 10  # MB/s

        # Embedding
        x = self.embed(input_ids)  # [B, T, D]

        # å±‚çº§è¿­ä»£
        for layer_idx in range(self.num_layers):
            # Cloud forwardï¼šä¼ å…¥ attention_maskï¼Œç”¨äºå†…éƒ¨åš pad+causal å±è”½
            if hasattr(torch.cuda, 'synchronize'):
                torch.cuda.synchronize()
            t0 = time.time()
            _, _, attn_weights = self.cloud.forward_cache(x, layer_idx, attention_mask)
            if hasattr(torch.cuda, 'synchronize'):
                torch.cuda.synchronize()
            cloud_time += time.time() - t0

            # Edge forwardï¼ˆä¿æŒä¸å˜ï¼‰
            x_cpu = x.to('cuda:0')
            attn_cpu = attn_weights.to('cuda:0')
            t1 = time.time()
            _, x_cpu = self.edge.forward_cache(x_cpu, layer_idx, attn_cpu)
            edge_time += time.time() - t1

            # ç½‘ç»œå¼€é”€ä¼°ç®—
            elements = attn_cpu.numel() * attn_cpu.element_size()
            net_time += elements / bandwidth / 1024 / 1024
            x = x_cpu.to(self.embed.weight.device)
            elements = x.numel() * x.element_size()
            net_time += elements / bandwidth / 1024 / 1024

            layer_calls += 1

        # Final normalization and LM head to get logits
        x = self.ln_f(x)
        logits = self.lm_head(x)
        return logits



    def generate(self, prompt, max_length=50, temperature=1.0, top_k=50):
        """
        è°ƒç”¨ forward æ–¹æ³•ç”Ÿæˆæ–‡æœ¬
        """
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')[0].tolist()

        # å¼€å§‹ç”Ÿæˆæ–‡æœ¬
        outputs = input_ids.copy()

        for token_idx in range(max_length):
            # å½“å‰tokenè¾“å…¥åˆ°æ¨¡å‹
            cur_input = torch.tensor([[outputs[-1]]]).to(self.embed.weight.device)
            logits = self.forward(cur_input)  # è°ƒç”¨forwardæ–¹æ³•

            # ä½¿ç”¨ top-k + æ¸©åº¦é‡‡æ ·ä»£æ›¿è´ªå¿ƒé‡‡æ ·
            next_logits = logits[:, -1, :] / temperature
            topk_vals, topk_idx = torch.topk(next_logits, k=top_k, dim=-1)
            probs = torch.softmax(topk_vals, dim=-1)
            next_id = topk_idx[0, torch.multinomial(probs, num_samples=1).item()].item()
            
            outputs.append(next_id)
            
            # å¦‚æœé‡åˆ°ç»“æŸç¬¦ï¼Œæå‰åœæ­¢
            if next_id == self.tokenizer.eos_token_id:
                print(f"  é‡åˆ°ç»“æŸç¬¦ï¼Œæå‰ç»“æŸç”Ÿæˆ")
                break

        return self.tokenizer.decode(outputs, clean_up_tokenization_spaces=True)


def load_svd_cache(
    model_dir: str = "svd_models",
    rates: list[float] = None,
    map_location: str = "cpu"
) -> dict[float, nn.ModuleList]:
    """
    ä»ç£ç›˜åŠ è½½æ‰€æœ‰å·²ä¿å­˜çš„ SVD åˆ†è§£å±‚ï¼Œè¿”å› rate->ModuleList çš„æ˜ å°„ã€‚

    Args:
        model_dir (str): å­˜æ”¾ .pt æ–‡ä»¶çš„ç›®å½•ã€‚
        rates (List[float], optional): éœ€è¦åŠ è½½çš„å‹ç¼©ç‡åˆ—è¡¨ï¼Œé»˜è®¤ [0.0,0.1,...,0.9]ã€‚
        map_location (str): torch.load çš„ map_location å‚æ•°ï¼Œé»˜è®¤ä¸º "cpu"ã€‚

    Returns:
        Dict[float, ModuleList]: key æ˜¯å‹ç¼©ç‡ï¼Œvalue æ˜¯åŠ è½½åçš„ ModuleListã€‚
    """
    if rates is None:
        rates = [round(i * 0.1, 1) for i in range(10)]
    svd_cache: dict[float, nn.ModuleList] = {}
    for rate in rates:
        fname = f"svd_layers_rate_{rate}.pt"
        fpath = os.path.join(model_dir, fname)
        if not os.path.isfile(fpath):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ï¼š{fpath}")
            continue
        # åŠ è½½
        ml = torch.load(fpath, map_location=map_location)
        if not isinstance(ml, nn.ModuleList):
            raise ValueError(f"æ–‡ä»¶ {fname} ä¸­çš„å¯¹è±¡ä¸æ˜¯ ModuleList")
        svd_cache[rate] = ml
        print(f"âœ” å·²åŠ è½½ï¼š{fname}")
    return svd_cache

import gc
import random
import os
if __name__ == "__main__":
    # åŸºæœ¬é…ç½®
    model_name    = 'AI-ModelScope/gpt-j-6b'
    device_cloud  = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    device_edge   = 'cuda:0'
    svd_device    = device_cloud if torch.cuda.is_available() else 'cpu'
    rates         = [round(i * 0.1, 1) for i in range(0,10)]   # [0.0, 0.1, â€¦, 0.9]

    # 1. ä¸‹è½½å¹¶åŠ è½½åŸå§‹ edge æ¨¡å‹
    original_edge = gptJ_edge(model_name=model_name).to(device_edge)
    num_layers    = original_edge.num_layers

    del original_edge
    
    gc.collect()
    
    torch.cuda.empty_cache()
    torch.cuda.synchronize()

    print(f"Allocated memory: {torch.cuda.memory_allocated(device_edge)}")
    print(f"Reserved memory: {torch.cuda.memory_reserved(device_edge)}")

    # 2. é¢„åˆ†è§£ï¼šå¯¹æ¯å±‚ã€æ¯ä¸ª rate è¿›è¡Œä¸€æ¬¡ SVDï¼Œç¼“å­˜åˆ°å†…å­˜ & ç£ç›˜
    svd_cache: dict = {}   # rate -> nn.ModuleList
    os.makedirs("svd_models", exist_ok=True)
    # with torch.no_grad():
    #     for i, layer in enumerate(original_edge.layers):
    #         svd_cache.setdefault(i, {})  # åˆå§‹åŒ–è¯¥å±‚çš„ç¼“å­˜å­—å…¸
    #         print(f"â–¶ï¸ å¤„ç†ç¬¬ {i} å±‚ SVD åˆ†è§£")
            
    #         for rate in rates:
    #             cache_path = f"svd_models/svd_layer_{i}_rate_{rate}.pt"
    #             print(f"  ğŸ”„ å‹ç¼©ç‡={rate}")
    #             svd_layer=None
    #             # å¦‚æœå·²æœ‰ç¼“å­˜ï¼Œå°è¯•åŠ è½½
    #             if rate in svd_cache[i]:
    #                 print(f"    âš¡ ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡è®¡ç®—ï¼Œå·²åœ¨ svd_cache ä¸­")
    #             elif os.path.exists(cache_path):
    #                 try:
    #                     mod = torch.load(cache_path, map_location='cpu',weights_only=False)
    #                     svd_cache[i][rate] = mod
    #                     print(f"    âš¡ ä»ç£ç›˜åŠ è½½ç¼“å­˜ï¼š{cache_path}")
    #                 except Exception as e:
    #                     print(f"    âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥ï¼š{e}")
    #                     gc.collect()
    #             else:
    #                 # æœªç¼“å­˜åˆ™æ‰§è¡Œåˆ†è§£
    #                 print(f"    â–¶ï¸ æ‰§è¡Œåˆ†è§£ï¼Œä½¿ç”¨è®¾å¤‡: {device_edge}")
    #                 svd_layer = SVDED_GPTJ_EDGE_Layer(
    #                     gptj_edge_layer=layer,
    #                     reduce_rate=rate,
    #                     device=device_edge,
    #                     svd_device=svd_device
    #                 )
    #                 # åˆ†è§£å®Œæˆåï¼Œç§»åŠ¨åˆ°CPUå¹¶æ¸…ç†æ˜¾å­˜
    #                 svded_layer=svd_layer.to('cpu')
    #                 torch.cuda.empty_cache()
    #                 gc.collect()
    #                 svd_cache[i][rate] = svded_layer
    #                 # ä¿å­˜åˆ°ç£ç›˜
    #                 torch.save(svded_layer, cache_path)
                    
    #             if svd_layer is not None:
    #                 # svd_layer.clear()
    #                 del svd_layer
    #             gc.collect()
    #             torch.cuda.synchronize()
    #             torch.cuda.empty_cache()
    #             print(f"    âœ” å·²ä¿å­˜ï¼š{cache_path}")
    #         # æ­¤å±‚æ‰€æœ‰ rate å®Œæˆåï¼Œå¼ºåˆ¶å›æ”¶æœ¬å±‚ä¸´æ—¶å˜é‡
    #         # del layer
    #         gc.collect()
    #         torch.cuda.empty_cache()

    print("æ­£å¸¸é€€å‡º")
    # 3. éšæœºç”Ÿæˆ 1000 ä¸ªè£å‰ªæ–¹æ¡ˆ
    schemes=[]
    for i in range(2,9):
        rates=[round(j * 0.1, 1) for j in range(0,i+1)]
        for k in range(20):
            temp=[random.choice(rates) for _ in range(num_layers)]
            schemes.append(temp)
    # schemes = [
    #     [random.choice(rates) for _ in range(num_layers)]
    #     for _ in range(1000)
    # ]
    # schemes[0]=[0.0 for _ in range(num_layers)]

    # 4. å¯¹æ¯ä¸ªæ–¹æ¡ˆï¼Œå¿«é€Ÿæ„å»º edge_model å¹¶è¯„ä¼°
    #    è¿™é‡Œå‡è®¾å·²æœ‰ä¸€ä¸ª eval_pipeline(pipeline, scheme_index) å‡½æ•°ï¼Œ
    #    å®ƒä¼šæŠŠ pipeline.edge æ¢æˆåŸºäºè¯¥æ–¹æ¡ˆçš„ SVD_GPTJ_Edge_Modelï¼Œ
    #    ç„¶åå¯¹ MINIPIPE æ•°æ®é›†åšä¸€æ¬¡ evalã€‚
    # from MINI_PIPE_TEST import evaluate_on_minipipe  # å‡è®¾å·²æœ‰è¯„ä¼°å‡½æ•°

    # # åˆå§‹åŒ–äº‘ç«¯ pipelineï¼ŒåªåŠ è½½ä¸€æ¬¡å¤§æ¨¡å‹
    pipeline = GPTJPipeline(
        model_name=model_name,
        device_cloud=device_cloud,
        device_edge=device_edge,
        svd_reduce_rate=-1,  # å ä½ï¼Œæ— å®é™…ç”¨åˆ°
        
    )


    evaluation_results = {}

    dataloader=load_and_tokenize_dataset("./minipile_cache",pipeline.tokenizer,1)

    import pickle
    with torch.no_grad():
        for idx, scheme in enumerate(schemes):
            torch.cuda.empty_cache()
            # 4.1 æ„å»ºä»…æ›¿æ¢ svd_layers çš„ edge æ¨¡å‹
            edge_model = pipeline.edge  # ç›´æ¥å¤ç”¨å¯¹è±¡
            temp = nn.ModuleList()

            # åŠ è½½æ¨¡å‹ç¼“å­˜å¹¶æ·»åŠ åˆ° svd_layers
            for i, rate in enumerate(scheme):
                cache_path = f"svd_models/svd_layer_{i}_rate_{rate}.pt"
                print(f"æ­£åœ¨åŠ è½½ï¼š{cache_path}")
                mod = torch.load(cache_path, map_location='cuda:0', weights_only=False)
                temp.append(mod)

            edge_model.svd_layers = temp
            edge_model.v_cache = [None] * num_layers
            
            # 4.2 å°† pipeline.edge æŒ‡å‘æ–°æ¨¡å‹å¹¶è¯„ä¼°
            pipeline.edge = edge_model.to('cuda:0')
            pipeline.cloud = pipeline.cloud.to('cuda:0')
            print(f"\n===== æ–¹æ¡ˆ {idx + 1}/200 =====")
            
            # è°ƒç”¨ evaluate_minipile_gptj å‡½æ•°å¹¶è·å–ç»“æœ
            eval_result = evaluate_minipile_gptj(pipeline, batch_size=1, Dataloader=dataloader)
            
            # å­˜å‚¨å½“å‰æ–¹æ¡ˆçš„è¯„ä¼°ç»“æœä¸ scheme
            evaluation_results[idx] = (scheme, eval_result)
            
            # æ¯æ¬¡è¯„ä¼°å®Œåå°†æ•´ä¸ª evaluation_results å­˜å‚¨åˆ°æ–‡ä»¶
            with open('evaluation_results.pkl', 'wb') as f:
                pickle.dump(evaluation_results, f)
            
            # é‡Šæ”¾å†…å­˜
            pipeline.edge.to('cpu')
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            # å¯é€‰ï¼šæ‰“å°æ¯ä¸ªæ–¹æ¡ˆçš„è¯„ä¼°ç»“æœ
            print(f"æ–¹æ¡ˆ {idx + 1} è¯„ä¼°ç»“æœ: avg_loss = {eval_result['avg_loss']}, perplexity = {eval_result['perplexity']}")
            
            # æ¸…ç†ç¼“å­˜
            del temp
            gc.collect()

    print("\nğŸ‰ å…¨éƒ¨ 1000 ä¸ªæ–¹æ¡ˆè¯„ä¼°å®Œæˆï¼")
