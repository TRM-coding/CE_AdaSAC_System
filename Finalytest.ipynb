{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE:loading_resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianruiming/miniconda3/envs/pytorch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tianruiming/miniconda3/envs/pytorch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE:loading_finished\n"
     ]
    }
   ],
   "source": [
    "from detection.DataGenerator import train_based_self_detection\n",
    "from detection.Loader.ResNet50Loader import Resnet50Loader\n",
    "import detection.Spliter\n",
    "import torch\n",
    "from torch import nn\n",
    "print(\"CODE:loading_resnet50\")\n",
    "model=Resnet50Loader().load()\n",
    "print(\"CODE:loading_finished\")\n",
    "device='cuda:3'\n",
    "back_device='cuda:3'\n",
    "quantisized_type=torch.qint8\n",
    "cut_step=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建量化模型用于测试acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization.observer import MovingAveragePerChannelMinMaxObserver\n",
    "observer = MovingAveragePerChannelMinMaxObserver(ch_axis=0,dtype=quantisized_type).to(device)\n",
    "\n",
    "class quantiseze_model(nn.Module):\n",
    "    def __init__(self,model_list):\n",
    "        super(quantiseze_model,self).__init__()\n",
    "        self.model_list=model_list\n",
    "    def forward(self,x):\n",
    "        x=self.model_list[0](x)\n",
    "        observer(x)\n",
    "        scale, zero_point = observer.calculate_qparams()\n",
    "        # scale=scale.to(device)\n",
    "        # zero_point=zero_point.to(device)\n",
    "        x_quantized = torch.quantize_per_channel(x, scales=scale, zero_points=zero_point, axis=0, dtype=quantisized_type)\n",
    "        x=self.model_list[1](x_quantized.dequantize())\n",
    "        observer(x)\n",
    "        scale, zero_point = observer.calculate_qparams()\n",
    "        # scale=scale.to(device)\n",
    "        # zero_point=zero_point.to(device)\n",
    "        x_quantized = torch.quantize_per_channel(x, scales=scale, zero_points=zero_point, axis=0, dtype=quantisized_type)\n",
    "        x=self.model_list[2](x_quantized.dequantize())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianruiming/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/tianruiming/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/tianruiming/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.82 loss: 1.4887244701385498\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "datamaker=train_based_self_detection(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    no_weight=True\n",
    ")\n",
    "\n",
    "input_data,output_label,label,highest_loss,lowest_loss= datamaker.make_data_pid(\n",
    "        total_number=100,\n",
    "        batch_size=100,\n",
    "        learning_rate=1,\n",
    "        warm_lr=1e-3,\n",
    "        channel=3,\n",
    "        dim1=224,\n",
    "        dim2=224,\n",
    "        output_size=1000,\n",
    "        randn_magnification=100,\n",
    "        confidence=1000000,\n",
    "        target_acc=0.8\n",
    "\n",
    ")\n",
    "print(input_data.dtype)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化搜索器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "max_latency 35.09849713242353\n",
      "min_latency 8.497443984564706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00, 18.09it/s]\n",
      "7it [00:00, 38.88it/s]\n",
      "7it [00:00, 43.93it/s]\n",
      "9it [00:00, 27.22it/s]\n",
      "7it [00:00, 36.72it/s]\n",
      "7it [00:00, 33.59it/s]\n",
      "7it [00:00, 16.83it/s]]\n",
      "9it [00:00, 13.11it/s]]\n",
      "7it [00:00, 13.35it/s]]\n",
      "7it [00:00, 19.77it/s]]\n",
      "7it [00:00, 22.09it/s]]\n",
      "7it [00:00, 14.07it/s]]\n",
      "7it [00:00, 22.36it/s]]\n",
      "9it [00:01,  6.29it/s]]\n",
      "7it [00:00,  9.64it/s]]\n",
      "7it [00:00,  9.07it/s]]\n",
      "20it [00:07,  1.36it/s]/home/tianruiming/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "23it [00:08,  2.68it/s]\n",
      "23it [00:00, 115.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD_finished\n",
      "Loss上限: 6.219280242919922\n",
      "acc上限: 0.0\n",
      "最长耗时: 17.57764629684706\n",
      "最短耗时: 9.939687708611764\n",
      "\n",
      "\n",
      "quantized_acc_list: [0.84, 0.81, 0.82, 0.81, 0.81, 0.81, 0.81, 0.81, 0.82, 0.82, 0.82, 0.84, 0.82, 0.83, 0.84, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.84, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.84, 0.84, 0.84, 0.84, 0.85, 0.85]\n",
      "quantized_network_list: [8.02816, 8.02816, 8.02816, 8.02816, 8.02816, 8.02816, 8.02816, 8.02816, 8.02816, 8.02816, 8.02816, 4.01408, 4.01408, 4.01408, 4.01408, 4.01408, 4.01408, 4.01408, 4.01408, 4.01408, 4.01408, 4.01408, 4.01408, 4.01408, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 2.00704, 1.00352, 1.00352, 1.00352, 1.00352, 1.00352, 1.00352]\n",
      "quantized_compute_list: [0.4031120037647059, 0.45924064828235295, 0.7470147659294117, 0.8909018247529412, 1.0347888835764707, 1.177363102117647, 1.465137219764706, 1.6090242785882352, 2.3289453206588235, 2.712644144188235, 2.904493555952941, 3.358282938729412, 3.7382923504941177, 3.9282970563764708, 3.2889681152, 3.4297117952000002, 3.7137965010823533, 3.8558388540235295, 3.996582534023529, 4.280667239905883, 4.422709592847059, 4.563453272847059, 4.847537978729411, 4.989580331670588, 5.328275415341177, 5.61236012122353, 5.754402474164706, 6.0953041212235295, 6.236054884894117, 6.52013959077647, 6.662181943717647, 6.802932707388235, 7.087017413270588, 7.229059766211765, 7.369810529882353, 7.653895235764706, 7.795937588705883, 7.93668835237647, 8.220773058258823, 8.3628154112, 8.503566174870588, 8.78765088075294, 8.929693233694117, 9.268395991341176, 9.55248069722353, 9.694523050164706, 10.034871285458824, 10.175625590964705, 10.459710296847058]\n",
      "quantized_time_list: [8.431272003764706, 8.487400648282353, 8.775174765929412, 8.91906182475294, 9.062948883576471, 9.205523102117647, 9.493297219764706, 9.637184278588235, 10.357105320658823, 10.740804144188235, 10.93265355595294, 7.372362938729411, 7.7523723504941175, 7.94237705637647, 7.303048115199999, 7.4437917952, 7.727876501082353, 7.869918854023529, 8.01066253402353, 8.294747239905883, 8.43678959284706, 8.57753327284706, 8.861617978729411, 9.003660331670588, 7.335315415341177, 7.61940012122353, 7.761442474164706, 8.10234412122353, 8.243094884894116, 8.52717959077647, 8.669221943717647, 8.809972707388235, 9.094057413270587, 9.236099766211765, 9.376850529882354, 9.660935235764706, 9.802977588705883, 9.943728352376471, 10.227813058258823, 10.3698554112, 10.510606174870588, 10.79469088075294, 10.936733233694117, 10.271915991341176, 10.55600069722353, 10.698043050164706, 11.038391285458824, 11.179145590964705, 11.463230296847058]\n",
      "\n",
      "min_quantized_time_list: 7.303048115199999\n",
      "max_quantized_time_list: 11.463230296847058\n",
      "normaled_time: [0.2711957888627923, 0.2846876606287123, 0.35386110185842456, 0.38844782247328047, 0.4230345430881368, 0.4573056909167469, 0.5264791321464591, 0.561065852761315, 0.7341162170570354, 0.8263474720299853, 0.87246309951646, 0.01666148752696442, 0.1080059035097896, 0.15367811150120209, 0.0, 0.033831133795270224, 0.10211773603485784, 0.13626103715465177, 0.17009217094992177, 0.2383787731895096, 0.27252207430930353, 0.3063532081045735, 0.37463981034416094, 0.40878311146395485, 0.007756222860510109, 0.07604282510009773, 0.11018612621989164, 0.1921300489073968, 0.22596288543352755, 0.2942494876731154, 0.3283927887929093, 0.36222562531904046, 0.4305122275586279, 0.4646555286784222, 0.4984883652045534, 0.5667749674441408, 0.6009182685639347, 0.6347511050900659, 0.7030377073296533, 0.7371810084494472, 0.7710138449755783, 0.8393004472151658, 0.8734437483349597, 0.7136389096704826, 0.7819255119100704, 0.8160688130298643, 0.8978797098688508, 0.9317133977604122, 1.0]\n",
      "normaled_acc: [0.7499999999999993, 0.0, 0.24999999999999792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24999999999999792, 0.24999999999999792, 0.24999999999999792, 0.7499999999999993, 0.24999999999999792, 0.4999999999999986, 0.7499999999999993, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999993, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999993, 0.7499999999999993, 0.7499999999999993, 0.7499999999999993, 1.0, 1.0]\n",
      "\n",
      "idx: 0 net_time: 8.02816 compute_time: 0.4031120037647059 acc: 0.84 F_per_point: 2.094800374502932\n",
      "idx: 1 net_time: 8.02816 compute_time: 0.45924064828235295 acc: 0.81 F_per_point: 1.5224126309816441\n",
      "idx: 2 net_time: 8.02816 compute_time: 0.7470147659294117 acc: 0.82 F_per_point: 1.5960922038159766\n",
      "idx: 3 net_time: 8.02816 compute_time: 0.8909018247529412 acc: 0.81 F_per_point: 1.4216451466100803\n",
      "idx: 4 net_time: 8.02816 compute_time: 1.0347888835764707 acc: 0.81 F_per_point: 1.390313417593775\n",
      "idx: 5 net_time: 8.02816 compute_time: 1.177363102117647 acc: 0.81 F_per_point: 1.3603182745818585\n",
      "idx: 6 net_time: 8.02816 compute_time: 1.465137219764706 acc: 0.81 F_per_point: 1.3028187450798714\n",
      "idx: 7 net_time: 8.02816 compute_time: 1.6090242785882352 acc: 0.81 F_per_point: 1.2755265714460484\n",
      "idx: 8 net_time: 8.02816 compute_time: 2.3289453206588235 acc: 0.82 F_per_point: 1.2943044258583667\n",
      "idx: 9 net_time: 8.02816 compute_time: 2.712644144188235 acc: 0.82 F_per_point: 1.2368337716591438\n",
      "idx: 10 net_time: 8.02816 compute_time: 2.904493555952941 acc: 0.82 F_per_point: 1.2100261020466234\n",
      "idx: 11 net_time: 4.01408 compute_time: 3.358282938729412 acc: 0.84 F_per_point: 2.3951832220232623\n",
      "idx: 12 net_time: 4.01408 compute_time: 3.7382923504941177 acc: 0.82 F_per_point: 1.862007898119174\n",
      "idx: 13 net_time: 4.01408 compute_time: 3.9282970563764708 acc: 0.83 F_per_point: 1.9898892236035288\n",
      "idx: 14 net_time: 4.01408 compute_time: 3.2889681152 acc: 0.84 F_per_point: 2.4176409225358593\n",
      "idx: 15 net_time: 4.01408 compute_time: 3.4297117952000002 acc: 0.85 F_per_point: 2.6730696521371975\n",
      "idx: 16 net_time: 4.01408 compute_time: 3.7137965010823533 acc: 0.85 F_per_point: 2.586340830502998\n",
      "idx: 17 net_time: 4.01408 compute_time: 3.8558388540235295 acc: 0.85 F_per_point: 2.5451474155552467\n",
      "idx: 18 net_time: 4.01408 compute_time: 3.996582534023529 acc: 0.85 F_per_point: 2.5056946005487957\n",
      "idx: 19 net_time: 4.01408 compute_time: 4.280667239905883 acc: 0.85 F_per_point: 2.4300137456546658\n",
      "idx: 20 net_time: 4.01408 compute_time: 4.422709592847059 acc: 0.85 F_per_point: 2.3940677613727215\n",
      "idx: 21 net_time: 4.01408 compute_time: 4.563453272847059 acc: 0.85 F_per_point: 2.359640650391534\n",
      "idx: 22 net_time: 4.01408 compute_time: 4.847537978729411 acc: 0.85 F_per_point: 2.2936004149820888\n",
      "idx: 23 net_time: 4.01408 compute_time: 4.989580331670588 acc: 0.85 F_per_point: 2.262233416454857\n",
      "idx: 24 net_time: 2.00704 compute_time: 5.328275415341177 acc: 0.85 F_per_point: 2.70778080541115\n",
      "idx: 25 net_time: 2.00704 compute_time: 5.61236012122353 acc: 0.85 F_per_point: 2.618760796032775\n",
      "idx: 26 net_time: 2.00704 compute_time: 5.754402474164706 acc: 0.85 F_per_point: 2.576479140224712\n",
      "idx: 27 net_time: 2.00704 compute_time: 6.0953041212235295 acc: 0.85 F_per_point: 2.480703378533406\n",
      "idx: 28 net_time: 2.00704 compute_time: 6.236054884894117 acc: 0.85 F_per_point: 2.443392465004336\n",
      "idx: 29 net_time: 2.00704 compute_time: 6.52013959077647 acc: 0.84 F_per_point: 2.0711830967735994\n",
      "idx: 30 net_time: 2.00704 compute_time: 6.662181943717647 acc: 0.85 F_per_point: 2.33783127329661\n",
      "idx: 31 net_time: 2.00704 compute_time: 6.802932707388235 acc: 0.85 F_per_point: 2.3052732727205556\n",
      "idx: 32 net_time: 2.00704 compute_time: 7.087017413270588 acc: 0.85 F_per_point: 2.242821678357392\n",
      "idx: 33 net_time: 2.00704 compute_time: 7.229059766211765 acc: 0.85 F_per_point: 2.2131591694903743\n",
      "idx: 34 net_time: 2.00704 compute_time: 7.369810529882353 acc: 0.85 F_per_point: 2.184748624123147\n",
      "idx: 35 net_time: 2.00704 compute_time: 7.653895235764706 acc: 0.85 F_per_point: 2.130252530274703\n",
      "idx: 36 net_time: 2.00704 compute_time: 7.795937588705883 acc: 0.85 F_per_point: 2.1043686295755917\n",
      "idx: 37 net_time: 2.00704 compute_time: 7.93668835237647 acc: 0.85 F_per_point: 2.0795772089176245\n",
      "idx: 38 net_time: 2.00704 compute_time: 8.220773058258823 acc: 0.85 F_per_point: 2.0320231908813753\n",
      "idx: 39 net_time: 2.00704 compute_time: 8.3628154112 acc: 0.85 F_per_point: 2.0094365540461343\n",
      "idx: 40 net_time: 2.00704 compute_time: 8.503566174870588 acc: 0.85 F_per_point: 1.9878032298597827\n",
      "idx: 41 net_time: 2.00704 compute_time: 8.78765088075294 acc: 0.85 F_per_point: 1.9463069597292861\n",
      "idx: 42 net_time: 2.00704 compute_time: 8.929693233694117 acc: 0.85 F_per_point: 1.926597559300911\n",
      "idx: 43 net_time: 1.00352 compute_time: 9.268395991341176 acc: 0.84 F_per_point: 1.7242866016333784\n",
      "idx: 44 net_time: 1.00352 compute_time: 9.55248069722353 acc: 0.84 F_per_point: 1.6803398600535777\n",
      "idx: 45 net_time: 1.00352 compute_time: 9.694523050164706 acc: 0.84 F_per_point: 1.6594665641020452\n",
      "idx: 46 net_time: 1.00352 compute_time: 10.034871285458824 acc: 0.84 F_per_point: 1.6122583518278684\n",
      "idx: 47 net_time: 1.00352 compute_time: 10.175625590964705 acc: 0.85 F_per_point: 1.8944769749984807\n",
      "Loss上限: 5.769647121429443\n",
      "acc上限: 0.04\n",
      "最长耗时: 9.276084356517647\n",
      "最短耗时: 5.482123885929412\n",
      "每层裁减上限: [2, 3, 3, 2, 2, 3, 2, 2, 3, 0, 2, 2, 1, 2, 3, 4, 3, 3, 2, 1, 2, 2, 1, 3, 2, 2]\n",
      "开始分配进程,总任务量: 80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "searcher=detection.Spliter.Recrusively_reduce_search(\n",
    "        model=model,\n",
    "        no_weight=True,\n",
    "        input_data=input_data,\n",
    "        output_label=output_label,\n",
    "        label=label,\n",
    "        device=device,\n",
    "        back_device=back_device,\n",
    "        highest_loss=highest_loss,\n",
    "        lowest_loss=lowest_loss,\n",
    "        local_speed=2.72e10,   #Flops/s\n",
    "        # local_speed=9.6e9,   #Flops/s\n",
    "        cloud_speed=1.7e13,    #Flops/s\n",
    "        network_speed=1e7,     #B/s\n",
    "        acc_cut_point=0.7,\n",
    "        # q=q,\n",
    ")\n",
    "searcher.init(cut_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.input_data.shape\n",
    "upper_bound=searcher.GA_init(50,step=cut_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算每个分割点的时间和准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "upper_num=min(upper_bound)\n",
    "quantized_network_list=[]\n",
    "quantized_compute_list=[]\n",
    "quantized_time_list=[]\n",
    "quantized_acc_list=[]\n",
    "for i in range(1,50):\n",
    "    torch.cuda.empty_cache()\n",
    "    cut_num=int((max(upper_bound[:i+1])+min(upper_bound[:i+1]))/2)\n",
    "    model_r,edge_layer_map=searcher.model_reduce([cut_num]*i)\n",
    "    model_nr,edge_layer_map=searcher.model_reduce([0]*i)\n",
    "    eA_r,c_r,eB_r=searcher.split(model_r,len(edge_layer_map))\n",
    "    # print(model_r)\n",
    "    # input()\n",
    "    eA_nr,c_nr,eB_nr=searcher.split(model_nr,len(edge_layer_map))\n",
    "    qm_r=quantiseze_model([eA_r,c_r,eB_r])\n",
    "    qm_r.eval()\n",
    "    qm_nr=quantiseze_model([eA_nr,c_nr,eB_nr])\n",
    "    qm_nr.eval()\n",
    "    quantized_network_list.append(searcher.network_evaluate_quantisized(qm_r,quantisized_type))\n",
    "    acc,loss=searcher.acc_loss_evaluate(qm_nr)\n",
    "    compute_time=searcher.latency_evaluate(eA_r,c_r,eB_r)\n",
    "    quantized_compute_list.append(compute_time)\n",
    "    quantized_acc_list.append(acc)\n",
    "    # quantized_time_list.append(compute_time+searcher.network_evaluate_quantisized(qm_r))\n",
    "    quantized_time_list.append(quantized_network_list[-1]+quantized_compute_list[-1])\n",
    "    # input()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"quantized_acc_list:\",quantized_acc_list)\n",
    "print(\"quantized_network_list:\",quantized_network_list)\n",
    "print(\"quantized_compute_list:\",quantized_compute_list)\n",
    "print(\"quantized_time_list:\",quantized_time_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归一化分割点时间和准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"min_quantized_time_list:\",min(quantized_time_list))\n",
    "print(\"max_quantized_time_list:\",max(quantized_time_list))\n",
    "normaled_time=[(x-min(quantized_time_list))/(max(quantized_time_list)-min(quantized_time_list)) for x in quantized_time_list]\n",
    "normaled_acc=[(x-min(quantized_acc_list))/(max(quantized_acc_list)-min(quantized_acc_list)) for x in quantized_acc_list]\n",
    "print(\"normaled_time:\",normaled_time)\n",
    "print(\"normaled_acc:\",normaled_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算各点\"伪\"F得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alpha=0.5\n",
    "print()\n",
    "def F_score(alpha,index):\n",
    "    return float(alpha*np.exp(1-normaled_time[index])+(1-alpha)*np.exp(normaled_acc[index]))\n",
    "F_per_point=[F_score(alpha,i) for i in range(0,48)]\n",
    "\n",
    "\n",
    "for i in range(0,48):\n",
    "    print(\"idx:\",i,\"net_time:\",quantized_network_list[i],\"compute_time:\",quantized_compute_list[i],\"acc:\",quantized_acc_list[i],\"F_per_point:\",F_per_point[i])\n",
    "F_per_point=[0,0]+F_per_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 确定最优划分点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index=F_per_point.index(max(F_per_point))\n",
    "best_index\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依据最优划分点进行搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_GA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumber_of_layer_to_reduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcut_step\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sdpcos_2025/Eckart-Young-based-ML-Inference-framework/detection/Spliter.py:383\u001b[0m, in \u001b[0;36mRecrusively_reduce_search.search_GA\u001b[0;34m(self, number_of_layer_to_reduce, alpha, step)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# 将任务分配给进程池中的进程\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始分配进程,总任务量:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(task_list))\n\u001b[0;32m--> 383\u001b[0m     \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtaski\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtask_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtask_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumworker\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minit_species\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minit_species\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumworker\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tasks are completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    389\u001b[0m     ed\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/pool.py:540\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     \u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m     job, idx \u001b[38;5;241m=\u001b[39m task[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:225\u001b[0m, in \u001b[0;36mreduce_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_tensor\u001b[39m(tensor):\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mis_leaf:\n\u001b[0;32m--> 225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCowardly refusing to serialize non-leaf tensor which requires_grad, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msince autograd does not support crossing process boundaries.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you just want to transfer the data, call detach() on the tensor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore serializing (e.g., putting it on the queue).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    232\u001b[0m     check_serializing_named_tensor(tensor)\n\u001b[1;32m    233\u001b[0m     torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mhooks\u001b[38;5;241m.\u001b[39mwarn_if_has_hooks(tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue)."
     ]
    }
   ],
   "source": [
    "searcher.search_GA(\n",
    "    number_of_layer_to_reduce=best_index,\n",
    "    alpha=0.7,\n",
    "    step=cut_step\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
