{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be478799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from modelscope.utils.hub import snapshot_download\n",
    "from detection.Loader.mymodel_file.gptJ_cloud import gptJ_cloud\n",
    "from detection.Loader.mymodel_file.gptJ_edge import gptJ_edge\n",
    "\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from  datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "from modelscope.utils.hub import snapshot_download\n",
    "import torch\n",
    "\n",
    "class GPTJPipeline:\n",
    "    def __init__(self, model_name='AI-ModelScope/gpt-j-6b', device_cloud='cuda:0', device_edge='cpu'):\n",
    "        # 使用 ModelScope 下载模型\n",
    "        print(f\"Downloading model {model_name} using ModelScope...\")\n",
    "        model_dir = snapshot_download(\n",
    "            repo_id=model_name,\n",
    "            cache_dir='./gpt-j-6b'\n",
    "        )\n",
    "        print(f\"Model downloaded to: {model_dir}\")\n",
    "        \n",
    "        # 使用本地模型路径加载 tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "        \n",
    "        # 设置 pad_token 为 eos_token（GPT-J 没有 pad_token）\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.cloud = gptJ_cloud(model_name=model_dir).to(device_cloud)\n",
    "        # 强制 edge 放在 CPU\n",
    "        self.edge = gptJ_edge(model_name=model_dir).to('cpu')\n",
    "        \n",
    "        # 获取 embedding 和输出层\n",
    "        self.embed = self.cloud.model.transformer.wte\n",
    "        self.ln_f = self.cloud.model.transformer.ln_f\n",
    "        self.lm_head = self.cloud.model.lm_head\n",
    "        self.num_layers = len(self.cloud.q_weights)\n",
    "    def forward(self,prompt):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')[0].tolist()\n",
    "        outputs = input_ids.copy()\n",
    "        # cur_id = torch.tensor([[outputs[-1]]]).to(self.embed.weight.device)\n",
    "        x = self.embed(torch.tensor(outputs).to('cuda:0'))\n",
    "        # x = self.embed(torch.tensor(outputs).to(self.embed.weight.device))\n",
    "        \n",
    "        for layer_idx in range(self.num_layers):\n",
    "            # use cache-enabled forward so attention spans all previous tokens\n",
    "            if hasattr(torch.cuda, 'synchronize'):\n",
    "                torch.cuda.synchronize()\n",
    "            t0 = time.time()\n",
    "            _, _, attn_weights = self.cloud.forward_cache(x, layer_idx)\n",
    "            if hasattr(torch.cuda, 'synchronize'):\n",
    "                torch.cuda.synchronize()\n",
    "            # cloud_time += time.time() - t0\n",
    "\n",
    "            x_cpu = x.to('cpu')\n",
    "            attn_cpu = attn_weights.to('cpu')\n",
    "            # t1 = time.time()\n",
    "            _, x_cpu = self.edge.forward_cache(x_cpu, layer_idx, attn_cpu)\n",
    "            # edge_time += time.time() - t1\n",
    "            \n",
    "            elements = attn_cpu.numel() * attn_cpu.element_size()  # B\n",
    "            # net_time += elements / bandwidth / 1024 / 1024\n",
    "            x = x_cpu.to(self.embed.weight.device)\n",
    "            elements = x.numel() * x.element_size()  # B\n",
    "            # net_time += elements / bandwidth / 1024 / 1024\n",
    "            # layer_calls += 1\n",
    "            \n",
    "        # final normalization and LM head to get logits\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, prompt, max_length=50, temperature=1.0, top_k=50):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')[0].tolist()\n",
    "        outputs = input_ids.copy()\n",
    "\n",
    "        # reset caches for a fresh generation\n",
    "        for i in range(self.num_layers):\n",
    "            self.cloud.k_cache[i] = None\n",
    "            self.edge.v_cache[i] = None\n",
    "\n",
    "        # 统计变量\n",
    "        cloud_time = 0.0\n",
    "        edge_time = 0.0\n",
    "        layer_calls = 0\n",
    "        net_time = 0.0\n",
    "        bandwidth = 10  # MB/s\n",
    "\n",
    "        # 上下文窗口大小\n",
    "        max_ctx = self.cloud.max_ctx\n",
    "\n",
    "        # 预热缓存：将 prompt 中每个 token 走一次 forward_cache\n",
    "        for pos, token_id in enumerate(input_ids):\n",
    "            # clamp 位置，防止越界\n",
    "            pos_clamped = pos if pos < max_ctx else max_ctx - 1\n",
    "            cur_id = torch.tensor([[token_id]]).to(self.embed.weight.device)\n",
    "            \n",
    "            # GPT-J 没有位置embedding，直接使用 token embedding\n",
    "            x = self.embed(cur_id)\n",
    "            \n",
    "            for layer_idx in range(self.num_layers):\n",
    "                # cloud on GPU\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                t0 = time.time()\n",
    "                _, _, attn_weights = self.cloud.forward_cache(x, layer_idx)\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                cloud_time += time.time() - t0\n",
    "                \n",
    "                # edge on CPU: 把 x 和 attn_weights 都搬到 cpu\n",
    "                x_cpu = x.to('cpu')\n",
    "                attn_cpu = attn_weights.to('cpu')\n",
    "                elements = attn_cpu.numel() * attn_cpu.element_size()  # B\n",
    "                net_time += elements / bandwidth / 1024 / 1024  # s\n",
    "                \n",
    "                t1 = time.time()\n",
    "                _, x_cpu = self.edge.forward_cache(x_cpu, layer_idx, attn_cpu)\n",
    "                edge_time += time.time() - t1\n",
    "                print(f\"edge_time_{layer_idx}:\",time.time()-t1)\n",
    "                # 回到 GPU 继续下一层\n",
    "                x = x_cpu.to(self.embed.weight.device)\n",
    "                elements = x.numel() * x.element_size()  # B\n",
    "                net_time += elements / bandwidth / 1024 / 1024\n",
    "                layer_calls += 1\n",
    "\n",
    "        # 真实生成阶段\n",
    "        for _ in range(max_length):\n",
    "            cur_id = torch.tensor([[outputs[-1]]]).to(self.embed.weight.device)\n",
    "            x = self.embed(cur_id)\n",
    "            # x = self.embed(torch.tensor(outputs).to(self.embed.weight.device))\n",
    "            \n",
    "            for layer_idx in range(self.num_layers):\n",
    "                # use cache-enabled forward so attention spans all previous tokens\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                t0 = time.time()\n",
    "                _, _, attn_weights = self.cloud.forward_cache(x, layer_idx)\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                cloud_time += time.time() - t0\n",
    "\n",
    "                x_cpu = x.to('cpu')\n",
    "                attn_cpu = attn_weights.to('cpu')\n",
    "                t1 = time.time()\n",
    "                _, x_cpu = self.edge.forward_cache(x_cpu, layer_idx, attn_cpu)\n",
    "                edge_time += time.time() - t1\n",
    "                \n",
    "                elements = attn_cpu.numel() * attn_cpu.element_size()  # B\n",
    "                net_time += elements / bandwidth / 1024 / 1024\n",
    "                x = x_cpu.to(self.embed.weight.device)\n",
    "                elements = x.numel() * x.element_size()  # B\n",
    "                net_time += elements / bandwidth / 1024 / 1024\n",
    "                layer_calls += 1\n",
    "                \n",
    "            # final normalization and LM head to get logits\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "            \n",
    "            # 用 top-k + 温度采样代替贪心 argmax\n",
    "            next_logits = logits[:, -1, :] / temperature\n",
    "            topk_vals, topk_idx = torch.topk(next_logits, k=top_k, dim=-1)\n",
    "            probs = torch.softmax(topk_vals, dim=-1)\n",
    "            next_id = topk_idx[0, torch.multinomial(probs, num_samples=1).item()].item()\n",
    "            outputs.append(next_id)\n",
    "            \n",
    "            if next_id == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # 打印平均耗时\n",
    "        if layer_calls > 0:\n",
    "            print(f\"Avg GPU(cloud) per-layer: {cloud_time/layer_calls:.4f}s, CPU(edge) per-layer: {edge_time/layer_calls:.4f}s, net: {net_time/layer_calls:.4f}s\")\n",
    "            print(f\"Avg GPU(cloud) per-token: {cloud_time/layer_calls+net_time/layer_calls:.4f}s, CPU(edge) per-token: {edge_time/layer_calls:.4f}s\")\n",
    "            \n",
    "        return self.tokenizer.decode(outputs, clean_up_tokenization_spaces=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e400f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 16:35:05,288 - modelscope - WARNING - Model revision not specified, use revision: v1.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./gpt-j-6b/AI-ModelScope/gpt-j-6b\n"
     ]
    }
   ],
   "source": [
    "model_path = snapshot_download(\n",
    "            repo_id='AI-ModelScope/gpt-j-6b',\n",
    "            cache_dir='./gpt-j-6b'\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            local_files_only=True,\n",
    "            trust_remote_code=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d448b81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since JeanKaddour/minipile couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at minipile_cache/JeanKaddour___minipile/default/0.0.0/18ad1b0c701eaa0de03d3cecfdd769cbc70ffbd0 (last modified on Tue Jul 15 14:28:44 2025).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_and_tokenize_dataset(cache_dir: str, tokenizer, batch_size: int = 1):\n",
    "\n",
    "    ds = load_dataset(\"JeanKaddour/minipile\", split=\"validation\", cache_dir=cache_dir)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(examples['text'], padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    tokenized = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    # Group the dataset into blocks of model_max_length\n",
    "    block_size = tokenizer.model_max_length\n",
    "    def group_texts(examples):\n",
    "        all_ids = sum(examples[\"input_ids\"], [])\n",
    "        total_len = (len(all_ids) // block_size) * block_size\n",
    "        blocks = [all_ids[i:i + block_size] for i in range(0, total_len, block_size)]\n",
    "        return {\"input_ids\": blocks}\n",
    "\n",
    "    lm_dataset = tokenized.map(group_texts, batched=True, remove_columns=[\"attention_mask\"])\n",
    "\n",
    "    # DataLoader setup\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    dataloader = DataLoader(lm_dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "dataloader=load_and_tokenize_dataset(\"./minipile_cache\",tokenizer,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94599a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model AI-ModelScope/gpt-j-6b using ModelScope...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 16:35:37,811 - modelscope - WARNING - Model revision not specified, use revision: v1.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./gpt-j-6b/AI-ModelScope/gpt-j-6b\n",
      "Model downloaded to: ./gpt-j-6b/AI-ModelScope/gpt-j-6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./gpt-j-6b/AI-ModelScope/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ./gpt-j-6b/AI-ModelScope/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'AI-ModelScope/gpt-j-6b'\n",
    "device_cloud = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device_edge = 'cuda:0'\n",
    "\n",
    "pipeline = GPTJPipeline(model_name=model_name, device_cloud=device_cloud, device_edge=device_edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a3e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_time_0: 0.00906991958618164\n",
      "edge_time_1: 0.011162281036376953\n",
      "edge_time_2: 0.0074765682220458984\n",
      "edge_time_3: 0.007546663284301758\n",
      "edge_time_4: 0.007815122604370117\n",
      "edge_time_5: 0.007494926452636719\n",
      "edge_time_6: 0.0074841976165771484\n",
      "edge_time_7: 0.008049488067626953\n",
      "edge_time_8: 0.01416635513305664\n",
      "edge_time_9: 0.0077266693115234375\n",
      "edge_time_10: 0.009167194366455078\n",
      "edge_time_11: 0.00717926025390625\n",
      "edge_time_12: 0.00768589973449707\n",
      "edge_time_13: 0.011844635009765625\n",
      "edge_time_14: 0.00760197639465332\n",
      "edge_time_15: 0.007195949554443359\n",
      "edge_time_16: 0.0072896480560302734\n",
      "edge_time_17: 0.0076525211334228516\n",
      "edge_time_18: 0.007966279983520508\n",
      "edge_time_19: 0.007086753845214844\n",
      "edge_time_20: 0.007016658782958984\n",
      "edge_time_21: 0.007021665573120117\n",
      "edge_time_22: 0.007078647613525391\n",
      "edge_time_23: 0.011701822280883789\n",
      "edge_time_24: 0.007598161697387695\n",
      "edge_time_25: 0.008166790008544922\n",
      "edge_time_26: 0.007725238800048828\n",
      "edge_time_27: 0.0072460174560546875\n",
      "edge_time_0: 0.007661581039428711\n",
      "edge_time_1: 0.0074236392974853516\n",
      "edge_time_2: 0.007422447204589844\n",
      "edge_time_3: 0.008326530456542969\n",
      "edge_time_4: 0.008112907409667969\n",
      "edge_time_5: 0.008241891860961914\n",
      "edge_time_6: 0.008096456527709961\n",
      "edge_time_7: 0.0075359344482421875\n",
      "edge_time_8: 0.0075969696044921875\n",
      "edge_time_9: 0.007775068283081055\n",
      "edge_time_10: 0.012167692184448242\n",
      "edge_time_11: 0.007263898849487305\n",
      "edge_time_12: 0.007246971130371094\n",
      "edge_time_13: 0.0069539546966552734\n",
      "edge_time_14: 0.006991147994995117\n",
      "edge_time_15: 0.007193803787231445\n",
      "edge_time_16: 0.007021188735961914\n",
      "edge_time_17: 0.0069811344146728516\n",
      "edge_time_18: 0.007001161575317383\n",
      "edge_time_19: 0.007057666778564453\n",
      "edge_time_20: 0.006994485855102539\n",
      "edge_time_21: 0.007104396820068359\n",
      "edge_time_22: 0.007064104080200195\n",
      "edge_time_23: 0.007016181945800781\n",
      "edge_time_24: 0.007143497467041016\n",
      "edge_time_25: 0.0071179866790771484\n",
      "edge_time_26: 0.010509252548217773\n",
      "edge_time_27: 0.0070514678955078125\n",
      "edge_time_0: 0.007307767868041992\n",
      "edge_time_1: 0.007428646087646484\n",
      "edge_time_2: 0.007333040237426758\n",
      "edge_time_3: 0.0074579715728759766\n",
      "edge_time_4: 0.007414102554321289\n",
      "edge_time_5: 0.007456541061401367\n",
      "edge_time_6: 0.007504463195800781\n",
      "edge_time_7: 0.007012128829956055\n",
      "edge_time_8: 0.007127046585083008\n",
      "edge_time_9: 0.0070841312408447266\n",
      "edge_time_10: 0.00703883171081543\n",
      "edge_time_11: 0.006983280181884766\n",
      "edge_time_12: 0.007113456726074219\n",
      "edge_time_13: 0.006989955902099609\n",
      "edge_time_14: 0.009940147399902344\n",
      "edge_time_15: 0.007088899612426758\n",
      "edge_time_16: 0.0069806575775146484\n",
      "edge_time_17: 0.0069806575775146484\n",
      "edge_time_18: 0.007073402404785156\n",
      "edge_time_19: 0.007050275802612305\n",
      "edge_time_20: 0.00696563720703125\n",
      "edge_time_21: 0.0071849822998046875\n",
      "edge_time_22: 0.007071733474731445\n",
      "edge_time_23: 0.006951093673706055\n",
      "edge_time_24: 0.007129669189453125\n",
      "edge_time_25: 0.0069048404693603516\n",
      "edge_time_26: 0.006979227066040039\n",
      "edge_time_27: 0.0070896148681640625\n",
      "edge_time_0: 0.009938240051269531\n",
      "edge_time_1: 0.014528274536132812\n",
      "edge_time_2: 0.012395143508911133\n",
      "edge_time_3: 0.007614850997924805\n",
      "edge_time_4: 0.008074045181274414\n",
      "edge_time_5: 0.008484125137329102\n",
      "edge_time_6: 0.009850740432739258\n",
      "edge_time_7: 0.010189056396484375\n",
      "edge_time_8: 0.008858442306518555\n",
      "edge_time_9: 0.007076740264892578\n",
      "edge_time_10: 0.007082223892211914\n",
      "edge_time_11: 0.0072727203369140625\n",
      "edge_time_12: 0.007353067398071289\n",
      "edge_time_13: 0.007121086120605469\n",
      "edge_time_14: 0.0090789794921875\n",
      "edge_time_15: 0.008717775344848633\n",
      "edge_time_16: 0.009143829345703125\n",
      "edge_time_17: 0.01096200942993164\n",
      "edge_time_18: 0.008021116256713867\n",
      "edge_time_19: 0.00715947151184082\n",
      "edge_time_20: 0.0072324275970458984\n",
      "edge_time_21: 0.007032155990600586\n",
      "edge_time_22: 0.007425546646118164\n",
      "edge_time_23: 0.007265567779541016\n",
      "edge_time_24: 0.007572174072265625\n",
      "edge_time_25: 0.007936477661132812\n",
      "edge_time_26: 0.007283449172973633\n",
      "edge_time_27: 0.009666204452514648\n",
      "Avg GPU(cloud) per-layer: 0.0022s, CPU(edge) per-layer: 0.0085s, net: 0.0009s\n",
      "Avg GPU(cloud) per-token: 0.0032s, CPU(edge) per-token: 0.0085s\n",
      "Once upon a time, in a lovely place of magical girl-ish, there lived seven sweet pink- and happy village, a village there was a very happy princesses in a prince charming woman named Eloise lived\n",
      "where-tale.A magical-\n",
      "\n",
      "\n",
      "\n",
      "edge_time_0: 0.008321523666381836\n",
      "edge_time_1: 0.007474422454833984\n",
      "edge_time_2: 0.010048151016235352\n",
      "edge_time_3: 0.007515430450439453\n",
      "edge_time_4: 0.007504940032958984\n",
      "edge_time_5: 0.00747227668762207\n",
      "edge_time_6: 0.007516622543334961\n",
      "edge_time_7: 0.007119417190551758\n",
      "edge_time_8: 0.007080078125\n",
      "edge_time_9: 0.006937265396118164\n",
      "edge_time_10: 0.006784915924072266\n",
      "edge_time_11: 0.006797313690185547\n",
      "edge_time_12: 0.006772518157958984\n",
      "edge_time_13: 0.006770610809326172\n",
      "edge_time_14: 0.008534908294677734\n",
      "edge_time_15: 0.006768226623535156\n",
      "edge_time_16: 0.006786823272705078\n",
      "edge_time_17: 0.00672149658203125\n",
      "edge_time_18: 0.006757259368896484\n",
      "edge_time_19: 0.00684356689453125\n",
      "edge_time_20: 0.006801128387451172\n",
      "edge_time_21: 0.006838560104370117\n",
      "edge_time_22: 0.008428335189819336\n",
      "edge_time_23: 0.014925479888916016\n",
      "edge_time_24: 0.01130223274230957\n",
      "edge_time_25: 0.007202625274658203\n",
      "edge_time_26: 0.007102012634277344\n",
      "edge_time_27: 0.007645606994628906\n",
      "edge_time_0: 0.010933876037597656\n",
      "edge_time_1: 0.008161306381225586\n",
      "edge_time_2: 0.007634639739990234\n",
      "edge_time_3: 0.007699728012084961\n",
      "edge_time_4: 0.0076446533203125\n",
      "edge_time_5: 0.007681131362915039\n",
      "edge_time_6: 0.007639408111572266\n",
      "edge_time_7: 0.0075342655181884766\n",
      "edge_time_8: 0.007493019104003906\n",
      "edge_time_9: 0.0076367855072021484\n",
      "edge_time_10: 0.007376432418823242\n",
      "edge_time_11: 0.0069811344146728516\n",
      "edge_time_12: 0.006958484649658203\n",
      "edge_time_13: 0.006876945495605469\n",
      "edge_time_14: 0.00689244270324707\n",
      "edge_time_15: 0.006873369216918945\n",
      "edge_time_16: 0.006910800933837891\n",
      "edge_time_17: 0.007081747055053711\n",
      "edge_time_18: 0.006959676742553711\n",
      "edge_time_19: 0.006968498229980469\n",
      "edge_time_20: 0.007033348083496094\n",
      "edge_time_21: 0.007241964340209961\n",
      "edge_time_22: 0.00729823112487793\n",
      "edge_time_23: 0.007405519485473633\n",
      "edge_time_24: 0.006936788558959961\n",
      "edge_time_25: 0.006855487823486328\n",
      "edge_time_26: 0.006855010986328125\n",
      "edge_time_27: 0.006837129592895508\n",
      "edge_time_0: 0.007552623748779297\n",
      "edge_time_1: 0.007560253143310547\n",
      "edge_time_2: 0.007554054260253906\n",
      "edge_time_3: 0.007581949234008789\n",
      "edge_time_4: 0.007544755935668945\n",
      "edge_time_5: 0.0075299739837646484\n",
      "edge_time_6: 0.007557392120361328\n",
      "edge_time_7: 0.007174491882324219\n",
      "edge_time_8: 0.0074024200439453125\n",
      "edge_time_9: 0.006922245025634766\n",
      "edge_time_10: 0.006942272186279297\n",
      "edge_time_11: 0.006963491439819336\n",
      "edge_time_12: 0.006946563720703125\n",
      "edge_time_13: 0.0069866180419921875\n",
      "edge_time_14: 0.006867408752441406\n",
      "edge_time_15: 0.006850004196166992\n",
      "edge_time_16: 0.006772518157958984\n",
      "edge_time_17: 0.007042646408081055\n",
      "edge_time_18: 0.006990909576416016\n",
      "edge_time_19: 0.0070149898529052734\n",
      "edge_time_20: 0.006907463073730469\n",
      "edge_time_21: 0.006906032562255859\n",
      "edge_time_22: 0.006865501403808594\n",
      "edge_time_23: 0.006905555725097656\n",
      "edge_time_24: 0.006822347640991211\n",
      "edge_time_25: 0.006906986236572266\n",
      "edge_time_26: 0.00705409049987793\n",
      "edge_time_27: 0.006964683532714844\n",
      "Avg GPU(cloud) per-layer: 0.0019s, CPU(edge) per-layer: 0.0085s, net: 0.0009s\n",
      "Avg GPU(cloud) per-token: 0.0028s, CPU(edge) per-token: 0.0085s\n",
      "China is a world leader in the area of rapid growing market for solar energy, and has the world’s economy, being a lot of renewable energy. It is a number of energy and is the biggest solar power generation and energy storage, but its renewable technology\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "generated_text = pipeline.generate(prompt, max_length=50)\n",
    "print(generated_text)\n",
    "prompt = \"China is a\"\n",
    "generated_text = pipeline.generate(prompt, max_length=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06fca695",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m logits=\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(logits.shape)\n\u001b[32m      3\u001b[39m predicted_ids = torch.argmax(logits, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mGPTJPipeline.forward\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m     52\u001b[39m     torch.cuda.synchronize()\n\u001b[32m     53\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m _, _, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcloud\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch.cuda, \u001b[33m'\u001b[39m\u001b[33msynchronize\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     56\u001b[39m     torch.cuda.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hy-tmp/sdpcos_2025/code/detection/Loader/mymodel_file/gptJ_cloud.py:129\u001b[39m, in \u001b[36mgptJ_cloud.forward_cache\u001b[39m\u001b[34m(self, x, layer_idx)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# 更新K缓存\u001b[39;00m\n\u001b[32m    128\u001b[39m k_cache = \u001b[38;5;28mself\u001b[39m.k_cache[layer_idx]\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m k_all = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_new\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m k_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m k_new\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Sliding window\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m k_all.size(\u001b[32m1\u001b[39m) > \u001b[38;5;28mself\u001b[39m.max_ctx:\n",
      "\u001b[31mRuntimeError\u001b[39m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "logits=pipeline.forward(prompt)\n",
    "print(logits.shape)\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_ids[0])  # 获取第一批次的词索引并转换为词\n",
    "\n",
    "# 3. 输出生成的文本\n",
    "generated_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82cba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "{'input_ids': tensor([[   48,    25,   198,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  48,   25,  198,  ..., -100, -100, -100]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# input_ids = input_ids[:, :-1]  # 去掉最后一个token，作为模型的输入# For causal language modeling, the target is the input itself\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# outputs = model(input_ids=input_ids,attention_mask=attn_mask)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     otuputs=\u001b[43mmodel\u001b[49m.forward(input_ids)\n\u001b[32m     20\u001b[39m     logits = outputs.logits  \n\u001b[32m     21\u001b[39m     torch.cuda.empty_cache()\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "device='cuda:0'    # Evaluation loop\n",
    "total_loss = 0.0\n",
    "total_batches = 0\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "# model=model.to(device)\n",
    "for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    print(\"yes\")\n",
    "    # target_ids = input_ids[:, 1:].contiguous()  # 去掉第一个token，生成目标序列\n",
    "    target_ids=batch['labels'].to(device)\n",
    "    print(batch)\n",
    "    attn_mask=batch['attention_mask'].to(device)\n",
    "    # input_ids = input_ids[:, :-1]  # 去掉最后一个token，作为模型的输入# For causal language modeling, the target is the input itself\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        # outputs = model(input_ids=input_ids,attention_mask=attn_mask)\n",
    "        otuputs=model.forward(input_ids)\n",
    "        logits = outputs.logits  \n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Compute loss (CrossEntropyLoss expects target to be in shape [batch_size, seq_len])\n",
    "    # We need to flatten logits and targets to compute cross-entropy loss correctly\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "    print(loss.item())\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    total_loss += loss.item()\n",
    "    total_batches += 1\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "avg_loss = total_loss / total_batches\n",
    "# perplexity = math.exp(avg_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
