# -----------------------------------------------------------------------------
# make_data_pid éšæœºè¾“å‡ºç©ºé—´ç»´åº¦ â€”â€” å¯¹é½åˆ° GPT-J å•å±‚è¾“å‡º (seq_len=1, hidden=4096)
TOTAL_NUMBER        = 1024         # åˆæˆæ ·æœ¬æ€»æ•°
BATCH_SIZE          = 8            # å°æ‰¹é‡å¤§å°

# 1. éšæœºç©ºé—´ç»´åº¦ï¼šå¯¹åº” transformer è¾“å‡º shape=(batch, seq_len, hidden)
CHANNEL             = 4096         # GPT-J-6B hidden_size
DIM1                = 1            # seq_len
DIM2                = 1            # å• token æ—¶å®½åº¦å– 1

# 2. åˆ†ç±»å¤´è¾“å‡ºï¼švocab_size
OUTPUT_SIZE         = 50400        # GPT-J-6B è¯è¡¨å¤§å°

# å­¦ä¹ ç‡ / å…¶ä»–è¶…å‚
LEARNING_RATE       = 1e-4         # å­¦ä¹ ç‡
WARM_LR             = 1e-5         # é¢„çƒ­å­¦ä¹ ç‡
RANDN_MAGNIFICATION = 1.0          # éšæœºå™ªå£°æ”¾å¤§å€æ•°
CONFIDENCE          = 0.9          # ç›®æ ‡ç½®ä¿¡åº¦
TARGET_ACC          = 0.8          # ç›®æ ‡å‡†ç¡®ç‡
# -----------------------------------------------------------------------------
# å°æ‰¹é‡æ•°æ®ä¿å­˜è·¯å¾„
SAVE_BATCH_PATH = "gptj_input_data/generated_batch.pt"
# -----------------------------------------------------------------------------

import os
import gc
import torch
import torch.nn as nn
from detection.DataGenerator_gptJ import InputOptimizer
from torch.profiler import profile, ProfilerActivity
from transformers import AutoTokenizer
from modelscope.utils.hub import snapshot_download
from detection.Loader.mymodel_file.gptJ_edge import gptJ_edge
from detection.SVD_model import SVDED_GPTJ_EDGE_Layer


class LayerWrapper(nn.Module):
    """åŒ…è£…å™¨æ¨¡å—ï¼Œç”¨äºå°†å±‚åŒ…è£…æˆnn.Moduleä»¥ä¾›torch.profilerä½¿ç”¨"""
    def __init__(self, layer):
        super().__init__()
        self.layer = layer
    
    def forward(self, x_input, attn_input):
        # æ¨¡æ‹Ÿv_cacheä¸ºNoneçš„æƒ…å†µï¼ˆç¬¬ä¸€ä¸ªtokenï¼‰
        _, output = self.layer.forward_cache(x_input, None, attn_input)
        return output


def init_flops(model_name='AI-ModelScope/gpt-j-6b', device='cuda:0'):
    """
    è®¡ç®—æ¯ä¸€å±‚åœ¨ä¸åŒSVDå‹ç¼©ç‡ä¸‹çš„FLOPS
    
    Args:
        model_name: æ¨¡å‹åç§°
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        dict: {layer_idx: [flops_0.0, flops_0.1, ..., flops_0.9]}
    """
    print(f"ğŸš€ å¼€å§‹åˆå§‹åŒ–FLOPSè®¡ç®—...")
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   - æ¨¡å‹: {model_name}")
    print(f"   - è®¾å¤‡: {device}")
    
    # ä½¿ç”¨ ModelScope ä¸‹è½½æ¨¡å‹
    print(f"ğŸ“¥ ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹ {model_name}...")
    model_dir = snapshot_download(
        repo_id=model_name,
        cache_dir='./gpt-j-6b'
    )
    print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œè·¯å¾„: {model_dir}")
    
    # åŠ è½½tokenizerç”¨äºè·å–è¾“å…¥è§„æ ¼
    print(f"ğŸ”¤ åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½edgeæ¨¡å‹
    print(f"ğŸ–¥ï¸  åŠ è½½è¾¹ç¼˜æ¨¡å‹...")
    original_edge = gptJ_edge(model_name=model_dir).to(device)
    num_layers = original_edge.num_layers
    print(f"ğŸ“Š æ¨¡å‹å…±æœ‰ {num_layers} å±‚")
    
    # å‡†å¤‡è¾“å…¥æ•°æ®ç”¨äºFLOPSè®¡ç®—
    batch_size = 1
    seq_len = 1
    hidden_size = original_edge.model.config.n_embd
    num_heads = original_edge.model.config.n_head
    
    input_x = torch.randn(batch_size, seq_len, hidden_size, dtype=torch.float16).to(device)
    attn_weights = torch.randn(batch_size, num_heads, seq_len, seq_len, dtype=torch.float16).to(device)
    
    print(f"ğŸ“ è¾“å…¥å¼ é‡è§„æ ¼:")
    print(f"   - input_x: {input_x.shape}")
    print(f"   - attn_weights: {attn_weights.shape}")
    
    flops_dict = {}
    for layer_idx in range(num_layers):
        print(f"\nğŸ”§ å¤„ç†ç¬¬ {layer_idx + 1}/{num_layers} å±‚...")
        flops_array = []
        for k in range(10):
            reduce_rate = k / 10.0
            print(f"  ğŸ“Š è®¡ç®— reduce_rate = {reduce_rate:.1f} çš„FLOPS...")
            try:
                if reduce_rate == 0.0:
                    test_layer = original_edge.layers[layer_idx]
                    wrapper_module = LayerWrapper(test_layer).to(device)
                else:
                    svd_layer = SVDED_GPTJ_EDGE_Layer(
                        gptj_edge_layer=original_edge.layers[layer_idx],
                        reduce_rate=reduce_rate,
                        device=device,
                        svd_device=device
                    )
                    wrapper_module = LayerWrapper(svd_layer).to(device)
                test_input_x = input_x.clone().detach()
                test_attn_weights = attn_weights.clone().detach()
                with profile(
                    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
                    record_shapes=True,
                    with_flops=True
                ) as prof:
                    _ = wrapper_module(test_input_x, test_attn_weights)
                flops = prof.key_averages().total_average().flops or 0
                flops_array.append(flops)
                print(f"    âœ… reduce_rate {reduce_rate:.1f}: {flops:,.0f} FLOPs")
                del wrapper_module
                if reduce_rate != 0.0:
                    del svd_layer
                del test_input_x, test_attn_weights
                torch.cuda.empty_cache(); gc.collect()
            except Exception as e:
                print(f"    âŒ reduce_rate {reduce_rate:.1f} è®¡ç®—å¤±è´¥: {e}")
                flops_array.append(0)
                torch.cuda.empty_cache(); gc.collect()
        flops_dict[layer_idx] = flops_array
        print(f"  ğŸ¯ ç¬¬ {layer_idx + 1} å±‚å®Œæˆ: {flops_array}")
        torch.cuda.empty_cache(); gc.collect()
    print(f"\nğŸ‰ FLOPSè®¡ç®—å®Œæˆï¼")
    del original_edge; torch.cuda.empty_cache(); gc.collect()
    return flops_dict


def save_flops_dict(flops_dict, filename="flops_dict.pt"):
    """ä¿å­˜FLOPSå­—å…¸åˆ°æ–‡ä»¶"""
    torch.save(flops_dict, filename)
    print(f"ğŸ’¾ FLOPSå­—å…¸å·²ä¿å­˜åˆ° {filename}")


def load_flops_dict(filename="flops_dict.pt"):
    """ä»æ–‡ä»¶åŠ è½½FLOPSå­—å…¸"""
    flops_dict = torch.load(filename)
    print(f"ğŸ“‚ FLOPSå­—å…¸å·²ä» {filename} åŠ è½½")
    return flops_dict


def print_flops_analysis(flops_dict):
    """æ‰“å°FLOPSåˆ†ææŠ¥å‘Š"""
    print(f"\nğŸ“Š FLOPSåˆ†ææŠ¥å‘Š")
    print("="*80)
    for layer_idx, flops_array in flops_dict.items():
        print(f"\nğŸ”¢ ç¬¬ {layer_idx} å±‚:")
        print(f"   reduce_rate: " + " ".join([f"{k/10:.1f}" for k in range(10)]))
        print(f"   FLOPs (M):   " + " ".join([f"{f/1e6:6.1f}" for f in flops_array]))
        original_flops = flops_array[0] if flops_array[0] > 0 else 1
        print(f"   å‹ç¼©æ¯”ä¾‹:    " + " ".join([f"{f/original_flops:6.2f}" if f>0 else "  N/A " for f in flops_array]))
    print(f"\n{'='*80}")


def generate_input_space(
    device: str = 'cuda:0',
    no_weight: bool = True
):
    """
    Generate a small batch of synthetic inputs for a GPT-J-6B model.
    Returns:
        input_data, output_label, label, highest_loss, lowest_loss
    """
    optimizer = InputOptimizer(
        model_name='AI-ModelScope/gpt-j-6b',
        device='cuda:0',
        batch_size=20,
        seq_len=64,
        hidden_size=4096,
        lr=1.2e-3,
        kd=1e-3  # example derivative gain
    )

    optimized_input,output_lable = optimizer.optimize(num_steps=50000, print_every=20)
    return optimized_input,output_lable


def save_generated_batch(
    device: str = "cuda:0",
    no_weight: bool = True,
    path: str = SAVE_BATCH_PATH
) -> str:
    """
    è°ƒç”¨ generate_input_space ç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡è¾“å…¥ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ã€‚
    Returns: æœ€ç»ˆä¿å­˜æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ã€‚
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)
    optimized_batch,output_lable=generate_input_space()
    batch = {
        "input_data": optimized_batch,
        "output_label":output_lable
    }
    torch.save(batch, path)
    return os.path.abspath(path)


def load_generated_batch(
    path: str = SAVE_BATCH_PATH
) -> dict:
    """
    ä»æ–‡ä»¶ä¸­è¯»å–ä¹‹å‰ä¿å­˜çš„å°æ‰¹é‡æ•°æ®ã€‚
    Returns: åŒ…å« input_data, output_label, label, highest_loss, lowest_lossã€‚
    """
    if not os.path.isfile(path):
        raise FileNotFoundError(f"No such batch file: {path}")
    return torch.load(path, map_location="cpu")


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šç”Ÿæˆå¹¶ä¿å­˜å°æ‰¹é‡æ•°æ®
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    model_dir = snapshot_download(repo_id='AI-ModelScope/gpt-j-6b', cache_dir='./gpt-j-6b')
    # model = gptJ_edge(model_name=model_dir).to(device)
    save_path = save_generated_batch( device=device)
    print(f"Batch saved to: {save_path}")

    # ç¤ºä¾‹ï¼šåŠ è½½å¹¶æ£€æŸ¥æ•°æ®
    batch = load_generated_batch(save_path)
    print({k: getattr(batch[k], 'shape', batch[k]) for k in batch})
