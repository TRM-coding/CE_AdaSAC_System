{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce13ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from modelscope.utils.hub import snapshot_download\n",
    "from detection.Loader.mymodel_file.gptJ_cloud import gptJ_cloud\n",
    "from detection.Loader.mymodel_file.gptJ_edge import gptJ_edge\n",
    "from detection.SVD_model import SVDED_GPTJ_EDGE_Layer\n",
    "\n",
    "class SVD_GPTJ_Edge_Model(nn.Module):\n",
    "    \"\"\"åŒ…å«æ‰€æœ‰SVDå±‚çš„å®Œæ•´edgeæ¨¡å‹\"\"\"\n",
    "    def __init__(self, original_edge, svd_reduce_rate, device='cpu', svd_device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.svd_device = svd_device\n",
    "        self.num_layers = original_edge.num_layers\n",
    "        self.max_ctx = original_edge.max_ctx\n",
    "        self.v_cache = [None] * self.num_layers\n",
    "        \n",
    "        print(f\"ğŸ”„ å¼€å§‹SVDåˆ†è§£å¤„ç†ï¼Œå‹ç¼©ç‡: {svd_reduce_rate}\")\n",
    "        print(f\"ğŸ“Š æ€»å…±éœ€è¦å¤„ç† {self.num_layers} å±‚...\")\n",
    "        print(f\"âš¡ SVDåˆ†è§£è®¾å¤‡: {svd_device}, è¿è¡Œè®¾å¤‡: {device}\")\n",
    "        \n",
    "        # ç”¨SVDå‹ç¼©çš„å±‚æ›¿æ¢åŸå§‹edgeå±‚\n",
    "        self.svd_layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            print(f\"  å¤„ç†ç¬¬ {i+1}/{self.num_layers} å±‚: \", end=\"\")\n",
    "            \n",
    "            # è·å–åŸå§‹edgeå±‚\n",
    "            if(i%2):\n",
    "                original_edge_layer = original_edge.layers[i]\n",
    "                # åˆ›å»ºSVDå‹ç¼©å±‚\n",
    "                svd_layer = SVDED_GPTJ_EDGE_Layer(\n",
    "                    gptj_edge_layer=original_edge_layer,\n",
    "                    reduce_rate=0,\n",
    "                    device=device,\n",
    "                    svd_device=svd_device  # ä¼ é€’SVDåˆ†è§£è®¾å¤‡\n",
    "                )\n",
    "                print(\"è·³è¿‡å‹ç¼© (å¥‡æ•°å±‚)\")\n",
    "                self.svd_layers.append(svd_layer)\n",
    "            else:\n",
    "                original_edge_layer = original_edge.layers[i]\n",
    "                # åˆ›å»ºSVDå‹ç¼©å±‚\n",
    "                print(f\"æ­£åœ¨è¿›è¡ŒSVDåˆ†è§£ (å‹ç¼©ç‡: {svd_reduce_rate})...\")\n",
    "                \n",
    "                # è®¡ç®—SVDåˆ†è§£çš„æ—¶é—´\n",
    "                svd_start_time = time.time()\n",
    "                svd_layer = SVDED_GPTJ_EDGE_Layer(\n",
    "                    gptj_edge_layer=original_edge_layer,\n",
    "                    reduce_rate=svd_reduce_rate,\n",
    "                    device=device,\n",
    "                    svd_device=svd_device  # ä¼ é€’SVDåˆ†è§£è®¾å¤‡\n",
    "                )\n",
    "                svd_end_time = time.time()\n",
    "                print(f\"    âœ… ç¬¬ {i+1} å±‚SVDåˆ†è§£å®Œæˆ (è€—æ—¶: {svd_end_time - svd_start_time:.2f}ç§’)\")\n",
    "                self.svd_layers.append(svd_layer)\n",
    "        \n",
    "        print(f\"ğŸ‰ æ‰€æœ‰å±‚çš„SVDåˆ†è§£å¤„ç†å®Œæˆï¼\")\n",
    "    \n",
    "    def forward_all_layers(self, x, attn_weights_list):\n",
    "        \"\"\"\n",
    "        å¤„ç†æ‰€æœ‰å±‚çš„å‰å‘ä¼ æ’­\n",
    "        x: Tensor [batch_size, seq_len, hidden]\n",
    "        attn_weights_list: List[Tensor] - æ¯å±‚çš„æ³¨æ„åŠ›æƒé‡\n",
    "        è¿”å›: æœ€ç»ˆçš„xè¾“å‡º\n",
    "        \"\"\"\n",
    "        current_x = x\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            attn_weights = attn_weights_list[layer_idx]\n",
    "            # ä½¿ç”¨SVDå‹ç¼©çš„å±‚\n",
    "            self.v_cache[layer_idx], current_x = self.svd_layers[layer_idx].forward_cache(\n",
    "                current_x, self.v_cache[layer_idx], attn_weights\n",
    "            )\n",
    "            \n",
    "            # åº”ç”¨sliding windowåˆ°ç¼“å­˜\n",
    "            if self.v_cache[layer_idx] is not None and self.v_cache[layer_idx].size(1) > self.max_ctx:\n",
    "                self.v_cache[layer_idx] = self.v_cache[layer_idx][:, -self.max_ctx:, :]\n",
    "        \n",
    "        return current_x\n",
    "\n",
    "class GPTJPipeline:\n",
    "    def __init__(self, model_name='AI-ModelScope/gpt-j-6b', device_cloud='cuda:0', device_edge='cpu', svd_reduce_rate=0.5, use_compile=True):\n",
    "        print(f\"ğŸš€ åˆå§‹åŒ–GPTJPipeline...\")\n",
    "        print(f\"ğŸ“‹ é…ç½®ä¿¡æ¯:\")\n",
    "        print(f\"   - æ¨¡å‹: {model_name}\")\n",
    "        print(f\"   - äº‘ç«¯è®¾å¤‡: {device_cloud}\")\n",
    "        print(f\"   - è¾¹ç¼˜è®¾å¤‡: {device_edge}\")\n",
    "        print(f\"   - SVDå‹ç¼©ç‡: {svd_reduce_rate}\")\n",
    "        \n",
    "        # ä½¿ç”¨ ModelScope ä¸‹è½½æ¨¡å‹\n",
    "        print(f\"ğŸ“¥ ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹ {model_name}...\")\n",
    "        model_dir = snapshot_download(\n",
    "            repo_id=model_name,\n",
    "            cache_dir='./gpt-j-6b'\n",
    "        )\n",
    "        print(f\"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œè·¯å¾„: {model_dir}\")\n",
    "        \n",
    "        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„åŠ è½½ tokenizer\n",
    "        print(f\"ğŸ”¤ åŠ è½½tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "        \n",
    "        # è®¾ç½® pad_token ä¸º eos_tokenï¼ˆGPT-J æ²¡æœ‰ pad_tokenï¼‰\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(f\"â˜ï¸  åŠ è½½äº‘ç«¯æ¨¡å‹åˆ° {device_cloud}...\")\n",
    "        self.cloud       = gptJ_cloud(model_name=model_dir).to(device_cloud)\n",
    "        print(f\"ğŸ–¥ï¸  åŠ è½½è¾¹ç¼˜æ¨¡å‹åˆ°CPU...\")\n",
    "        # å¼ºåˆ¶ edge æ”¾åœ¨ CPU\n",
    "        original_edge    = gptJ_edge (model_name=model_dir).to('cpu')\n",
    "        self.embed       = self.cloud.model.transformer.wte\n",
    "        self.ln_f        = self.cloud.model.transformer.ln_f\n",
    "        self.lm_head     = self.cloud.model.lm_head\n",
    "        self.num_layers  = len(self.cloud.q_weights)\n",
    "        \n",
    "        print(f\"ğŸ“Š æ¨¡å‹å…±æœ‰ {self.num_layers} å±‚\")\n",
    "        \n",
    "        # SVDå‹ç¼©å‚æ•°\n",
    "        self.svd_reduce_rate = svd_reduce_rate\n",
    "        self.use_compile = use_compile\n",
    "        \n",
    "        # åˆ›å»ºæ•´ä¸ªSVD edgeæ¨¡å‹\n",
    "        print(f\"ğŸ”§ åˆ›å»ºSVDè¾¹ç¼˜æ¨¡å‹...\")\n",
    "        # å¦‚æœæœ‰GPUï¼Œå…ˆåœ¨GPUä¸Šè¿›è¡ŒSVDåˆ†è§£ï¼Œç„¶åç§»åˆ°CPU\n",
    "        svd_device = device_cloud if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"ğŸ”§ SVDåˆ†è§£å°†åœ¨ {svd_device} ä¸Šè¿›è¡Œ...\")\n",
    "        \n",
    "        self.svd_edge_model = SVD_GPTJ_Edge_Model(\n",
    "            original_edge=original_edge,\n",
    "            svd_reduce_rate=svd_reduce_rate,\n",
    "            device='cpu',  # æœ€ç»ˆè¿è¡Œåœ¨CPUä¸Š\n",
    "            svd_device=svd_device  # ä½†SVDåˆ†è§£åœ¨GPUä¸Šè¿›è¡Œ\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… GPTJPipelineåˆå§‹åŒ–å®Œæˆï¼\")\n",
    "        print(f\"ğŸ¯ å‡†å¤‡å¼€å§‹æ¨ç†ï¼ŒSVDå‹ç¼©ç‡: {self.svd_reduce_rate}\")\n",
    "    \n",
    "    def generate(self, prompt, max_length=50, temperature=1.0, top_k=50):\n",
    "        print(f\"ğŸ”„ å¼€å§‹æ–‡æœ¬ç”Ÿæˆ...\")\n",
    "        print(f\"ğŸ“ è¾“å…¥æç¤º: '{prompt}'\")\n",
    "        print(f\"âš™ï¸  ç”Ÿæˆå‚æ•°: max_length={max_length}, temperature={temperature}, top_k={top_k}\")\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')[0].tolist()\n",
    "        outputs   = input_ids.copy()\n",
    "\n",
    "        # reset caches for a fresh generation\n",
    "        print(f\"ğŸ—‚ï¸  æ¸…ç©ºç¼“å­˜...\")\n",
    "        for i in range(self.num_layers):\n",
    "            self.cloud.k_cache[i] = None\n",
    "            self.svd_edge_model.v_cache[i] = None\n",
    "\n",
    "        # ç»Ÿè®¡å˜é‡\n",
    "        cloud_time = 0.0\n",
    "        edge_time  = 0.0\n",
    "        layer_calls= 0\n",
    "        net_time=0.0\n",
    "        bandwidth=10 #MB/s\n",
    "\n",
    "        # ä¸Šä¸‹æ–‡çª—å£å¤§å°\n",
    "        max_ctx = self.cloud.max_ctx\n",
    "\n",
    "        print(f\"ğŸ”¥ é¢„çƒ­é˜¶æ®µï¼šå¤„ç† {len(input_ids)} ä¸ªæç¤ºtoken...\")\n",
    "        # é¢„çƒ­ç¼“å­˜ï¼šå°† prompt ä¸­æ¯ä¸ª token èµ°ä¸€æ¬¡ forward_cache\n",
    "        for pos, token_id in enumerate(input_ids):\n",
    "            print(f\"  å¤„ç†æç¤ºtoken {pos+1}/{len(input_ids)}\")\n",
    "            # clamp ä½ç½®ï¼Œé˜²æ­¢è¶Šç•Œ\n",
    "            pos_clamped = pos if pos < max_ctx else max_ctx - 1\n",
    "            cur_id = torch.tensor([[token_id]]).to(self.embed.weight.device)\n",
    "            \n",
    "            # GPT-J æ²¡æœ‰ä½ç½®embeddingï¼Œç›´æ¥ä½¿ç”¨ token embedding\n",
    "            x = self.embed(cur_id)\n",
    "            \n",
    "            # æ”¶é›†æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›æƒé‡\n",
    "            attn_weights_list = []\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                # cloud on GPU\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                t0 = time.time()\n",
    "                _, _, attn_weights = self.cloud.forward_cache(x, layer_idx)\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                cloud_time += time.time() - t0\n",
    "                \n",
    "                # ç§»åŠ¨åˆ°CPUå¹¶æ”¶é›†æ³¨æ„åŠ›æƒé‡\n",
    "                attn_weights_cpu = attn_weights.to('cpu')\n",
    "                attn_weights_list.append(attn_weights_cpu)\n",
    "                \n",
    "                # è®¡ç®—ç½‘ç»œä¼ è¾“æ—¶é—´\n",
    "                elements = attn_weights_cpu.numel() * attn_weights_cpu.element_size()  # B\n",
    "                net_time += elements / bandwidth / 1024 / 1024  # s\n",
    "                \n",
    "                layer_calls += 1\n",
    "            \n",
    "            # ä½¿ç”¨SVD edgeæ¨¡å‹å¤„ç†æ‰€æœ‰å±‚\n",
    "            t1 = time.time()\n",
    "            x_cpu = x.to('cpu')\n",
    "            final_x = self.svd_edge_model.forward_all_layers(x_cpu, attn_weights_list)\n",
    "            edge_time += time.time() - t1\n",
    "            \n",
    "            # å›åˆ°GPUä¸ºä¸‹ä¸€ä¸ªtokenåšå‡†å¤‡\n",
    "            x = final_x.to(self.embed.weight.device)\n",
    "            elements = x.numel() * x.element_size()  # B\n",
    "            net_time += elements / bandwidth / 1024 / 1024\n",
    "\n",
    "        print(f\"ğŸ¯ ç”Ÿæˆé˜¶æ®µï¼šå¼€å§‹ç”Ÿæˆæ–°token...\")\n",
    "        # çœŸå®ç”Ÿæˆé˜¶æ®µ\n",
    "        for token_idx in range(max_length):\n",
    "            if token_idx % 5 == 0:  # æ¯5ä¸ªtokenæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦\n",
    "                print(f\"  ç”Ÿæˆè¿›åº¦: {token_idx}/{max_length}\")\n",
    "                \n",
    "            cur_id = torch.tensor([[outputs[-1]]]).to(self.embed.weight.device)\n",
    "            x = self.embed(cur_id)\n",
    "            \n",
    "            # æ”¶é›†æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›æƒé‡\n",
    "            attn_weights_list = []\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                # use cache-enabled forward so attention spans all previous tokens\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                t0 = time.time()\n",
    "                _, _, attn_weights = self.cloud.forward_cache(x, layer_idx)\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                cloud_time += time.time() - t0\n",
    "\n",
    "                # ç§»åŠ¨åˆ°CPUå¹¶æ”¶é›†æ³¨æ„åŠ›æƒé‡\n",
    "                attn_weights_cpu = attn_weights.to('cpu')\n",
    "                attn_weights_list.append(attn_weights_cpu)\n",
    "                \n",
    "                # è®¡ç®—ç½‘ç»œä¼ è¾“æ—¶é—´\n",
    "                elements = attn_weights_cpu.numel() * attn_weights_cpu.element_size()  # B\n",
    "                net_time += elements / bandwidth / 1024 / 1024\n",
    "                \n",
    "                layer_calls += 1\n",
    "            \n",
    "            # ä½¿ç”¨SVD edgeæ¨¡å‹å¤„ç†æ‰€æœ‰å±‚\n",
    "            t1 = time.time()\n",
    "            x_cpu = x.to('cpu')\n",
    "            final_x = self.svd_edge_model.forward_all_layers(x_cpu, attn_weights_list)\n",
    "            edge_time += time.time() - t1\n",
    "            \n",
    "            # å›åˆ°GPUç»§ç»­\n",
    "            x = final_x.to(self.embed.weight.device)\n",
    "            elements = x.numel() * x.element_size()  # B\n",
    "            net_time += elements / bandwidth / 1024 / 1024\n",
    "            \n",
    "            # final normalization and LM head to get logits\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "            \n",
    "            # ç”¨ top-k + æ¸©åº¦é‡‡æ ·ä»£æ›¿è´ªå¿ƒ argmax\n",
    "            next_logits = logits[:, -1, :] / temperature\n",
    "            topk_vals, topk_idx = torch.topk(next_logits, k=top_k, dim=-1)\n",
    "            probs = torch.softmax(topk_vals, dim=-1)\n",
    "            next_id = topk_idx[0, torch.multinomial(probs, num_samples=1).item()].item()\n",
    "            outputs.append(next_id)\n",
    "            \n",
    "            if next_id == self.tokenizer.eos_token_id:\n",
    "                print(f\"  é‡åˆ°ç»“æŸç¬¦ï¼Œæå‰ç»“æŸç”Ÿæˆ\")\n",
    "                break\n",
    "\n",
    "        print(f\"ğŸ“Š ç”Ÿæˆå®Œæˆï¼Œç»Ÿè®¡æ€§èƒ½æ•°æ®...\")\n",
    "        # æ‰“å°å¹³å‡è€—æ—¶\n",
    "        if layer_calls > 0:\n",
    "            avg_cloud_time = cloud_time / (layer_calls / self.num_layers)\n",
    "            avg_edge_time = edge_time / (layer_calls / self.num_layers)\n",
    "            avg_net_time = net_time / (layer_calls / self.num_layers)\n",
    "            print(f\"â˜ï¸  å¹³å‡GPU(cloud)æ¯tokenè€—æ—¶: {avg_cloud_time:.4f}s\")\n",
    "            print(f\"ğŸ–¥ï¸  å¹³å‡CPU(edge)æ¯tokenè€—æ—¶: {avg_edge_time:.4f}s\")\n",
    "            print(f\"ğŸŒ å¹³å‡ç½‘ç»œä¼ è¾“æ¯tokenè€—æ—¶: {avg_net_time:.4f}s\")\n",
    "            print(f\"ğŸ”„ æ€»å¹³å‡æ¯tokenè€—æ—¶: {avg_cloud_time + avg_edge_time + avg_net_time:.4f}s\")\n",
    "            \n",
    "        return self.tokenizer.decode(outputs, clean_up_tokenization_spaces=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de122dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'AI-ModelScope/gpt-j-6b'\n",
    "device_cloud = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device_edge = 'cpu'\n",
    "\n",
    "# æ£€æŸ¥CUDAå¯ç”¨æ€§\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ® æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œå°†ä½¿ç”¨GPUè¿›è¡Œäº‘ç«¯è®¡ç®—\")\n",
    "    print(f\"ğŸ”§ GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œäº‘ç«¯è®¡ç®—\")\n",
    "\n",
    "# æµ‹è¯•ä¸åŒçš„SVDå‹ç¼©ç‡\n",
    "svd_rate=0\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ§ª æµ‹è¯•SVDå‹ç¼©ç‡: {svd_rate}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "try:\n",
    "    pipeline = GPTJPipeline(\n",
    "        model_name=model_name, \n",
    "        device_cloud=device_cloud, \n",
    "        device_edge=device_edge,\n",
    "        svd_reduce_rate=svd_rate\n",
    "    )\n",
    "    \n",
    "\n",
    "    prompt = \"Once upon a time\"\n",
    "    print(f\"\\nğŸ’¬ æç¤ºè¯: '{prompt}'\")\n",
    "    \n",
    "    print(f\"â±ï¸  å¼€å§‹ç”Ÿæˆæ–‡æœ¬...\")\n",
    "    start_time = time.time()\n",
    "    generated_text = pipeline.generate(prompt, max_length=20)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\nğŸ“ ç”Ÿæˆçš„æ–‡æœ¬:\")\n",
    "    print(f\"   {generated_text}\")\n",
    "    print(f\"â±ï¸  æ€»ç”Ÿæˆæ—¶é—´: {end_time - start_time:.2f}ç§’\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ğŸ SVDå‹ç¼©ç‡ {svd_rate} æµ‹è¯•å®Œæˆ\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d6c18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
