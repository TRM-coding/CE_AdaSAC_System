{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce13ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from modelscope.utils.hub import snapshot_download\n",
    "from detection.Loader.mymodel_file.gptJ_cloud import gptJ_cloud\n",
    "from detection.Loader.mymodel_file.gptJ_edge import gptJ_edge\n",
    "from detection.SVD_model import SVDED_GPTJ_EDGE_Layer\n",
    "\n",
    "class SVD_GPTJ_Edge_Model(nn.Module):\n",
    "    \"\"\"包含所有SVD层的完整edge模型\"\"\"\n",
    "    def __init__(self, original_edge, svd_reduce_rate, device='cpu', svd_device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.svd_device = svd_device\n",
    "        self.num_layers = original_edge.num_layers\n",
    "        self.max_ctx = original_edge.max_ctx\n",
    "        self.v_cache = [None] * self.num_layers\n",
    "        \n",
    "        print(f\"🔄 开始SVD分解处理，压缩率: {svd_reduce_rate}\")\n",
    "        print(f\"📊 总共需要处理 {self.num_layers} 层...\")\n",
    "        print(f\"⚡ SVD分解设备: {svd_device}, 运行设备: {device}\")\n",
    "        \n",
    "        # 用SVD压缩的层替换原始edge层\n",
    "        self.svd_layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            print(f\"  处理第 {i+1}/{self.num_layers} 层: \", end=\"\")\n",
    "            \n",
    "            # 获取原始edge层\n",
    "            if(i%2):\n",
    "                original_edge_layer = original_edge.layers[i]\n",
    "                # 创建SVD压缩层\n",
    "                svd_layer = SVDED_GPTJ_EDGE_Layer(\n",
    "                    gptj_edge_layer=original_edge_layer,\n",
    "                    reduce_rate=0,\n",
    "                    device=device,\n",
    "                    svd_device=svd_device  # 传递SVD分解设备\n",
    "                )\n",
    "                print(\"跳过压缩 (奇数层)\")\n",
    "                self.svd_layers.append(svd_layer)\n",
    "            else:\n",
    "                original_edge_layer = original_edge.layers[i]\n",
    "                # 创建SVD压缩层\n",
    "                print(f\"正在进行SVD分解 (压缩率: {svd_reduce_rate})...\")\n",
    "                \n",
    "                # 计算SVD分解的时间\n",
    "                svd_start_time = time.time()\n",
    "                svd_layer = SVDED_GPTJ_EDGE_Layer(\n",
    "                    gptj_edge_layer=original_edge_layer,\n",
    "                    reduce_rate=svd_reduce_rate,\n",
    "                    device=device,\n",
    "                    svd_device=svd_device  # 传递SVD分解设备\n",
    "                )\n",
    "                svd_end_time = time.time()\n",
    "                print(f\"    ✅ 第 {i+1} 层SVD分解完成 (耗时: {svd_end_time - svd_start_time:.2f}秒)\")\n",
    "                self.svd_layers.append(svd_layer)\n",
    "        \n",
    "        print(f\"🎉 所有层的SVD分解处理完成！\")\n",
    "    \n",
    "    def forward_all_layers(self, x, attn_weights_list):\n",
    "        \"\"\"\n",
    "        处理所有层的前向传播\n",
    "        x: Tensor [batch_size, seq_len, hidden]\n",
    "        attn_weights_list: List[Tensor] - 每层的注意力权重\n",
    "        返回: 最终的x输出\n",
    "        \"\"\"\n",
    "        current_x = x\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            attn_weights = attn_weights_list[layer_idx]\n",
    "            # 使用SVD压缩的层\n",
    "            self.v_cache[layer_idx], current_x = self.svd_layers[layer_idx].forward_cache(\n",
    "                current_x, self.v_cache[layer_idx], attn_weights\n",
    "            )\n",
    "            \n",
    "            # 应用sliding window到缓存\n",
    "            if self.v_cache[layer_idx] is not None and self.v_cache[layer_idx].size(1) > self.max_ctx:\n",
    "                self.v_cache[layer_idx] = self.v_cache[layer_idx][:, -self.max_ctx:, :]\n",
    "        \n",
    "        return current_x\n",
    "\n",
    "class GPTJPipeline:\n",
    "    def __init__(self, model_name='AI-ModelScope/gpt-j-6b', device_cloud='cuda:0', device_edge='cpu', svd_reduce_rate=0.5, use_compile=True):\n",
    "        print(f\"🚀 初始化GPTJPipeline...\")\n",
    "        print(f\"📋 配置信息:\")\n",
    "        print(f\"   - 模型: {model_name}\")\n",
    "        print(f\"   - 云端设备: {device_cloud}\")\n",
    "        print(f\"   - 边缘设备: {device_edge}\")\n",
    "        print(f\"   - SVD压缩率: {svd_reduce_rate}\")\n",
    "        \n",
    "        # 使用 ModelScope 下载模型\n",
    "        print(f\"📥 使用ModelScope下载模型 {model_name}...\")\n",
    "        model_dir = snapshot_download(\n",
    "            repo_id=model_name,\n",
    "            cache_dir='./gpt-j-6b'\n",
    "        )\n",
    "        print(f\"✅ 模型下载完成，路径: {model_dir}\")\n",
    "        \n",
    "        # 使用本地模型路径加载 tokenizer\n",
    "        print(f\"🔤 加载tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "        \n",
    "        # 设置 pad_token 为 eos_token（GPT-J 没有 pad_token）\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(f\"☁️  加载云端模型到 {device_cloud}...\")\n",
    "        self.cloud       = gptJ_cloud(model_name=model_dir).to(device_cloud)\n",
    "        print(f\"🖥️  加载边缘模型到CPU...\")\n",
    "        # 强制 edge 放在 CPU\n",
    "        original_edge    = gptJ_edge (model_name=model_dir).to('cpu')\n",
    "        self.embed       = self.cloud.model.transformer.wte\n",
    "        self.ln_f        = self.cloud.model.transformer.ln_f\n",
    "        self.lm_head     = self.cloud.model.lm_head\n",
    "        self.num_layers  = len(self.cloud.q_weights)\n",
    "        \n",
    "        print(f\"📊 模型共有 {self.num_layers} 层\")\n",
    "        \n",
    "        # SVD压缩参数\n",
    "        self.svd_reduce_rate = svd_reduce_rate\n",
    "        self.use_compile = use_compile\n",
    "        \n",
    "        # 创建整个SVD edge模型\n",
    "        print(f\"🔧 创建SVD边缘模型...\")\n",
    "        # 如果有GPU，先在GPU上进行SVD分解，然后移到CPU\n",
    "        svd_device = device_cloud if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"🔧 SVD分解将在 {svd_device} 上进行...\")\n",
    "        \n",
    "        self.svd_edge_model = SVD_GPTJ_Edge_Model(\n",
    "            original_edge=original_edge,\n",
    "            svd_reduce_rate=svd_reduce_rate,\n",
    "            device='cpu',  # 最终运行在CPU上\n",
    "            svd_device=svd_device  # 但SVD分解在GPU上进行\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ GPTJPipeline初始化完成！\")\n",
    "        print(f\"🎯 准备开始推理，SVD压缩率: {self.svd_reduce_rate}\")\n",
    "    \n",
    "    def generate(self, prompt, max_length=50, temperature=1.0, top_k=50):\n",
    "        print(f\"🔄 开始文本生成...\")\n",
    "        print(f\"📝 输入提示: '{prompt}'\")\n",
    "        print(f\"⚙️  生成参数: max_length={max_length}, temperature={temperature}, top_k={top_k}\")\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')[0].tolist()\n",
    "        outputs   = input_ids.copy()\n",
    "\n",
    "        # reset caches for a fresh generation\n",
    "        print(f\"🗂️  清空缓存...\")\n",
    "        for i in range(self.num_layers):\n",
    "            self.cloud.k_cache[i] = None\n",
    "            self.svd_edge_model.v_cache[i] = None\n",
    "\n",
    "        # 统计变量\n",
    "        cloud_time = 0.0\n",
    "        edge_time  = 0.0\n",
    "        layer_calls= 0\n",
    "        net_time=0.0\n",
    "        bandwidth=10 #MB/s\n",
    "\n",
    "        # 上下文窗口大小\n",
    "        max_ctx = self.cloud.max_ctx\n",
    "\n",
    "        print(f\"🔥 预热阶段：处理 {len(input_ids)} 个提示token...\")\n",
    "        # 预热缓存：将 prompt 中每个 token 走一次 forward_cache\n",
    "        for pos, token_id in enumerate(input_ids):\n",
    "            print(f\"  处理提示token {pos+1}/{len(input_ids)}\")\n",
    "            # clamp 位置，防止越界\n",
    "            pos_clamped = pos if pos < max_ctx else max_ctx - 1\n",
    "            cur_id = torch.tensor([[token_id]]).to(self.embed.weight.device)\n",
    "            \n",
    "            # GPT-J 没有位置embedding，直接使用 token embedding\n",
    "            x = self.embed(cur_id)\n",
    "            \n",
    "            # 收集所有层的注意力权重\n",
    "            attn_weights_list = []\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                # cloud on GPU\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                t0 = time.time()\n",
    "                _, _, attn_weights = self.cloud.forward_cache(x, layer_idx)\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                cloud_time += time.time() - t0\n",
    "                \n",
    "                # 移动到CPU并收集注意力权重\n",
    "                attn_weights_cpu = attn_weights.to('cpu')\n",
    "                attn_weights_list.append(attn_weights_cpu)\n",
    "                \n",
    "                # 计算网络传输时间\n",
    "                elements = attn_weights_cpu.numel() * attn_weights_cpu.element_size()  # B\n",
    "                net_time += elements / bandwidth / 1024 / 1024  # s\n",
    "                \n",
    "                layer_calls += 1\n",
    "            \n",
    "            # 使用SVD edge模型处理所有层\n",
    "            t1 = time.time()\n",
    "            x_cpu = x.to('cpu')\n",
    "            final_x = self.svd_edge_model.forward_all_layers(x_cpu, attn_weights_list)\n",
    "            edge_time += time.time() - t1\n",
    "            \n",
    "            # 回到GPU为下一个token做准备\n",
    "            x = final_x.to(self.embed.weight.device)\n",
    "            elements = x.numel() * x.element_size()  # B\n",
    "            net_time += elements / bandwidth / 1024 / 1024\n",
    "\n",
    "        print(f\"🎯 生成阶段：开始生成新token...\")\n",
    "        # 真实生成阶段\n",
    "        for token_idx in range(max_length):\n",
    "            if token_idx % 5 == 0:  # 每5个token显示一次进度\n",
    "                print(f\"  生成进度: {token_idx}/{max_length}\")\n",
    "                \n",
    "            cur_id = torch.tensor([[outputs[-1]]]).to(self.embed.weight.device)\n",
    "            x = self.embed(cur_id)\n",
    "            \n",
    "            # 收集所有层的注意力权重\n",
    "            attn_weights_list = []\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                # use cache-enabled forward so attention spans all previous tokens\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                t0 = time.time()\n",
    "                _, _, attn_weights = self.cloud.forward_cache(x, layer_idx)\n",
    "                if hasattr(torch.cuda, 'synchronize'):\n",
    "                    torch.cuda.synchronize()\n",
    "                cloud_time += time.time() - t0\n",
    "\n",
    "                # 移动到CPU并收集注意力权重\n",
    "                attn_weights_cpu = attn_weights.to('cpu')\n",
    "                attn_weights_list.append(attn_weights_cpu)\n",
    "                \n",
    "                # 计算网络传输时间\n",
    "                elements = attn_weights_cpu.numel() * attn_weights_cpu.element_size()  # B\n",
    "                net_time += elements / bandwidth / 1024 / 1024\n",
    "                \n",
    "                layer_calls += 1\n",
    "            \n",
    "            # 使用SVD edge模型处理所有层\n",
    "            t1 = time.time()\n",
    "            x_cpu = x.to('cpu')\n",
    "            final_x = self.svd_edge_model.forward_all_layers(x_cpu, attn_weights_list)\n",
    "            edge_time += time.time() - t1\n",
    "            \n",
    "            # 回到GPU继续\n",
    "            x = final_x.to(self.embed.weight.device)\n",
    "            elements = x.numel() * x.element_size()  # B\n",
    "            net_time += elements / bandwidth / 1024 / 1024\n",
    "            \n",
    "            # final normalization and LM head to get logits\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "            \n",
    "            # 用 top-k + 温度采样代替贪心 argmax\n",
    "            next_logits = logits[:, -1, :] / temperature\n",
    "            topk_vals, topk_idx = torch.topk(next_logits, k=top_k, dim=-1)\n",
    "            probs = torch.softmax(topk_vals, dim=-1)\n",
    "            next_id = topk_idx[0, torch.multinomial(probs, num_samples=1).item()].item()\n",
    "            outputs.append(next_id)\n",
    "            \n",
    "            if next_id == self.tokenizer.eos_token_id:\n",
    "                print(f\"  遇到结束符，提前结束生成\")\n",
    "                break\n",
    "\n",
    "        print(f\"📊 生成完成，统计性能数据...\")\n",
    "        # 打印平均耗时\n",
    "        if layer_calls > 0:\n",
    "            avg_cloud_time = cloud_time / (layer_calls / self.num_layers)\n",
    "            avg_edge_time = edge_time / (layer_calls / self.num_layers)\n",
    "            avg_net_time = net_time / (layer_calls / self.num_layers)\n",
    "            print(f\"☁️  平均GPU(cloud)每token耗时: {avg_cloud_time:.4f}s\")\n",
    "            print(f\"🖥️  平均CPU(edge)每token耗时: {avg_edge_time:.4f}s\")\n",
    "            print(f\"🌐 平均网络传输每token耗时: {avg_net_time:.4f}s\")\n",
    "            print(f\"🔄 总平均每token耗时: {avg_cloud_time + avg_edge_time + avg_net_time:.4f}s\")\n",
    "            \n",
    "        return self.tokenizer.decode(outputs, clean_up_tokenization_spaces=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de122dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'AI-ModelScope/gpt-j-6b'\n",
    "device_cloud = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device_edge = 'cpu'\n",
    "\n",
    "# 检查CUDA可用性\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎮 检测到CUDA设备，将使用GPU进行云端计算\")\n",
    "    print(f\"🔧 GPU设备: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"⚠️  未检测到CUDA设备，将使用CPU进行云端计算\")\n",
    "\n",
    "# 测试不同的SVD压缩率\n",
    "svd_rate=0\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"🧪 测试SVD压缩率: {svd_rate}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "try:\n",
    "    pipeline = GPTJPipeline(\n",
    "        model_name=model_name, \n",
    "        device_cloud=device_cloud, \n",
    "        device_edge=device_edge,\n",
    "        svd_reduce_rate=svd_rate\n",
    "    )\n",
    "    \n",
    "\n",
    "    prompt = \"Once upon a time\"\n",
    "    print(f\"\\n💬 提示词: '{prompt}'\")\n",
    "    \n",
    "    print(f\"⏱️  开始生成文本...\")\n",
    "    start_time = time.time()\n",
    "    generated_text = pipeline.generate(prompt, max_length=20)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n📝 生成的文本:\")\n",
    "    print(f\"   {generated_text}\")\n",
    "    print(f\"⏱️  总生成时间: {end_time - start_time:.2f}秒\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 测试失败: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"🏁 SVD压缩率 {svd_rate} 测试完成\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n🎉 所有测试完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d6c18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
