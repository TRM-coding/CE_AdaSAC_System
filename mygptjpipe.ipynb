{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1819ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu126/fused_svd_matmul/build.ninja...\n",
      "Building extension module fused_svd_matmul...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_svd_matmul...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import tracemalloc\n",
    "from transformers import AutoTokenizer\n",
    "from modelscope.utils.hub import snapshot_download\n",
    "from detection.Loader.mymodel_file.gptJ_cloud import gptJ_cloud\n",
    "from detection.Loader.mymodel_file.gptJ_edge import gptJ_edge\n",
    "from detection.SVD_model import SVDED_GPTJ_EDGE_Layer\n",
    "from detection.MINI_PIPE_EVAL import load_and_tokenize_dataset\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"æ€§èƒ½ç›‘æ§ç±»ï¼Œç”¨äºè®°å½•CPUæ—¶é—´å’Œå†…å­˜ä½¿ç”¨æƒ…å†µ\"\"\"\n",
    "    def __init__(self):\n",
    "        self.process = psutil.Process(os.getpid())\n",
    "        self.reset_stats()\n",
    "        \n",
    "    def reset_stats(self):\n",
    "        \"\"\"é‡ç½®ç»Ÿè®¡æ•°æ®\"\"\"\n",
    "        self.cloud_gpu_times = []\n",
    "        self.edge_cpu_times = []\n",
    "        self.network_times = []\n",
    "        self.memory_snapshots = []\n",
    "        self.token_count = 0\n",
    "        self.layer_calls = 0\n",
    "        \n",
    "        # è¯¦ç»†çš„è®¡æ—¶ç»Ÿè®¡\n",
    "        self.cloud_total_time = 0.0\n",
    "        self.edge_total_time = 0.0\n",
    "        self.network_total_time = 0.0\n",
    "        \n",
    "        # å†…å­˜ç»Ÿè®¡\n",
    "        self.initial_memory = self.get_memory_mb()\n",
    "        self.peak_memory = self.initial_memory\n",
    "        \n",
    "    def get_memory_mb(self):\n",
    "        \"\"\"è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)\"\"\"\n",
    "        return self.process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    def get_cpu_percent(self):\n",
    "        \"\"\"è·å–CPUä½¿ç”¨ç‡\"\"\"\n",
    "        return self.process.cpu_percent()\n",
    "    \n",
    "    def start_memory_tracking(self):\n",
    "        \"\"\"å¼€å§‹å†…å­˜è·Ÿè¸ª\"\"\"\n",
    "        tracemalloc.start()\n",
    "        \n",
    "    def stop_memory_tracking(self):\n",
    "        \"\"\"åœæ­¢å†…å­˜è·Ÿè¸ªå¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        if tracemalloc.is_tracing():\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            return current / 1024 / 1024, peak / 1024 / 1024  # è½¬æ¢ä¸ºMB\n",
    "        return 0, 0\n",
    "    \n",
    "    def record_cloud_time(self, time_taken):\n",
    "        \"\"\"è®°å½•äº‘ç«¯GPUæ—¶é—´\"\"\"\n",
    "        self.cloud_gpu_times.append(time_taken)\n",
    "        self.cloud_total_time += time_taken\n",
    "        \n",
    "    def record_edge_time(self, time_taken):\n",
    "        \"\"\"è®°å½•è¾¹ç¼˜CPUæ—¶é—´\"\"\"\n",
    "        self.edge_cpu_times.append(time_taken)\n",
    "        self.edge_total_time += time_taken\n",
    "        \n",
    "    def record_network_time(self, time_taken):\n",
    "        \"\"\"è®°å½•ç½‘ç»œä¼ è¾“æ—¶é—´\"\"\"\n",
    "        self.network_times.append(time_taken)\n",
    "        self.network_total_time += time_taken\n",
    "        \n",
    "    def record_memory_snapshot(self, phase=\"\"):\n",
    "        \"\"\"è®°å½•å†…å­˜å¿«ç…§\"\"\"\n",
    "        current_memory = self.get_memory_mb()\n",
    "        self.memory_snapshots.append({\n",
    "            'phase': phase,\n",
    "            'memory_mb': current_memory,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        self.peak_memory = max(self.peak_memory, current_memory)\n",
    "        \n",
    "    def increment_counters(self):\n",
    "        \"\"\"å¢åŠ è®¡æ•°å™¨\"\"\"\n",
    "        self.layer_calls += 1\n",
    "        \n",
    "    def increment_token_count(self):\n",
    "        \"\"\"å¢åŠ tokenè®¡æ•°\"\"\"\n",
    "        self.token_count += 1\n",
    "        \n",
    "    def print_detailed_report(self):\n",
    "        \"\"\"æ‰“å°è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ” è¯¦ç»†æ€§èƒ½åˆ†ææŠ¥å‘Š\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # åŸºæœ¬ç»Ÿè®¡\n",
    "        print(f\"ğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "        print(f\"   ğŸ”¢ å¤„ç†çš„Tokenæ•°é‡: {self.token_count}\")\n",
    "        print(f\"   ğŸ”¢ æ€»å±‚è°ƒç”¨æ¬¡æ•°: {self.layer_calls}\")\n",
    "        print(f\"   ğŸ”¢ å¹³å‡æ¯tokenå±‚è°ƒç”¨: {self.layer_calls/max(1, self.token_count):.1f}\")\n",
    "        \n",
    "        # æ—¶é—´ç»Ÿè®¡\n",
    "        print(f\"\\nâ±ï¸  æ—¶é—´ç»Ÿè®¡ (æ€»è®¡):\")\n",
    "        print(f\"   â˜ï¸  GPUäº‘ç«¯æ€»æ—¶é—´: {self.cloud_total_time:.4f}s\")\n",
    "        print(f\"   ğŸ–¥ï¸  CPUè¾¹ç¼˜æ€»æ—¶é—´: {self.edge_total_time:.4f}s\")\n",
    "        print(f\"   ğŸŒ ç½‘ç»œä¼ è¾“æ€»æ—¶é—´: {self.network_total_time:.4f}s\")\n",
    "        print(f\"   ğŸ”„ æ€»å¤„ç†æ—¶é—´: {self.cloud_total_time + self.edge_total_time + self.network_total_time:.4f}s\")\n",
    "        \n",
    "        # å¹³å‡æ—¶é—´ç»Ÿè®¡\n",
    "        if self.token_count > 0:\n",
    "            print(f\"\\nâ±ï¸  å¹³å‡æ¯Tokenæ—¶é—´:\")\n",
    "            print(f\"   â˜ï¸  GPUäº‘ç«¯å¹³å‡: {self.cloud_total_time/self.token_count:.4f}s\")\n",
    "            print(f\"   ğŸ–¥ï¸  CPUè¾¹ç¼˜å¹³å‡: {self.edge_total_time/self.token_count:.4f}s\")\n",
    "            print(f\"   ğŸŒ ç½‘ç»œä¼ è¾“å¹³å‡: {self.network_total_time/self.token_count:.4f}s\")\n",
    "            print(f\"   ğŸ”„ æ€»å¹³å‡: {(self.cloud_total_time + self.edge_total_time + self.network_total_time)/self.token_count:.4f}s\")\n",
    "        \n",
    "        # å†…å­˜ç»Ÿè®¡\n",
    "        current_memory = self.get_memory_mb()\n",
    "        memory_diff = current_memory - self.initial_memory\n",
    "        print(f\"\\nğŸ’¾ å†…å­˜ä½¿ç”¨ç»Ÿè®¡:\")\n",
    "        print(f\"   ğŸ“ˆ åˆå§‹å†…å­˜: {self.initial_memory:.2f}MB\")\n",
    "        print(f\"   ğŸ“Š å½“å‰å†…å­˜: {current_memory:.2f}MB\")\n",
    "        print(f\"   ğŸ“ˆ å³°å€¼å†…å­˜: {self.peak_memory:.2f}MB\")\n",
    "        print(f\"   ğŸ“Š å†…å­˜å˜åŒ–: {memory_diff:+.2f}MB\")\n",
    "        \n",
    "        # CPUä½¿ç”¨ç‡\n",
    "        cpu_percent = self.get_cpu_percent()\n",
    "        print(f\"   ğŸ”¥ å½“å‰CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%\")\n",
    "        \n",
    "        # è·å–å†…å­˜è·Ÿè¸ªä¿¡æ¯\n",
    "        if hasattr(self, '_tracemalloc_peak'):\n",
    "            print(f\"   ğŸ” å†…å­˜è·Ÿè¸ªå³°å€¼: {self._tracemalloc_peak:.2f}MB\")\n",
    "        \n",
    "        # æ—¶é—´åˆ†å¸ƒåˆ†æ\n",
    "        if len(self.cloud_gpu_times) > 0:\n",
    "            print(f\"\\nğŸ“ˆ GPUæ—¶é—´åˆ†å¸ƒ:\")\n",
    "            print(f\"   æœ€å°: {min(self.cloud_gpu_times):.4f}s\")\n",
    "            print(f\"   æœ€å¤§: {max(self.cloud_gpu_times):.4f}s\")\n",
    "            print(f\"   å¹³å‡: {sum(self.cloud_gpu_times)/len(self.cloud_gpu_times):.4f}s\")\n",
    "            \n",
    "        if len(self.edge_cpu_times) > 0:\n",
    "            print(f\"\\nğŸ“ˆ CPUæ—¶é—´åˆ†å¸ƒ:\")\n",
    "            print(f\"   æœ€å°: {min(self.edge_cpu_times):.4f}s\")\n",
    "            print(f\"   æœ€å¤§: {max(self.edge_cpu_times):.4f}s\")\n",
    "            print(f\"   å¹³å‡: {sum(self.edge_cpu_times)/len(self.edge_cpu_times):.4f}s\")\n",
    "        \n",
    "        # æ€§èƒ½æ¯”è¾ƒ\n",
    "        if self.cloud_total_time > 0 and self.edge_total_time > 0:\n",
    "            ratio = self.edge_total_time / self.cloud_total_time\n",
    "            print(f\"\\nğŸ” æ€§èƒ½æ¯”è¾ƒ:\")\n",
    "            print(f\"   CPU/GPUæ—¶é—´æ¯”: {ratio:.2f}x\")\n",
    "            if ratio > 1:\n",
    "                print(f\"   ğŸ’¡ CPUæ¯”GPUæ…¢ {ratio:.1f} å€\")\n",
    "            else:\n",
    "                print(f\"   ğŸ’¡ CPUæ¯”GPUå¿« {1/ratio:.1f} å€\")\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "    def print_memory_timeline(self):\n",
    "        \"\"\"æ‰“å°å†…å­˜ä½¿ç”¨æ—¶é—´çº¿\"\"\"\n",
    "        if len(self.memory_snapshots) > 0:\n",
    "            print(f\"\\nğŸ“Š å†…å­˜ä½¿ç”¨æ—¶é—´çº¿:\")\n",
    "            for i, snapshot in enumerate(self.memory_snapshots):\n",
    "                print(f\"   {i+1}. {snapshot['phase']}: {snapshot['memory_mb']:.2f}MB\")\n",
    "                \n",
    "    def get_summary_stats(self):\n",
    "        \"\"\"è¿”å›æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            'cloud_total_time': self.cloud_total_time,\n",
    "            'edge_total_time': self.edge_total_time,\n",
    "            'network_total_time': self.network_total_time,\n",
    "            'token_count': self.token_count,\n",
    "            'layer_calls': self.layer_calls,\n",
    "            'memory_usage_mb': self.get_memory_mb(),\n",
    "            'memory_peak_mb': self.peak_memory,\n",
    "            'memory_diff_mb': self.get_memory_mb() - self.initial_memory\n",
    "        }\n",
    "\n",
    "class SVD_GPTJ_Edge_Model(nn.Module):\n",
    "    \"\"\"åŒ…å«æ‰€æœ‰SVDå±‚çš„å®Œæ•´edgeæ¨¡å‹ï¼Œå…¼å®¹åŸå§‹edgeæ¨¡å‹æ¥å£\"\"\"\n",
    "    def __init__(self, original_edge, svd_reduce_rate, device='cpu', svd_device='cpu',No_init=False):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.svd_device = svd_device\n",
    "        self.num_layers = original_edge.num_layers\n",
    "        self.max_ctx = original_edge.max_ctx\n",
    "        self.v_cache = [None] * self.num_layers\n",
    "        \n",
    "        print(f\"ğŸ”„ å¼€å§‹SVDåˆ†è§£å¤„ç†ï¼Œå‹ç¼©ç‡: {svd_reduce_rate}\")\n",
    "        print(f\"ğŸ“Š æ€»å…±éœ€è¦å¤„ç† {self.num_layers} å±‚...\")\n",
    "        print(f\"âš¡ SVDåˆ†è§£è®¾å¤‡: {svd_device}, è¿è¡Œè®¾å¤‡: {device}\")\n",
    "        \n",
    "        # ç”¨SVDå‹ç¼©çš„å±‚æ›¿æ¢åŸå§‹edgeå±‚\n",
    "        self.svd_layers = nn.ModuleList()\n",
    "        if(not No_init):\n",
    "            for i in range(self.num_layers):\n",
    "                print(f\"  å¤„ç†ç¬¬ {i+1}/{self.num_layers} å±‚: \", end=\"\")\n",
    "                original_edge_layer = original_edge.layers[i]\n",
    "                    # å¥‡æ•°å±‚è·³è¿‡å‹ç¼©\n",
    "                if isinstance(svd_reduce_rate, list):\n",
    "\n",
    "                    svd_layer = SVDED_GPTJ_EDGE_Layer(\n",
    "                        gptj_edge_layer=original_edge_layer,\n",
    "                        reduce_rate=svd_reduce_rate[i],\n",
    "                        device=device,\n",
    "                        svd_device=svd_device\n",
    "                    )\n",
    "                else:\n",
    "                    svd_layer = SVDED_GPTJ_EDGE_Layer(\n",
    "                        gptj_edge_layer=original_edge_layer,\n",
    "                        reduce_rate=svd_reduce_rate,\n",
    "                        device=device,\n",
    "                        svd_device=svd_device\n",
    "                    )\n",
    "                print(\"è·³è¿‡å‹ç¼© (å¥‡æ•°å±‚)\")\n",
    "                self.svd_layers.append(svd_layer)\n",
    "        \n",
    "        print(f\"ğŸ‰ æ‰€æœ‰å±‚çš„SVDåˆ†è§£å¤„ç†å®Œæˆï¼\")\n",
    "    \n",
    "    def forward_no_cache(self,x,layer_idx,attn_weights):\n",
    "        output=self.svd_layers[layer_idx].forward_no_cache(\n",
    "            x,  attn_weights\n",
    "        )\n",
    "        return output\n",
    "    \n",
    "    def forward_cache(self, x, layer_idx, attn_weights):\n",
    "        \"\"\"\n",
    "        å…¼å®¹åŸå§‹edgeæ¨¡å‹çš„forward_cacheæ¥å£\n",
    "        Args:\n",
    "            x: è¾“å…¥tensor\n",
    "            layer_idx: å±‚ç´¢å¼•\n",
    "            attn_weights: æ³¨æ„åŠ›æƒé‡\n",
    "        Returns:\n",
    "            tuple: (v_cache, output_x) - ä¸åŸå§‹edgeæ¨¡å‹ç›¸åŒçš„è¿”å›æ ¼å¼\n",
    "        \"\"\"\n",
    "        # ä½¿ç”¨SVDå‹ç¼©çš„å±‚è¿›è¡Œå‰å‘ä¼ æ’­\n",
    "        # tim1=time.time()\n",
    "        self.v_cache[layer_idx], output_x = self.svd_layers[layer_idx].forward_cache(\n",
    "            x, self.v_cache[layer_idx], attn_weights\n",
    "        )\n",
    "        # tim2=time.time()\n",
    "        # print(f\"layer_{layer_idx}_forward_time:\",tim2-tim1)\n",
    "        \n",
    "        # åº”ç”¨sliding windowåˆ°ç¼“å­˜\n",
    "        if self.v_cache[layer_idx] is not None and self.v_cache[layer_idx].size(1) > self.max_ctx:\n",
    "            self.v_cache[layer_idx] = self.v_cache[layer_idx][:, -self.max_ctx:, :]\n",
    "        # tim3=time.time()\n",
    "        # print(f\"layer_{layer_idx}_memory_time:\",tim3-tim2)\n",
    "\n",
    "        return self.v_cache[layer_idx], output_x\n",
    "\n",
    "class GPTJPipeline(nn.Module):\n",
    "    def __init__(self, model_name='AI-ModelScope/gpt-j-6b', device_cloud='cuda:0', device_edge='cpu', svd_reduce_rate=0.5, use_compile=True,edge=None):\n",
    "        super(GPTJPipeline, self).__init__()\n",
    "        print(f\"ğŸš€ åˆå§‹åŒ–GPTJPipeline...\")\n",
    "        print(f\"ğŸ“‹ é…ç½®ä¿¡æ¯:\")\n",
    "        print(f\"   - æ¨¡å‹: {model_name}\")\n",
    "        print(f\"   - äº‘ç«¯è®¾å¤‡: {device_cloud}\")\n",
    "        print(f\"   - è¾¹ç¼˜è®¾å¤‡: {device_edge}\")\n",
    "        print(f\"   - SVDå‹ç¼©ç‡: {svd_reduce_rate}\")\n",
    "        \n",
    "        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨\n",
    "        self.performance_monitor = PerformanceMonitor()\n",
    "        \n",
    "        # ä½¿ç”¨ ModelScope ä¸‹è½½æ¨¡å‹\n",
    "        print(f\"ğŸ“¥ ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹ {model_name}...\")\n",
    "        model_dir = snapshot_download(\n",
    "            repo_id=model_name,\n",
    "            cache_dir='./gpt-j-6b'\n",
    "        )\n",
    "        print(f\"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œè·¯å¾„: {model_dir}\")\n",
    "        \n",
    "        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„åŠ è½½ tokenizer\n",
    "        print(f\"ğŸ”¤ åŠ è½½tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "        \n",
    "        # è®¾ç½® pad_token ä¸º eos_tokenï¼ˆGPT-J æ²¡æœ‰ pad_tokenï¼‰\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(f\"â˜ï¸  åŠ è½½äº‘ç«¯æ¨¡å‹åˆ° {device_cloud}...\")\n",
    "        self.cloud = gptJ_cloud(model_name=model_dir).to(device_cloud)\n",
    "        print(f\"ğŸ–¥ï¸  åŠ è½½è¾¹ç¼˜æ¨¡å‹åˆ°CPU...\")\n",
    "        # å¼ºåˆ¶ edge æ”¾åœ¨ CPU\n",
    "        original_edge = gptJ_edge(model_name=model_dir).to('cpu')\n",
    "        self.embed = self.cloud.model.transformer.wte\n",
    "        self.ln_f = self.cloud.model.transformer.ln_f\n",
    "        self.lm_head = self.cloud.model.lm_head\n",
    "        self.num_layers = len(self.cloud.q_weights)\n",
    "        \n",
    "        print(f\"ğŸ“Š æ¨¡å‹å…±æœ‰ {self.num_layers} å±‚\")\n",
    "        \n",
    "        # SVDå‹ç¼©å‚æ•°\n",
    "        self.svd_reduce_rate = svd_reduce_rate\n",
    "        self.use_compile = use_compile\n",
    "        \n",
    "        # åˆ›å»ºæ•´ä¸ªSVD edgeæ¨¡å‹\n",
    "        print(f\"ğŸ”§ åˆ›å»ºSVDè¾¹ç¼˜æ¨¡å‹...\")\n",
    "        # å¦‚æœæœ‰GPUï¼Œå…ˆåœ¨GPUä¸Šè¿›è¡ŒSVDåˆ†è§£ï¼Œç„¶åç§»åˆ°CPU\n",
    "        svd_device = device_cloud if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"ğŸ”§ SVDåˆ†è§£å°†åœ¨ {svd_device} ä¸Šè¿›è¡Œ...\")\n",
    "        \n",
    "        if(svd_reduce_rate!=-1):\n",
    "            self.edge = SVD_GPTJ_Edge_Model(\n",
    "                original_edge=original_edge,\n",
    "                svd_reduce_rate=svd_reduce_rate,\n",
    "                device='cpu',  # æœ€ç»ˆè¿è¡Œåœ¨CPUä¸Š\n",
    "                svd_device=svd_device  # ä½†SVDåˆ†è§£åœ¨GPUä¸Šè¿›è¡Œ\n",
    "            )\n",
    "        else:\n",
    "            self.edge=self.edge = SVD_GPTJ_Edge_Model(\n",
    "                original_edge=original_edge,\n",
    "                svd_reduce_rate=svd_reduce_rate,\n",
    "                device='cpu',  # æœ€ç»ˆè¿è¡Œåœ¨CPUä¸Š\n",
    "                svd_device=svd_device,  # ä½†SVDåˆ†è§£åœ¨GPUä¸Šè¿›è¡Œ\n",
    "                No_init=True\n",
    "            )\n",
    "        \n",
    "        print(f\"âœ… GPTJPipelineåˆå§‹åŒ–å®Œæˆï¼\")\n",
    "        print(f\"ğŸ¯ å‡†å¤‡å¼€å§‹æ¨ç†ï¼ŒSVDå‹ç¼©ç‡: {self.svd_reduce_rate}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # def forward(self, input_ids, attention_mask=None):\n",
    "    #     \"\"\"\n",
    "    #     Forward pass with RoPE and causal mask support,\n",
    "    #     using cloud/edge split with caching.\n",
    "    #     Args:\n",
    "    #         input_ids (LongTensor): [B, T] input token IDs.\n",
    "    #         attention_mask (BoolTensor, optional): [B, T] mask for valid tokens.\n",
    "    #     Returns:\n",
    "    #         Tensor: logits of shape [B, T, vocab_size]\n",
    "    #     \"\"\"\n",
    "    #     # Reset caches for a fresh generation or evaluation\n",
    "    #     for i in range(self.num_layers):\n",
    "    #         self.cloud.k_cache[i] = None\n",
    "    #         self.edge.v_cache[i] = None\n",
    "\n",
    "    #     B, T = input_ids.size()\n",
    "    #     device = input_ids.device\n",
    "\n",
    "    #     # Token embeddings (GPT-J uses RoPE internally, no explicit positional embedding)\n",
    "    #     x = self.embed(input_ids)  # [B, T, D]\n",
    "\n",
    "    #     # Prepare attention mask: combine causal mask with token mask\n",
    "    #     if attention_mask is None:\n",
    "    #         attention_mask = torch.ones((B, T), device=device)\n",
    "    #     # Expand token mask to [B, 1, T] for broadcasting\n",
    "    #     token_mask = attention_mask.unsqueeze(1)  # [B, 1, T]\n",
    "    #     # Causal mask [1, T, T]\n",
    "    #     causal = torch.tril(torch.ones((T, T), device=device)).unsqueeze(0)\n",
    "    #     # Final mask [B, T, T]: broadcast token_mask along query dimension\n",
    "    #     attn_mask = token_mask * causal  # [B, T, T]\n",
    "\n",
    "    #     # Cloud-Edge split forward\n",
    "    #     for layer_idx in range(self.num_layers):\n",
    "    #         # Move hidden and mask to cloud device\n",
    "    #         x_cloud = x.to(self.cloud.device)\n",
    "    #         mask_cloud = attn_mask.to(self.cloud.device)\n",
    "    #         # cloud forward: returns (hidden, kv_cache, attn_weights)\n",
    "    #         x_cloud, _, attn_w = self.cloud.forward_cache(\n",
    "    #             x_cloud, layer_idx\n",
    "    #         )\n",
    "    #         # Move to edge for remaining computation\n",
    "    #         x_edge = x_cloud.to(self.edge.device)\n",
    "    #         attn_edge = attn_w.to(self.edge.device)\n",
    "    #         x_edge, _ = self.edge.forward_cache(\n",
    "    #             x_edge, layer_idx, attn_weights=attn_edge\n",
    "    #         )\n",
    "    #         # Bring back to main device\n",
    "    #         x = x_edge.to(device)\n",
    "\n",
    "    #     # Final normalization and LM head\n",
    "    #     x = self.ln_f(x)           # [B, T, D]\n",
    "    #     logits = self.lm_head(x)   # [B, T, vocab_size]\n",
    "    #     return logits\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # 1. ç”Ÿæˆ padding mask: pad_token_id ä½ç½®ä¸º 0ï¼Œå…¶å®ƒä¸º 1\n",
    "        #    å‡è®¾ self.config.pad_token_id å·²ç»è¢«è®¾ç½®\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()  # [B, T]\n",
    "\n",
    "        # Reset caches for a fresh generation\n",
    "        for i in range(self.num_layers):\n",
    "            self.cloud.k_cache[i] = None\n",
    "            self.edge.v_cache[i] = None\n",
    "\n",
    "        # Statistics variables\n",
    "        cloud_time = edge_time = net_time = 0.0\n",
    "        layer_calls = 0\n",
    "        bandwidth = 10  # MB/s\n",
    "\n",
    "        # Embedding\n",
    "        x = self.embed(input_ids)  # [B, T, D]\n",
    "\n",
    "        # å±‚çº§è¿­ä»£\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            # Cloud forwardï¼šä¼ å…¥ attention_maskï¼Œç”¨äºå†…éƒ¨åš pad+causal å±è”½\n",
    "            if hasattr(torch.cuda, 'synchronize'):\n",
    "                torch.cuda.synchronize()\n",
    "            t0 = time.time()\n",
    "            _, _, attn_weights = self.cloud.forward_cache(x, layer_idx, attention_mask)\n",
    "            if hasattr(torch.cuda, 'synchronize'):\n",
    "                torch.cuda.synchronize()\n",
    "            cloud_time += time.time() - t0\n",
    "\n",
    "            # Edge forwardï¼ˆä¿æŒä¸å˜ï¼‰\n",
    "            x_cpu = x.to('cuda:0')\n",
    "            attn_cpu = attn_weights.to('cuda:0')\n",
    "            t1 = time.time()\n",
    "            _, x_cpu = self.edge.forward_cache(x_cpu, layer_idx, attn_cpu)\n",
    "            edge_time += time.time() - t1\n",
    "\n",
    "            # ç½‘ç»œå¼€é”€ä¼°ç®—\n",
    "            elements = attn_cpu.numel() * attn_cpu.element_size()\n",
    "            net_time += elements / bandwidth / 1024 / 1024\n",
    "            x = x_cpu.to(self.embed.weight.device)\n",
    "            elements = x.numel() * x.element_size()\n",
    "            net_time += elements / bandwidth / 1024 / 1024\n",
    "\n",
    "            layer_calls += 1\n",
    "\n",
    "        # Final normalization and LM head to get logits\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    # def forward (self, input_ids):\n",
    "    #     outputs = input_ids\n",
    "\n",
    "    #     # Reset caches for a fresh generation\n",
    "    #     for i in range (self.num_layers):\n",
    "    #         self.cloud.k_cache [i] = None\n",
    "    #         self.edge.v_cache [i] = None\n",
    "\n",
    "    #     # Statistics variables\n",
    "    #     cloud_time = 0.0\n",
    "    #     edge_time = 0.0\n",
    "    #     layer_calls = 0\n",
    "    #     net_time = 0.0\n",
    "    #     bandwidth = 10  # MB/s\n",
    "\n",
    "    #     # Context window size\n",
    "    #     max_ctx = self.cloud.max_ctx\n",
    "    #     x = self.embed(outputs)\n",
    "    #     # Process the input sequence step by step for causal language modeling\n",
    "    #     # for token_idx in range (outputs.size(1) - 1):  # Exclude the last token for target generation\n",
    "    #         # For each token in the sequence, we use the preceding tokens for input\n",
    "    #         # cur_input = outputs [:, token_idx].unsqueeze(-1)  # Take current token as input\n",
    "            \n",
    "\n",
    "    #     for layer_idx in range (self.num_layers):\n",
    "    #         # Use cache-enabled forward so attention spans all previous tokens\n",
    "    #         if hasattr (torch.cuda, 'synchronize'):\n",
    "    #             torch.cuda.synchronize()\n",
    "    #         t0 = time.time()\n",
    "    #         _, _, attn_weights = self.cloud.forward_cache (x, layer_idx)\n",
    "    #         if hasattr (torch.cuda, 'synchronize'):\n",
    "    #             torch.cuda.synchronize()\n",
    "    #         cloud_time += time.time() - t0\n",
    "\n",
    "    #         x_cpu = x.to('cuda:0')\n",
    "    #         attn_cpu = attn_weights.to('cuda:0')\n",
    "    #         t1 = time.time()\n",
    "    #         _, x_cpu = self.edge.forward_cache (x_cpu, layer_idx, attn_cpu)\n",
    "    #         edge_time += time.time() - t1\n",
    "\n",
    "    #         elements = attn_cpu.numel() * attn_cpu.element_size ()  # B\n",
    "    #         net_time += elements / bandwidth / 1024 / 1024\n",
    "    #         x = x_cpu.to(self.embed.weight.device)\n",
    "    #         elements = x.numel() * x.element_size ()  # B\n",
    "    #         net_time += elements / bandwidth / 1024 / 1024\n",
    "    #         layer_calls += 1\n",
    "\n",
    "    #     # Final normalization and LM head to get logits\n",
    "    #     x = self.ln_f (x)\n",
    "    #     logits = self.lm_head (x)\n",
    "\n",
    "    #     return logits\n",
    "\n",
    "    def generate(self, prompt, max_length=50, temperature=1.0, top_k=50):\n",
    "        \"\"\"\n",
    "        è°ƒç”¨ forward æ–¹æ³•ç”Ÿæˆæ–‡æœ¬\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')[0].tolist()\n",
    "\n",
    "        # å¼€å§‹ç”Ÿæˆæ–‡æœ¬\n",
    "        outputs = input_ids.copy()\n",
    "\n",
    "        for token_idx in range(max_length):\n",
    "            # å½“å‰tokenè¾“å…¥åˆ°æ¨¡å‹\n",
    "            cur_input = torch.tensor([[outputs[-1]]]).to(self.embed.weight.device)\n",
    "            logits = self.forward(cur_input)  # è°ƒç”¨forwardæ–¹æ³•\n",
    "\n",
    "            # ä½¿ç”¨ top-k + æ¸©åº¦é‡‡æ ·ä»£æ›¿è´ªå¿ƒé‡‡æ ·\n",
    "            next_logits = logits[:, -1, :] / temperature\n",
    "            topk_vals, topk_idx = torch.topk(next_logits, k=top_k, dim=-1)\n",
    "            probs = torch.softmax(topk_vals, dim=-1)\n",
    "            next_id = topk_idx[0, torch.multinomial(probs, num_samples=1).item()].item()\n",
    "            \n",
    "            outputs.append(next_id)\n",
    "            \n",
    "            # å¦‚æœé‡åˆ°ç»“æŸç¬¦ï¼Œæå‰åœæ­¢\n",
    "            if next_id == self.tokenizer.eos_token_id:\n",
    "                print(f\"  é‡åˆ°ç»“æŸç¬¦ï¼Œæå‰ç»“æŸç”Ÿæˆ\")\n",
    "                break\n",
    "\n",
    "        return self.tokenizer.decode(outputs, clean_up_tokenization_spaces=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ åˆå§‹åŒ–GPTJPipeline...\n",
      "ğŸ“‹ é…ç½®ä¿¡æ¯:\n",
      "   - æ¨¡å‹: AI-ModelScope/gpt-j-6b\n",
      "   - äº‘ç«¯è®¾å¤‡: cuda:0\n",
      "   - è¾¹ç¼˜è®¾å¤‡: cuda:0\n",
      "   - SVDå‹ç¼©ç‡: -1\n",
      "ğŸ“¥ ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹ AI-ModelScope/gpt-j-6b...\n",
      "Downloading Model from https://www.modelscope.cn to directory: ./gpt-j-6b/AI-ModelScope/gpt-j-6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 19:50:43,660 - modelscope - WARNING - Model revision not specified, use revision: v1.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œè·¯å¾„: ./gpt-j-6b/AI-ModelScope/gpt-j-6b\n",
      "ğŸ”¤ åŠ è½½tokenizer...\n",
      "â˜ï¸  åŠ è½½äº‘ç«¯æ¨¡å‹åˆ° cuda:0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./gpt-j-6b/AI-ModelScope/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸  åŠ è½½è¾¹ç¼˜æ¨¡å‹åˆ°CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./gpt-j-6b/AI-ModelScope/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æ¨¡å‹å…±æœ‰ 28 å±‚\n",
      "ğŸ”§ åˆ›å»ºSVDè¾¹ç¼˜æ¨¡å‹...\n",
      "ğŸ”§ SVDåˆ†è§£å°†åœ¨ cuda:0 ä¸Šè¿›è¡Œ...\n",
      "ğŸ”„ å¼€å§‹SVDåˆ†è§£å¤„ç†ï¼Œå‹ç¼©ç‡: -1\n",
      "ğŸ“Š æ€»å…±éœ€è¦å¤„ç† 28 å±‚...\n",
      "âš¡ SVDåˆ†è§£è®¾å¤‡: cuda:0, è¿è¡Œè®¾å¤‡: cpu\n",
      "ğŸ‰ æ‰€æœ‰å±‚çš„SVDåˆ†è§£å¤„ç†å®Œæˆï¼\n",
      "âœ… GPTJPipelineåˆå§‹åŒ–å®Œæˆï¼\n",
      "ğŸ¯ å‡†å¤‡å¼€å§‹æ¨ç†ï¼ŒSVDå‹ç¼©ç‡: -1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# åŸºæœ¬é…ç½®\n",
    "model_name    = 'AI-ModelScope/gpt-j-6b'\n",
    "device_cloud  = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device_edge   = 'cuda:0'\n",
    "svd_device    = device_cloud if torch.cuda.is_available() else 'cpu'\n",
    "rates         = [round(i * 0.1, 1) for i in range(0,10)]   # [0.0, 0.1, â€¦, 0.9]\n",
    "\n",
    "# 1. ä¸‹è½½å¹¶åŠ è½½åŸå§‹ edge æ¨¡å‹\n",
    "# original_edge = gptJ_edge(model_name=model_name).to(device_edge)\n",
    "num_layers    = 28\n",
    "\n",
    "# del original_edge\n",
    "\n",
    "pipeline = GPTJPipeline(\n",
    "    model_name=model_name,\n",
    "    device_cloud=device_cloud,\n",
    "    device_edge=device_edge,\n",
    "    svd_reduce_rate=-1,  # å ä½ï¼Œæ— å®é™…ç”¨åˆ°\n",
    "    \n",
    ")\n",
    "\n",
    "scheme=[0.0 for _ in range(num_layers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98071d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since JeanKaddour/minipile couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at minipile_cache/JeanKaddour___minipile/default/0.0.0/18ad1b0c701eaa0de03d3cecfdd769cbc70ffbd0 (last modified on Tue Jul 15 14:28:44 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_0_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_1_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_2_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_3_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_4_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_5_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_6_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_7_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_8_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_9_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_10_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_11_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_12_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_13_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_14_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_15_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_16_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_17_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_18_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_19_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_20_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_21_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_22_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_23_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_24_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_25_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_26_rate_0.0.pt\n",
      "æ­£åœ¨åŠ è½½ï¼šsvd_models/svd_layer_27_rate_0.0.pt\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "dataloader=load_and_tokenize_dataset(\"./minipile_cache\",pipeline.tokenizer,1)\n",
    "\n",
    "import pickle\n",
    "with torch.no_grad():\n",
    "    # 4.1 æ„å»ºä»…æ›¿æ¢ svd_layers çš„ edge æ¨¡å‹\n",
    "    edge_model = pipeline.edge  # ç›´æ¥å¤ç”¨å¯¹è±¡\n",
    "    temp = nn.ModuleList()\n",
    "\n",
    "    # åŠ è½½æ¨¡å‹ç¼“å­˜å¹¶æ·»åŠ åˆ° svd_layers\n",
    "    for i, rate in enumerate(scheme):\n",
    "        cache_path = f\"svd_models/svd_layer_{i}_rate_{rate}.pt\"\n",
    "        print(f\"æ­£åœ¨åŠ è½½ï¼š{cache_path}\")\n",
    "        mod = torch.load(cache_path, map_location='cuda:0', weights_only=False)\n",
    "        temp.append(mod)\n",
    "\n",
    "    edge_model.svd_layers = temp\n",
    "    edge_model.v_cache = [None] * num_layers\n",
    "    \n",
    "    # 4.2 å°† pipeline.edge æŒ‡å‘æ–°æ¨¡å‹å¹¶è¯„ä¼°\n",
    "    pipeline.edge = edge_model.to('cuda:0')\n",
    "    pipeline.cloud = pipeline.cloud.to('cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce8c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|â–‹         | 9/125 [00:04<01:02,  1.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     51\u001b[39m         perplexity = math.exp(avg_loss)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mavg_loss\u001b[39m\u001b[33m\"\u001b[39m: avg_loss, \u001b[33m\"\u001b[39m\u001b[33mperplexity\u001b[39m\u001b[33m\"\u001b[39m: perplexity}\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m eval_result = \u001b[43mevaluate_minipile_gptj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mevaluate_minipile_gptj\u001b[39m\u001b[34m(model, batch_size, cache_dir, Dataloader)\u001b[39m\n\u001b[32m     31\u001b[39m labels       = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].to(device)          \u001b[38;5;66;03m# [B, T], pad å·²ç»æ˜¯ -100\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     logits  = outputs                     \u001b[38;5;66;03m# [B, T, V]\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# æ‰‹åŠ¨ shiftï¼šlogits ä¸¢æ‰æœ€åä¸€ä½ï¼Œlabels ä¸¢æ‰ç¬¬ä¸€ä½\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 407\u001b[39m, in \u001b[36mGPTJPipeline.forward\u001b[39m\u001b[34m(self, input_ids)\u001b[39m\n\u001b[32m    405\u001b[39m _, _, attn_weights = \u001b[38;5;28mself\u001b[39m.cloud.forward_cache(x, layer_idx, attention_mask)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch.cuda, \u001b[33m'\u001b[39m\u001b[33msynchronize\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m cloud_time += time.time() - t0\n\u001b[32m    410\u001b[39m \u001b[38;5;66;03m# Edge forwardï¼ˆä¿æŒä¸å˜ï¼‰\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:1039\u001b[39m, in \u001b[36msynchronize\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m   1031\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Wait for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[32m   1032\u001b[39m \n\u001b[32m   1033\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1036\u001b[39m \u001b[33;03m        if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1038\u001b[39m _lazy_init()\n\u001b[32m-> \u001b[39m\u001b[32m1039\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:497\u001b[39m, in \u001b[36mdevice.__exit__\u001b[39m\u001b[34m(self, type, value, traceback)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    495\u001b[39m     \u001b[38;5;28mself\u001b[39m.prev_idx = torch.cuda._exchange_device(\u001b[38;5;28mself\u001b[39m.idx)\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m: Any, value: Any, traceback: Any):\n\u001b[32m    498\u001b[39m     \u001b[38;5;28mself\u001b[39m.idx = torch.cuda._maybe_exchange_device(\u001b[38;5;28mself\u001b[39m.prev_idx)\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:497\u001b[39m, in \u001b[36mdevice.__exit__\u001b[39m\u001b[34m(self, type, value, traceback)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    495\u001b[39m     \u001b[38;5;28mself\u001b[39m.prev_idx = torch.cuda._exchange_device(\u001b[38;5;28mself\u001b[39m.idx)\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m: Any, value: Any, traceback: Any):\n\u001b[32m    498\u001b[39m     \u001b[38;5;28mself\u001b[39m.idx = torch.cuda._maybe_exchange_device(\u001b[38;5;28mself\u001b[39m.prev_idx)\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:2017\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:16\u001b[39m, in \u001b[36mis_thread_alive\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t._handle.is_done()\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[33m\"\u001b[39m\u001b[33m_is_stopped\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# Python 3.12 and earlier has this\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_thread_alive\u001b[39m(t):\n\u001b[32m     17\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t._is_stopped\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[33m\"\u001b[39m\u001b[33m_Thread__stopped\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "def evaluate_minipile_gptj(model, batch_size: int = 1, cache_dir: str = \"./minipile_cache\", Dataloader=None) -> dict:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and tokenize dataset\n",
    "    tokenizer = model.tokenizer  # already initialized in the pipeline\n",
    "    dataloader = None\n",
    "    if Dataloader is None:\n",
    "        dataloader = load_and_tokenize_dataset(cache_dir, tokenizer, batch_size)\n",
    "    else:\n",
    "        dataloader = Dataloader\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=-100)\n",
    "\n",
    "    # Evaluation loop\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # æ‹¿åˆ°å®Œæ•´çš„ input_ids, attention_mask, å’Œå·²ç»è¢« collator è®¾å¥½ -100 çš„ labels\n",
    "            input_ids    = batch['input_ids'].to(device)       # [B, T]\n",
    "            attention_mask = batch['attention_mask'].to(device)# [B, T]\n",
    "            labels       = batch['labels'].to(device)          # [B, T], pad å·²ç»æ˜¯ -100\n",
    "\n",
    "            # æ‰“å°ä¸€ä¸‹æ•°æ®å½¢çŠ¶å’Œå†…å®¹ä»¥ä¾¿è°ƒè¯•\n",
    "            print(f\"è¾“å…¥å½¢çŠ¶: input_ids={input_ids.shape}, attention_mask={attention_mask.shape}, labels={labels.shape}\")\n",
    "            print(f\"ç¬¬ä¸€ä¸ªæ ·æœ¬å‰10ä¸ªtoken - input_ids: {input_ids[0, :10]}\")\n",
    "            print(f\"ç¬¬ä¸€ä¸ªæ ·æœ¬å‰10ä¸ªlabel: {labels[0, :10]}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # ä¼ é€’attention_maskç»™forwardå‡½æ•°\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits  = outputs                     # [B, T, V]\n",
    "\n",
    "            # æ‰‹åŠ¨ shiftï¼šlogits ä¸¢æ‰æœ€åä¸€ä½ï¼Œlabels ä¸¢æ‰ç¬¬ä¸€ä½\n",
    "            shift_logits = logits[:, :-1, :].contiguous()    # [B, T-1, V]\n",
    "            shift_labels = labels[:, 1:].contiguous()        # [B, T-1]\n",
    "\n",
    "            # è®¡ç®—äº¤å‰ç†µ lossï¼Œignore_index=-100 ä¼šè·³è¿‡æ‰€æœ‰ pad ä½ç½®\n",
    "            loss = criterion(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),  # [(B*(T-1)), V]\n",
    "                shift_labels.view(-1)                          # [(B*(T-1))]\n",
    "            )\n",
    "            print(f\"å½“å‰batch loss: {loss.item()}\")\n",
    "            total_loss   += loss.item()\n",
    "            total_batches+= 1\n",
    "            \n",
    "            # åªå¤„ç†å‰å‡ ä¸ªbatchè¿›è¡Œè°ƒè¯•\n",
    "            if total_batches >= 3:\n",
    "                break\n",
    "\n",
    "        avg_loss = total_loss / total_batches\n",
    "        perplexity = math.exp(avg_loss)\n",
    "\n",
    "    return {\"avg_loss\": avg_loss, \"perplexity\": perplexity}\n",
    "\n",
    "eval_result = evaluate_minipile_gptj(pipeline, batch_size=1, Dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa206d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3066fec8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "module GPTJPipeline not in sys.modules",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGPTJPipeline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/importlib/__init__.py:148\u001b[39m, in \u001b[36mreload\u001b[39m\u001b[34m(module)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.modules.get(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module:\n\u001b[32m    147\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m not in sys.modules\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg.format(name), name=name)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _RELOADING:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _RELOADING[name]\n",
      "\u001b[31mImportError\u001b[39m: module GPTJPipeline not in sys.modules"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(GPTJPipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
