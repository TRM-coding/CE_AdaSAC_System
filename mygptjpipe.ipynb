{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1819ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu126/fused_svd_matmul/build.ninja...\n",
      "Building extension module fused_svd_matmul...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_svd_matmul...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import tracemalloc\n",
    "from transformers import AutoTokenizer\n",
    "from modelscope.utils.hub import snapshot_download\n",
    "from detection.Loader.mymodel_file.gptJ_cloud import gptJ_cloud\n",
    "from detection.Loader.mymodel_file.gptJ_edge import gptJ_edge\n",
    "from detection.SVD_model import SVDED_GPTJ_EDGE_Layer\n",
    "from detection.MINI_PIPE_EVAL import load_and_tokenize_dataset\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"性能监控类，用于记录CPU时间和内存使用情况\"\"\"\n",
    "    def __init__(self):\n",
    "        self.process = psutil.Process(os.getpid())\n",
    "        self.reset_stats()\n",
    "        \n",
    "    def reset_stats(self):\n",
    "        \"\"\"重置统计数据\"\"\"\n",
    "        self.cloud_gpu_times = []\n",
    "        self.edge_cpu_times = []\n",
    "        self.network_times = []\n",
    "        self.memory_snapshots = []\n",
    "        self.token_count = 0\n",
    "        self.layer_calls = 0\n",
    "        \n",
    "        # 详细的计时统计\n",
    "        self.cloud_total_time = 0.0\n",
    "        self.edge_total_time = 0.0\n",
    "        self.network_total_time = 0.0\n",
    "        \n",
    "        # 内存统计\n",
    "        self.initial_memory = self.get_memory_mb()\n",
    "        self.peak_memory = self.initial_memory\n",
    "        \n",
    "    def get_memory_mb(self):\n",
    "        \"\"\"获取当前内存使用量(MB)\"\"\"\n",
    "        return self.process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    def get_cpu_percent(self):\n",
    "        \"\"\"获取CPU使用率\"\"\"\n",
    "        return self.process.cpu_percent()\n",
    "    \n",
    "    def start_memory_tracking(self):\n",
    "        \"\"\"开始内存跟踪\"\"\"\n",
    "        tracemalloc.start()\n",
    "        \n",
    "    def stop_memory_tracking(self):\n",
    "        \"\"\"停止内存跟踪并返回统计信息\"\"\"\n",
    "        if tracemalloc.is_tracing():\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            return current / 1024 / 1024, peak / 1024 / 1024  # 转换为MB\n",
    "        return 0, 0\n",
    "    \n",
    "    def record_cloud_time(self, time_taken):\n",
    "        \"\"\"记录云端GPU时间\"\"\"\n",
    "        self.cloud_gpu_times.append(time_taken)\n",
    "        self.cloud_total_time += time_taken\n",
    "        \n",
    "    def record_edge_time(self, time_taken):\n",
    "        \"\"\"记录边缘CPU时间\"\"\"\n",
    "        self.edge_cpu_times.append(time_taken)\n",
    "        self.edge_total_time += time_taken\n",
    "        \n",
    "    def record_network_time(self, time_taken):\n",
    "        \"\"\"记录网络传输时间\"\"\"\n",
    "        self.network_times.append(time_taken)\n",
    "        self.network_total_time += time_taken\n",
    "        \n",
    "    def record_memory_snapshot(self, phase=\"\"):\n",
    "        \"\"\"记录内存快照\"\"\"\n",
    "        current_memory = self.get_memory_mb()\n",
    "        self.memory_snapshots.append({\n",
    "            'phase': phase,\n",
    "            'memory_mb': current_memory,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        self.peak_memory = max(self.peak_memory, current_memory)\n",
    "        \n",
    "    def increment_counters(self):\n",
    "        \"\"\"增加计数器\"\"\"\n",
    "        self.layer_calls += 1\n",
    "        \n",
    "    def increment_token_count(self):\n",
    "        \"\"\"增加token计数\"\"\"\n",
    "        self.token_count += 1\n",
    "        \n",
    "    def print_detailed_report(self):\n",
    "        \"\"\"打印详细的性能报告\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"🔍 详细性能分析报告\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # 基本统计\n",
    "        print(f\"📊 基本统计信息:\")\n",
    "        print(f\"   🔢 处理的Token数量: {self.token_count}\")\n",
    "        print(f\"   🔢 总层调用次数: {self.layer_calls}\")\n",
    "        print(f\"   🔢 平均每token层调用: {self.layer_calls/max(1, self.token_count):.1f}\")\n",
    "        \n",
    "        # 时间统计\n",
    "        print(f\"\\n⏱️  时间统计 (总计):\")\n",
    "        print(f\"   ☁️  GPU云端总时间: {self.cloud_total_time:.4f}s\")\n",
    "        print(f\"   🖥️  CPU边缘总时间: {self.edge_total_time:.4f}s\")\n",
    "        print(f\"   🌐 网络传输总时间: {self.network_total_time:.4f}s\")\n",
    "        print(f\"   🔄 总处理时间: {self.cloud_total_time + self.edge_total_time + self.network_total_time:.4f}s\")\n",
    "        \n",
    "        # 平均时间统计\n",
    "        if self.token_count > 0:\n",
    "            print(f\"\\n⏱️  平均每Token时间:\")\n",
    "            print(f\"   ☁️  GPU云端平均: {self.cloud_total_time/self.token_count:.4f}s\")\n",
    "            print(f\"   🖥️  CPU边缘平均: {self.edge_total_time/self.token_count:.4f}s\")\n",
    "            print(f\"   🌐 网络传输平均: {self.network_total_time/self.token_count:.4f}s\")\n",
    "            print(f\"   🔄 总平均: {(self.cloud_total_time + self.edge_total_time + self.network_total_time)/self.token_count:.4f}s\")\n",
    "        \n",
    "        # 内存统计\n",
    "        current_memory = self.get_memory_mb()\n",
    "        memory_diff = current_memory - self.initial_memory\n",
    "        print(f\"\\n💾 内存使用统计:\")\n",
    "        print(f\"   📈 初始内存: {self.initial_memory:.2f}MB\")\n",
    "        print(f\"   📊 当前内存: {current_memory:.2f}MB\")\n",
    "        print(f\"   📈 峰值内存: {self.peak_memory:.2f}MB\")\n",
    "        print(f\"   📊 内存变化: {memory_diff:+.2f}MB\")\n",
    "        \n",
    "        # CPU使用率\n",
    "        cpu_percent = self.get_cpu_percent()\n",
    "        print(f\"   🔥 当前CPU使用率: {cpu_percent:.1f}%\")\n",
    "        \n",
    "        # 获取内存跟踪信息\n",
    "        if hasattr(self, '_tracemalloc_peak'):\n",
    "            print(f\"   🔍 内存跟踪峰值: {self._tracemalloc_peak:.2f}MB\")\n",
    "        \n",
    "        # 时间分布分析\n",
    "        if len(self.cloud_gpu_times) > 0:\n",
    "            print(f\"\\n📈 GPU时间分布:\")\n",
    "            print(f\"   最小: {min(self.cloud_gpu_times):.4f}s\")\n",
    "            print(f\"   最大: {max(self.cloud_gpu_times):.4f}s\")\n",
    "            print(f\"   平均: {sum(self.cloud_gpu_times)/len(self.cloud_gpu_times):.4f}s\")\n",
    "            \n",
    "        if len(self.edge_cpu_times) > 0:\n",
    "            print(f\"\\n📈 CPU时间分布:\")\n",
    "            print(f\"   最小: {min(self.edge_cpu_times):.4f}s\")\n",
    "            print(f\"   最大: {max(self.edge_cpu_times):.4f}s\")\n",
    "            print(f\"   平均: {sum(self.edge_cpu_times)/len(self.edge_cpu_times):.4f}s\")\n",
    "        \n",
    "        # 性能比较\n",
    "        if self.cloud_total_time > 0 and self.edge_total_time > 0:\n",
    "            ratio = self.edge_total_time / self.cloud_total_time\n",
    "            print(f\"\\n🔍 性能比较:\")\n",
    "            print(f\"   CPU/GPU时间比: {ratio:.2f}x\")\n",
    "            if ratio > 1:\n",
    "                print(f\"   💡 CPU比GPU慢 {ratio:.1f} 倍\")\n",
    "            else:\n",
    "                print(f\"   💡 CPU比GPU快 {1/ratio:.1f} 倍\")\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "    def print_memory_timeline(self):\n",
    "        \"\"\"打印内存使用时间线\"\"\"\n",
    "        if len(self.memory_snapshots) > 0:\n",
    "            print(f\"\\n📊 内存使用时间线:\")\n",
    "            for i, snapshot in enumerate(self.memory_snapshots):\n",
    "                print(f\"   {i+1}. {snapshot['phase']}: {snapshot['memory_mb']:.2f}MB\")\n",
    "                \n",
    "    def get_summary_stats(self):\n",
    "        \"\"\"返回摘要统计信息\"\"\"\n",
    "        return {\n",
    "            'cloud_total_time': self.cloud_total_time,\n",
    "            'edge_total_time': self.edge_total_time,\n",
    "            'network_total_time': self.network_total_time,\n",
    "            'token_count': self.token_count,\n",
    "            'layer_calls': self.layer_calls,\n",
    "            'memory_usage_mb': self.get_memory_mb(),\n",
    "            'memory_peak_mb': self.peak_memory,\n",
    "            'memory_diff_mb': self.get_memory_mb() - self.initial_memory\n",
    "        }\n",
    "\n",
    "class SVD_GPTJ_Edge_Model(nn.Module):\n",
    "    \"\"\"包含所有SVD层的完整edge模型，兼容原始edge模型接口\"\"\"\n",
    "    def __init__(self, original_edge, svd_reduce_rate, device='cpu', svd_device='cpu',No_init=False):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.svd_device = svd_device\n",
    "        self.num_layers = original_edge.num_layers\n",
    "        self.max_ctx = original_edge.max_ctx\n",
    "        self.v_cache = [None] * self.num_layers\n",
    "        \n",
    "        print(f\"🔄 开始SVD分解处理，压缩率: {svd_reduce_rate}\")\n",
    "        print(f\"📊 总共需要处理 {self.num_layers} 层...\")\n",
    "        print(f\"⚡ SVD分解设备: {svd_device}, 运行设备: {device}\")\n",
    "        \n",
    "        # 用SVD压缩的层替换原始edge层\n",
    "        self.svd_layers = nn.ModuleList()\n",
    "        if(not No_init):\n",
    "            for i in range(self.num_layers):\n",
    "                print(f\"  处理第 {i+1}/{self.num_layers} 层: \", end=\"\")\n",
    "                original_edge_layer = original_edge.layers[i]\n",
    "                    # 奇数层跳过压缩\n",
    "                if isinstance(svd_reduce_rate, list):\n",
    "\n",
    "                    svd_layer = SVDED_GPTJ_EDGE_Layer(\n",
    "                        gptj_edge_layer=original_edge_layer,\n",
    "                        reduce_rate=svd_reduce_rate[i],\n",
    "                        device=device,\n",
    "                        svd_device=svd_device\n",
    "                    )\n",
    "                else:\n",
    "                    svd_layer = SVDED_GPTJ_EDGE_Layer(\n",
    "                        gptj_edge_layer=original_edge_layer,\n",
    "                        reduce_rate=svd_reduce_rate,\n",
    "                        device=device,\n",
    "                        svd_device=svd_device\n",
    "                    )\n",
    "                print(\"跳过压缩 (奇数层)\")\n",
    "                self.svd_layers.append(svd_layer)\n",
    "        \n",
    "        print(f\"🎉 所有层的SVD分解处理完成！\")\n",
    "    \n",
    "    def forward_no_cache(self,x,layer_idx,attn_weights):\n",
    "        output=self.svd_layers[layer_idx].forward_no_cache(\n",
    "            x,  attn_weights\n",
    "        )\n",
    "        return output\n",
    "    \n",
    "    def forward_cache(self, x, layer_idx, attn_weights):\n",
    "        \"\"\"\n",
    "        兼容原始edge模型的forward_cache接口\n",
    "        Args:\n",
    "            x: 输入tensor\n",
    "            layer_idx: 层索引\n",
    "            attn_weights: 注意力权重\n",
    "        Returns:\n",
    "            tuple: (v_cache, output_x) - 与原始edge模型相同的返回格式\n",
    "        \"\"\"\n",
    "        # 使用SVD压缩的层进行前向传播\n",
    "        # tim1=time.time()\n",
    "        self.v_cache[layer_idx], output_x = self.svd_layers[layer_idx].forward_cache(\n",
    "            x, self.v_cache[layer_idx], attn_weights\n",
    "        )\n",
    "        # tim2=time.time()\n",
    "        # print(f\"layer_{layer_idx}_forward_time:\",tim2-tim1)\n",
    "        \n",
    "        # 应用sliding window到缓存\n",
    "        if self.v_cache[layer_idx] is not None and self.v_cache[layer_idx].size(1) > self.max_ctx:\n",
    "            self.v_cache[layer_idx] = self.v_cache[layer_idx][:, -self.max_ctx:, :]\n",
    "        # tim3=time.time()\n",
    "        # print(f\"layer_{layer_idx}_memory_time:\",tim3-tim2)\n",
    "\n",
    "        return self.v_cache[layer_idx], output_x\n",
    "\n",
    "class GPTJPipeline(nn.Module):\n",
    "    def __init__(self, model_name='AI-ModelScope/gpt-j-6b', device_cloud='cuda:0', device_edge='cpu', svd_reduce_rate=0.5, use_compile=True,edge=None):\n",
    "        super(GPTJPipeline, self).__init__()\n",
    "        print(f\"🚀 初始化GPTJPipeline...\")\n",
    "        print(f\"📋 配置信息:\")\n",
    "        print(f\"   - 模型: {model_name}\")\n",
    "        print(f\"   - 云端设备: {device_cloud}\")\n",
    "        print(f\"   - 边缘设备: {device_edge}\")\n",
    "        print(f\"   - SVD压缩率: {svd_reduce_rate}\")\n",
    "        \n",
    "        # 初始化性能监控器\n",
    "        self.performance_monitor = PerformanceMonitor()\n",
    "        \n",
    "        # 使用 ModelScope 下载模型\n",
    "        print(f\"📥 使用ModelScope下载模型 {model_name}...\")\n",
    "        model_dir = snapshot_download(\n",
    "            repo_id=model_name,\n",
    "            cache_dir='./gpt-j-6b'\n",
    "        )\n",
    "        print(f\"✅ 模型下载完成，路径: {model_dir}\")\n",
    "        \n",
    "        # 使用本地模型路径加载 tokenizer\n",
    "        print(f\"🔤 加载tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "        \n",
    "        # 设置 pad_token 为 eos_token（GPT-J 没有 pad_token）\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(f\"☁️  加载云端模型到 {device_cloud}...\")\n",
    "        self.cloud = gptJ_cloud(model_name=model_dir).to(device_cloud)\n",
    "        print(f\"🖥️  加载边缘模型到CPU...\")\n",
    "        # 强制 edge 放在 CPU\n",
    "        original_edge = gptJ_edge(model_name=model_dir).to('cpu')\n",
    "        self.embed = self.cloud.model.transformer.wte\n",
    "        self.ln_f = self.cloud.model.transformer.ln_f\n",
    "        self.lm_head = self.cloud.model.lm_head\n",
    "        self.num_layers = len(self.cloud.q_weights)\n",
    "        \n",
    "        print(f\"📊 模型共有 {self.num_layers} 层\")\n",
    "        \n",
    "        # SVD压缩参数\n",
    "        self.svd_reduce_rate = svd_reduce_rate\n",
    "        self.use_compile = use_compile\n",
    "        \n",
    "        # 创建整个SVD edge模型\n",
    "        print(f\"🔧 创建SVD边缘模型...\")\n",
    "        # 如果有GPU，先在GPU上进行SVD分解，然后移到CPU\n",
    "        svd_device = device_cloud if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"🔧 SVD分解将在 {svd_device} 上进行...\")\n",
    "        \n",
    "        if(svd_reduce_rate!=-1):\n",
    "            self.edge = SVD_GPTJ_Edge_Model(\n",
    "                original_edge=original_edge,\n",
    "                svd_reduce_rate=svd_reduce_rate,\n",
    "                device='cpu',  # 最终运行在CPU上\n",
    "                svd_device=svd_device  # 但SVD分解在GPU上进行\n",
    "            )\n",
    "        else:\n",
    "            self.edge=self.edge = SVD_GPTJ_Edge_Model(\n",
    "                original_edge=original_edge,\n",
    "                svd_reduce_rate=svd_reduce_rate,\n",
    "                device='cpu',  # 最终运行在CPU上\n",
    "                svd_device=svd_device,  # 但SVD分解在GPU上进行\n",
    "                No_init=True\n",
    "            )\n",
    "        \n",
    "        print(f\"✅ GPTJPipeline初始化完成！\")\n",
    "        print(f\"🎯 准备开始推理，SVD压缩率: {self.svd_reduce_rate}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # def forward(self, input_ids, attention_mask=None):\n",
    "    #     \"\"\"\n",
    "    #     Forward pass with RoPE and causal mask support,\n",
    "    #     using cloud/edge split with caching.\n",
    "    #     Args:\n",
    "    #         input_ids (LongTensor): [B, T] input token IDs.\n",
    "    #         attention_mask (BoolTensor, optional): [B, T] mask for valid tokens.\n",
    "    #     Returns:\n",
    "    #         Tensor: logits of shape [B, T, vocab_size]\n",
    "    #     \"\"\"\n",
    "    #     # Reset caches for a fresh generation or evaluation\n",
    "    #     for i in range(self.num_layers):\n",
    "    #         self.cloud.k_cache[i] = None\n",
    "    #         self.edge.v_cache[i] = None\n",
    "\n",
    "    #     B, T = input_ids.size()\n",
    "    #     device = input_ids.device\n",
    "\n",
    "    #     # Token embeddings (GPT-J uses RoPE internally, no explicit positional embedding)\n",
    "    #     x = self.embed(input_ids)  # [B, T, D]\n",
    "\n",
    "    #     # Prepare attention mask: combine causal mask with token mask\n",
    "    #     if attention_mask is None:\n",
    "    #         attention_mask = torch.ones((B, T), device=device)\n",
    "    #     # Expand token mask to [B, 1, T] for broadcasting\n",
    "    #     token_mask = attention_mask.unsqueeze(1)  # [B, 1, T]\n",
    "    #     # Causal mask [1, T, T]\n",
    "    #     causal = torch.tril(torch.ones((T, T), device=device)).unsqueeze(0)\n",
    "    #     # Final mask [B, T, T]: broadcast token_mask along query dimension\n",
    "    #     attn_mask = token_mask * causal  # [B, T, T]\n",
    "\n",
    "    #     # Cloud-Edge split forward\n",
    "    #     for layer_idx in range(self.num_layers):\n",
    "    #         # Move hidden and mask to cloud device\n",
    "    #         x_cloud = x.to(self.cloud.device)\n",
    "    #         mask_cloud = attn_mask.to(self.cloud.device)\n",
    "    #         # cloud forward: returns (hidden, kv_cache, attn_weights)\n",
    "    #         x_cloud, _, attn_w = self.cloud.forward_cache(\n",
    "    #             x_cloud, layer_idx\n",
    "    #         )\n",
    "    #         # Move to edge for remaining computation\n",
    "    #         x_edge = x_cloud.to(self.edge.device)\n",
    "    #         attn_edge = attn_w.to(self.edge.device)\n",
    "    #         x_edge, _ = self.edge.forward_cache(\n",
    "    #             x_edge, layer_idx, attn_weights=attn_edge\n",
    "    #         )\n",
    "    #         # Bring back to main device\n",
    "    #         x = x_edge.to(device)\n",
    "\n",
    "    #     # Final normalization and LM head\n",
    "    #     x = self.ln_f(x)           # [B, T, D]\n",
    "    #     logits = self.lm_head(x)   # [B, T, vocab_size]\n",
    "    #     return logits\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # 1. 生成 padding mask: pad_token_id 位置为 0，其它为 1\n",
    "        #    假设 self.config.pad_token_id 已经被设置\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()  # [B, T]\n",
    "\n",
    "        # Reset caches for a fresh generation\n",
    "        for i in range(self.num_layers):\n",
    "            self.cloud.k_cache[i] = None\n",
    "            self.edge.v_cache[i] = None\n",
    "\n",
    "        # Statistics variables\n",
    "        cloud_time = edge_time = net_time = 0.0\n",
    "        layer_calls = 0\n",
    "        bandwidth = 10  # MB/s\n",
    "\n",
    "        # Embedding\n",
    "        x = self.embed(input_ids)  # [B, T, D]\n",
    "\n",
    "        # 层级迭代\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            # Cloud forward：传入 attention_mask，用于内部做 pad+causal 屏蔽\n",
    "            if hasattr(torch.cuda, 'synchronize'):\n",
    "                torch.cuda.synchronize()\n",
    "            t0 = time.time()\n",
    "            _, _, attn_weights = self.cloud.forward_cache(x, layer_idx, attention_mask)\n",
    "            if hasattr(torch.cuda, 'synchronize'):\n",
    "                torch.cuda.synchronize()\n",
    "            cloud_time += time.time() - t0\n",
    "\n",
    "            # Edge forward（保持不变）\n",
    "            x_cpu = x.to('cuda:0')\n",
    "            attn_cpu = attn_weights.to('cuda:0')\n",
    "            t1 = time.time()\n",
    "            _, x_cpu = self.edge.forward_cache(x_cpu, layer_idx, attn_cpu)\n",
    "            edge_time += time.time() - t1\n",
    "\n",
    "            # 网络开销估算\n",
    "            elements = attn_cpu.numel() * attn_cpu.element_size()\n",
    "            net_time += elements / bandwidth / 1024 / 1024\n",
    "            x = x_cpu.to(self.embed.weight.device)\n",
    "            elements = x.numel() * x.element_size()\n",
    "            net_time += elements / bandwidth / 1024 / 1024\n",
    "\n",
    "            layer_calls += 1\n",
    "\n",
    "        # Final normalization and LM head to get logits\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    # def forward (self, input_ids):\n",
    "    #     outputs = input_ids\n",
    "\n",
    "    #     # Reset caches for a fresh generation\n",
    "    #     for i in range (self.num_layers):\n",
    "    #         self.cloud.k_cache [i] = None\n",
    "    #         self.edge.v_cache [i] = None\n",
    "\n",
    "    #     # Statistics variables\n",
    "    #     cloud_time = 0.0\n",
    "    #     edge_time = 0.0\n",
    "    #     layer_calls = 0\n",
    "    #     net_time = 0.0\n",
    "    #     bandwidth = 10  # MB/s\n",
    "\n",
    "    #     # Context window size\n",
    "    #     max_ctx = self.cloud.max_ctx\n",
    "    #     x = self.embed(outputs)\n",
    "    #     # Process the input sequence step by step for causal language modeling\n",
    "    #     # for token_idx in range (outputs.size(1) - 1):  # Exclude the last token for target generation\n",
    "    #         # For each token in the sequence, we use the preceding tokens for input\n",
    "    #         # cur_input = outputs [:, token_idx].unsqueeze(-1)  # Take current token as input\n",
    "            \n",
    "\n",
    "    #     for layer_idx in range (self.num_layers):\n",
    "    #         # Use cache-enabled forward so attention spans all previous tokens\n",
    "    #         if hasattr (torch.cuda, 'synchronize'):\n",
    "    #             torch.cuda.synchronize()\n",
    "    #         t0 = time.time()\n",
    "    #         _, _, attn_weights = self.cloud.forward_cache (x, layer_idx)\n",
    "    #         if hasattr (torch.cuda, 'synchronize'):\n",
    "    #             torch.cuda.synchronize()\n",
    "    #         cloud_time += time.time() - t0\n",
    "\n",
    "    #         x_cpu = x.to('cuda:0')\n",
    "    #         attn_cpu = attn_weights.to('cuda:0')\n",
    "    #         t1 = time.time()\n",
    "    #         _, x_cpu = self.edge.forward_cache (x_cpu, layer_idx, attn_cpu)\n",
    "    #         edge_time += time.time() - t1\n",
    "\n",
    "    #         elements = attn_cpu.numel() * attn_cpu.element_size ()  # B\n",
    "    #         net_time += elements / bandwidth / 1024 / 1024\n",
    "    #         x = x_cpu.to(self.embed.weight.device)\n",
    "    #         elements = x.numel() * x.element_size ()  # B\n",
    "    #         net_time += elements / bandwidth / 1024 / 1024\n",
    "    #         layer_calls += 1\n",
    "\n",
    "    #     # Final normalization and LM head to get logits\n",
    "    #     x = self.ln_f (x)\n",
    "    #     logits = self.lm_head (x)\n",
    "\n",
    "    #     return logits\n",
    "\n",
    "    def generate(self, prompt, max_length=50, temperature=1.0, top_k=50):\n",
    "        \"\"\"\n",
    "        调用 forward 方法生成文本\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')[0].tolist()\n",
    "\n",
    "        # 开始生成文本\n",
    "        outputs = input_ids.copy()\n",
    "\n",
    "        for token_idx in range(max_length):\n",
    "            # 当前token输入到模型\n",
    "            cur_input = torch.tensor([[outputs[-1]]]).to(self.embed.weight.device)\n",
    "            logits = self.forward(cur_input)  # 调用forward方法\n",
    "\n",
    "            # 使用 top-k + 温度采样代替贪心采样\n",
    "            next_logits = logits[:, -1, :] / temperature\n",
    "            topk_vals, topk_idx = torch.topk(next_logits, k=top_k, dim=-1)\n",
    "            probs = torch.softmax(topk_vals, dim=-1)\n",
    "            next_id = topk_idx[0, torch.multinomial(probs, num_samples=1).item()].item()\n",
    "            \n",
    "            outputs.append(next_id)\n",
    "            \n",
    "            # 如果遇到结束符，提前停止\n",
    "            if next_id == self.tokenizer.eos_token_id:\n",
    "                print(f\"  遇到结束符，提前结束生成\")\n",
    "                break\n",
    "\n",
    "        return self.tokenizer.decode(outputs, clean_up_tokenization_spaces=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 初始化GPTJPipeline...\n",
      "📋 配置信息:\n",
      "   - 模型: AI-ModelScope/gpt-j-6b\n",
      "   - 云端设备: cuda:0\n",
      "   - 边缘设备: cuda:0\n",
      "   - SVD压缩率: -1\n",
      "📥 使用ModelScope下载模型 AI-ModelScope/gpt-j-6b...\n",
      "Downloading Model from https://www.modelscope.cn to directory: ./gpt-j-6b/AI-ModelScope/gpt-j-6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 19:50:43,660 - modelscope - WARNING - Model revision not specified, use revision: v1.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型下载完成，路径: ./gpt-j-6b/AI-ModelScope/gpt-j-6b\n",
      "🔤 加载tokenizer...\n",
      "☁️  加载云端模型到 cuda:0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./gpt-j-6b/AI-ModelScope/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️  加载边缘模型到CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./gpt-j-6b/AI-ModelScope/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 模型共有 28 层\n",
      "🔧 创建SVD边缘模型...\n",
      "🔧 SVD分解将在 cuda:0 上进行...\n",
      "🔄 开始SVD分解处理，压缩率: -1\n",
      "📊 总共需要处理 28 层...\n",
      "⚡ SVD分解设备: cuda:0, 运行设备: cpu\n",
      "🎉 所有层的SVD分解处理完成！\n",
      "✅ GPTJPipeline初始化完成！\n",
      "🎯 准备开始推理，SVD压缩率: -1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 基本配置\n",
    "model_name    = 'AI-ModelScope/gpt-j-6b'\n",
    "device_cloud  = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device_edge   = 'cuda:0'\n",
    "svd_device    = device_cloud if torch.cuda.is_available() else 'cpu'\n",
    "rates         = [round(i * 0.1, 1) for i in range(0,10)]   # [0.0, 0.1, …, 0.9]\n",
    "\n",
    "# 1. 下载并加载原始 edge 模型\n",
    "# original_edge = gptJ_edge(model_name=model_name).to(device_edge)\n",
    "num_layers    = 28\n",
    "\n",
    "# del original_edge\n",
    "\n",
    "pipeline = GPTJPipeline(\n",
    "    model_name=model_name,\n",
    "    device_cloud=device_cloud,\n",
    "    device_edge=device_edge,\n",
    "    svd_reduce_rate=-1,  # 占位，无实际用到\n",
    "    \n",
    ")\n",
    "\n",
    "scheme=[0.0 for _ in range(num_layers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98071d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since JeanKaddour/minipile couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at minipile_cache/JeanKaddour___minipile/default/0.0.0/18ad1b0c701eaa0de03d3cecfdd769cbc70ffbd0 (last modified on Tue Jul 15 14:28:44 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载：svd_models/svd_layer_0_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_1_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_2_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_3_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_4_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_5_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_6_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_7_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_8_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_9_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_10_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_11_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_12_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_13_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_14_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_15_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_16_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_17_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_18_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_19_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_20_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_21_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_22_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_23_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_24_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_25_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_26_rate_0.0.pt\n",
      "正在加载：svd_models/svd_layer_27_rate_0.0.pt\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "dataloader=load_and_tokenize_dataset(\"./minipile_cache\",pipeline.tokenizer,1)\n",
    "\n",
    "import pickle\n",
    "with torch.no_grad():\n",
    "    # 4.1 构建仅替换 svd_layers 的 edge 模型\n",
    "    edge_model = pipeline.edge  # 直接复用对象\n",
    "    temp = nn.ModuleList()\n",
    "\n",
    "    # 加载模型缓存并添加到 svd_layers\n",
    "    for i, rate in enumerate(scheme):\n",
    "        cache_path = f\"svd_models/svd_layer_{i}_rate_{rate}.pt\"\n",
    "        print(f\"正在加载：{cache_path}\")\n",
    "        mod = torch.load(cache_path, map_location='cuda:0', weights_only=False)\n",
    "        temp.append(mod)\n",
    "\n",
    "    edge_model.svd_layers = temp\n",
    "    edge_model.v_cache = [None] * num_layers\n",
    "    \n",
    "    # 4.2 将 pipeline.edge 指向新模型并评估\n",
    "    pipeline.edge = edge_model.to('cuda:0')\n",
    "    pipeline.cloud = pipeline.cloud.to('cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce8c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|▋         | 9/125 [00:04<01:02,  1.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     51\u001b[39m         perplexity = math.exp(avg_loss)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mavg_loss\u001b[39m\u001b[33m\"\u001b[39m: avg_loss, \u001b[33m\"\u001b[39m\u001b[33mperplexity\u001b[39m\u001b[33m\"\u001b[39m: perplexity}\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m eval_result = \u001b[43mevaluate_minipile_gptj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mevaluate_minipile_gptj\u001b[39m\u001b[34m(model, batch_size, cache_dir, Dataloader)\u001b[39m\n\u001b[32m     31\u001b[39m labels       = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].to(device)          \u001b[38;5;66;03m# [B, T], pad 已经是 -100\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     logits  = outputs                     \u001b[38;5;66;03m# [B, T, V]\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 手动 shift：logits 丢掉最后一位，labels 丢掉第一位\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 407\u001b[39m, in \u001b[36mGPTJPipeline.forward\u001b[39m\u001b[34m(self, input_ids)\u001b[39m\n\u001b[32m    405\u001b[39m _, _, attn_weights = \u001b[38;5;28mself\u001b[39m.cloud.forward_cache(x, layer_idx, attention_mask)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch.cuda, \u001b[33m'\u001b[39m\u001b[33msynchronize\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m cloud_time += time.time() - t0\n\u001b[32m    410\u001b[39m \u001b[38;5;66;03m# Edge forward（保持不变）\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:1039\u001b[39m, in \u001b[36msynchronize\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m   1031\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Wait for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[32m   1032\u001b[39m \n\u001b[32m   1033\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1036\u001b[39m \u001b[33;03m        if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1038\u001b[39m _lazy_init()\n\u001b[32m-> \u001b[39m\u001b[32m1039\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:497\u001b[39m, in \u001b[36mdevice.__exit__\u001b[39m\u001b[34m(self, type, value, traceback)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    495\u001b[39m     \u001b[38;5;28mself\u001b[39m.prev_idx = torch.cuda._exchange_device(\u001b[38;5;28mself\u001b[39m.idx)\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m: Any, value: Any, traceback: Any):\n\u001b[32m    498\u001b[39m     \u001b[38;5;28mself\u001b[39m.idx = torch.cuda._maybe_exchange_device(\u001b[38;5;28mself\u001b[39m.prev_idx)\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:497\u001b[39m, in \u001b[36mdevice.__exit__\u001b[39m\u001b[34m(self, type, value, traceback)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    495\u001b[39m     \u001b[38;5;28mself\u001b[39m.prev_idx = torch.cuda._exchange_device(\u001b[38;5;28mself\u001b[39m.idx)\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m: Any, value: Any, traceback: Any):\n\u001b[32m    498\u001b[39m     \u001b[38;5;28mself\u001b[39m.idx = torch.cuda._maybe_exchange_device(\u001b[38;5;28mself\u001b[39m.prev_idx)\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:2017\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:16\u001b[39m, in \u001b[36mis_thread_alive\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t._handle.is_done()\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[33m\"\u001b[39m\u001b[33m_is_stopped\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# Python 3.12 and earlier has this\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_thread_alive\u001b[39m(t):\n\u001b[32m     17\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t._is_stopped\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[33m\"\u001b[39m\u001b[33m_Thread__stopped\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "def evaluate_minipile_gptj(model, batch_size: int = 1, cache_dir: str = \"./minipile_cache\", Dataloader=None) -> dict:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and tokenize dataset\n",
    "    tokenizer = model.tokenizer  # already initialized in the pipeline\n",
    "    dataloader = None\n",
    "    if Dataloader is None:\n",
    "        dataloader = load_and_tokenize_dataset(cache_dir, tokenizer, batch_size)\n",
    "    else:\n",
    "        dataloader = Dataloader\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=-100)\n",
    "\n",
    "    # Evaluation loop\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # 拿到完整的 input_ids, attention_mask, 和已经被 collator 设好 -100 的 labels\n",
    "            input_ids    = batch['input_ids'].to(device)       # [B, T]\n",
    "            attention_mask = batch['attention_mask'].to(device)# [B, T]\n",
    "            labels       = batch['labels'].to(device)          # [B, T], pad 已经是 -100\n",
    "\n",
    "            # 打印一下数据形状和内容以便调试\n",
    "            print(f\"输入形状: input_ids={input_ids.shape}, attention_mask={attention_mask.shape}, labels={labels.shape}\")\n",
    "            print(f\"第一个样本前10个token - input_ids: {input_ids[0, :10]}\")\n",
    "            print(f\"第一个样本前10个label: {labels[0, :10]}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # 传递attention_mask给forward函数\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits  = outputs                     # [B, T, V]\n",
    "\n",
    "            # 手动 shift：logits 丢掉最后一位，labels 丢掉第一位\n",
    "            shift_logits = logits[:, :-1, :].contiguous()    # [B, T-1, V]\n",
    "            shift_labels = labels[:, 1:].contiguous()        # [B, T-1]\n",
    "\n",
    "            # 计算交叉熵 loss，ignore_index=-100 会跳过所有 pad 位置\n",
    "            loss = criterion(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),  # [(B*(T-1)), V]\n",
    "                shift_labels.view(-1)                          # [(B*(T-1))]\n",
    "            )\n",
    "            print(f\"当前batch loss: {loss.item()}\")\n",
    "            total_loss   += loss.item()\n",
    "            total_batches+= 1\n",
    "            \n",
    "            # 只处理前几个batch进行调试\n",
    "            if total_batches >= 3:\n",
    "                break\n",
    "\n",
    "        avg_loss = total_loss / total_batches\n",
    "        perplexity = math.exp(avg_loss)\n",
    "\n",
    "    return {\"avg_loss\": avg_loss, \"perplexity\": perplexity}\n",
    "\n",
    "eval_result = evaluate_minipile_gptj(pipeline, batch_size=1, Dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa206d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3066fec8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "module GPTJPipeline not in sys.modules",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGPTJPipeline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/importlib/__init__.py:148\u001b[39m, in \u001b[36mreload\u001b[39m\u001b[34m(module)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.modules.get(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module:\n\u001b[32m    147\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m not in sys.modules\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg.format(name), name=name)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _RELOADING:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _RELOADING[name]\n",
      "\u001b[31mImportError\u001b[39m: module GPTJPipeline not in sys.modules"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(GPTJPipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
