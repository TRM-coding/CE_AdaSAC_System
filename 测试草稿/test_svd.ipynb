{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianruiming/Eckart-Young-based-ML-Inference-framework/detection/DataGenerator.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model=torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "import detection.DataGenerator\n",
    "import detection.Model_transfer\n",
    "model_path='./model/conv.pth'\n",
    "data_generator = detection.DataGenerator.train_based_self_detection(\n",
    "    model_path=model_path,\n",
    "    device='cuda:1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianruiming/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/tianruiming/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_update: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianruiming/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_update: 0.0002\n",
      "lr_update: 0.00030000000000000003\n",
      "lr_update: 0.0004\n",
      "lr_update: 0.0005\n",
      "lr_update: 0.0006000000000000001\n",
      "lr_update: 0.0007000000000000001\n",
      "lr_update: 0.0008000000000000001\n",
      "lr_update: 0.0009000000000000002\n",
      "lr_update: 0.0010000000000000002\n",
      "lr_update: 0.0011000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0012000000000000003\n",
      "lr_update: 0.0011989027261734009\n",
      "lr_update: 0.0011833012104034424\n",
      "lr_update: 0.0011543482542037964\n",
      "lr_update: 0.0011128783226013184\n",
      "lr_update: 0.001060381531715393\n",
      "lr_update: 0.000999346375465393\n",
      "lr_update: 0.0009314566850662231\n",
      "lr_update: 0.0008594244718551636\n",
      "lr_update: 0.000785335898399353\n",
      "lr_update: 0.0007109940052032471\n",
      "lr_update: 0.0006384104490280151\n",
      "lr_update: 0.000568583607673645\n",
      "lr_update: 0.0005027353763580322\n",
      "lr_update: 0.0004415363073348999\n",
      "lr_update: 0.0003853142261505127\n",
      "lr_update: 0.00033462047576904297\n",
      "lr_update: 0.00028887391090393066\n",
      "lr_update: 0.0002485513687133789\n",
      "lr_update: 0.0002129673957824707\n",
      "lr_update: 0.0001818835735321045\n",
      "lr_update: 0.00015497207641601562\n",
      "lr_update: 0.00013141334056854248\n",
      "lr_update: 0.00011150538921356201\n",
      "lr_update: 9.432435035705566e-05\n",
      "lr_update: 7.964670658111572e-05\n",
      "lr_update: 6.720423698425293e-05\n",
      "lr_update: 5.65648078918457e-05\n",
      "lr_update: 4.7653913497924805e-05\n",
      "lr_update: 4.00543212890625e-05\n",
      "lr_update: 3.3661723136901855e-05\n",
      "lr_update: 2.822279930114746e-05\n",
      "lr_update: 2.384185791015625e-05\n",
      "lr_update: 2.0012259483337402e-05\n",
      "lr_update: 1.691281795501709e-05\n",
      "lr_update: 1.4260411262512207e-05\n",
      "lr_update: 1.2040138244628906e-05\n",
      "lr_update: 1.017749309539795e-05\n",
      "lr_update: 8.627772331237793e-06\n",
      "lr_update: 7.286667823791504e-06\n",
      "lr_update: 6.154179573059082e-06\n",
      "lr_update: 5.230307579040527e-06\n",
      "lr_update: 4.425644874572754e-06\n",
      "lr_update: 3.7103891372680664e-06\n",
      "lr_update: 3.159046173095703e-06\n",
      "lr_update: 2.682209014892578e-06\n",
      "lr_update: 2.2798776626586914e-06\n",
      "lr_update: 1.9669532775878906e-06\n",
      "lr_update: 1.6689300537109375e-06\n",
      "lr_update: 1.430511474609375e-06\n",
      "lr_update: 1.2218952178955078e-06\n",
      "lr_update: 1.0579824447631836e-06\n",
      "lr_update: 8.791685104370117e-07\n",
      "lr_update: 7.599592208862305e-07\n",
      "lr_update: 6.556510925292969e-07\n",
      "lr_update: 5.960464477539062e-07\n",
      "lr_update: 4.917383193969727e-07\n",
      "lr_update: 4.3213367462158203e-07\n",
      "lr_update: 3.725290298461914e-07\n",
      "lr_update: 3.427267074584961e-07\n",
      "lr_update: 2.980232238769531e-07\n",
      "lr_update: 2.682209014892578e-07\n",
      "lr_update: 2.2351741790771484e-07\n",
      "lr_update: 1.7881393432617188e-07\n",
      "lr_update: 1.341104507446289e-07\n",
      "lr_update: 1.043081283569336e-07\n",
      "lr_update: 4.470348358154297e-08\n",
      "lr_update: 1.4901161193847656e-08\n",
      "lr_update: 0\n"
     ]
    }
   ],
   "source": [
    "data_input,output_lable,lable=data_generator.make_data_pid(\n",
    "    batch_size=10000,\n",
    "    learning_rate=1,\n",
    "    channel=1,\n",
    "    dim1=28,\n",
    "    dim2=28,\n",
    "    output_size=10,\n",
    "    randn_magnification=30,\n",
    "    confidence=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianruiming/Eckart-Young-based-ML-Inference-framework/detection/Searcher.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model=torch.load(model_path)\n",
      "/home/tianruiming/Eckart-Young-based-ML-Inference-framework/detection/Model_transfer.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model=torch.load(model_path)\n",
      "/home/tianruiming/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serched_solution: 215\r"
     ]
    }
   ],
   "source": [
    "import detection.Searcher as Searcher\n",
    "\n",
    "model_path='./model/conv.pth'\n",
    "searcher = Searcher.Recrusively_reduce_search(\n",
    "    model_path=model_path,\n",
    "    input_data=data_input,\n",
    "    output_label=output_lable,\n",
    "    label=lable,\n",
    "    device='cuda:1',\n",
    ")\n",
    "acc_data=[]\n",
    "loss_data=[]\n",
    "searcher.search(\n",
    "    bound=3,\n",
    "    step=0.2,\n",
    "    acc_data=acc_data,\n",
    "    loss_data=loss_data,\n",
    ")\n",
    "# print(searcher.acc_evaluate(data_input,lable,model))\n",
    "# print(searcher.loss_evaluate(data_input,output_lable,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([np.float64(1.0), np.float64(1.0), np.float64(1.0)], 0.1023)\n",
      "([np.float64(1.0), np.float64(1.0), np.float64(0.8)], 0.1023)\n",
      "([np.float64(1.0), np.float64(1.0), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(1.0), np.float64(1.0), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(1.0), np.float64(1.0), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(1.0), np.float64(1.0), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.8), np.float64(1.0)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.8), np.float64(0.8)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.8), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.8), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.8), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.8), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.6000000000000001), np.float64(1.0)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.6000000000000001), np.float64(0.8)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.6000000000000001), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.6000000000000001), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.6000000000000001), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.6000000000000001), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.40000000000000013), np.float64(1.0)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.40000000000000013), np.float64(0.8)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.40000000000000013), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.40000000000000013), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.40000000000000013), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.40000000000000013), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.20000000000000018), np.float64(1.0)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.20000000000000018), np.float64(0.8)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.20000000000000018), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.20000000000000018), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.20000000000000018), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(1.0), np.float64(0.20000000000000018), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(1.0), np.float64(2.220446049250313e-16), np.float64(1.0)], 0.1023)\n",
      "([np.float64(1.0), np.float64(2.220446049250313e-16), np.float64(0.8)], 0.1023)\n",
      "([np.float64(1.0), np.float64(2.220446049250313e-16), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(1.0), np.float64(2.220446049250313e-16), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(1.0), np.float64(2.220446049250313e-16), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(1.0), np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(0.8), np.float64(1.0), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.8), np.float64(1.0), np.float64(0.8)], 0.1023)\n",
      "([np.float64(0.8), np.float64(1.0), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(0.8), np.float64(1.0), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(0.8), np.float64(1.0), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(0.8), np.float64(1.0), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(0.8), np.float64(0.8), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.8), np.float64(0.8), np.float64(0.8)], 0.9963)\n",
      "([np.float64(0.8), np.float64(0.8), np.float64(0.6000000000000001)], 0.9987)\n",
      "([np.float64(0.8), np.float64(0.8), np.float64(0.40000000000000013)], 0.9986)\n",
      "([np.float64(0.8), np.float64(0.8), np.float64(0.20000000000000018)], 0.9985)\n",
      "([np.float64(0.8), np.float64(0.8), np.float64(2.220446049250313e-16)], 0.9985)\n",
      "([np.float64(0.8), np.float64(0.6000000000000001), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.8), np.float64(0.6000000000000001), np.float64(0.8)], 0.9979)\n",
      "([np.float64(0.8), np.float64(0.6000000000000001), np.float64(0.6000000000000001)], 0.9993)\n",
      "([np.float64(0.8), np.float64(0.6000000000000001), np.float64(0.40000000000000013)], 0.9993)\n",
      "([np.float64(0.8), np.float64(0.6000000000000001), np.float64(0.20000000000000018)], 0.9994)\n",
      "([np.float64(0.8), np.float64(0.6000000000000001), np.float64(2.220446049250313e-16)], 0.9993)\n",
      "([np.float64(0.8), np.float64(0.40000000000000013), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.8), np.float64(0.40000000000000013), np.float64(0.8)], 0.9989)\n",
      "([np.float64(0.8), np.float64(0.40000000000000013), np.float64(0.6000000000000001)], 0.9995)\n",
      "([np.float64(0.8), np.float64(0.40000000000000013), np.float64(0.40000000000000013)], 0.9996)\n",
      "([np.float64(0.8), np.float64(0.40000000000000013), np.float64(0.20000000000000018)], 0.9995)\n",
      "([np.float64(0.8), np.float64(0.40000000000000013), np.float64(2.220446049250313e-16)], 0.9995)\n",
      "([np.float64(0.8), np.float64(0.20000000000000018), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.8), np.float64(0.20000000000000018), np.float64(0.8)], 0.9987)\n",
      "([np.float64(0.8), np.float64(0.20000000000000018), np.float64(0.6000000000000001)], 0.9995)\n",
      "([np.float64(0.8), np.float64(0.20000000000000018), np.float64(0.40000000000000013)], 0.9995)\n",
      "([np.float64(0.8), np.float64(0.20000000000000018), np.float64(0.20000000000000018)], 0.9994)\n",
      "([np.float64(0.8), np.float64(0.20000000000000018), np.float64(2.220446049250313e-16)], 0.9994)\n",
      "([np.float64(0.8), np.float64(2.220446049250313e-16), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.8), np.float64(2.220446049250313e-16), np.float64(0.8)], 0.9989)\n",
      "([np.float64(0.8), np.float64(2.220446049250313e-16), np.float64(0.6000000000000001)], 0.9996)\n",
      "([np.float64(0.8), np.float64(2.220446049250313e-16), np.float64(0.40000000000000013)], 0.9996)\n",
      "([np.float64(0.8), np.float64(2.220446049250313e-16), np.float64(0.20000000000000018)], 0.9993)\n",
      "([np.float64(0.8), np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16)], 0.9993)\n",
      "([np.float64(0.6000000000000001), np.float64(1.0), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(1.0), np.float64(0.8)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(1.0), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(1.0), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(1.0), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(1.0), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(0.8), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(0.8), np.float64(0.8)], 0.997)\n",
      "([np.float64(0.6000000000000001), np.float64(0.8), np.float64(0.6000000000000001)], 0.9989)\n",
      "([np.float64(0.6000000000000001), np.float64(0.8), np.float64(0.40000000000000013)], 0.999)\n",
      "([np.float64(0.6000000000000001), np.float64(0.8), np.float64(0.20000000000000018)], 0.9989)\n",
      "([np.float64(0.6000000000000001), np.float64(0.8), np.float64(2.220446049250313e-16)], 0.9989)\n",
      "([np.float64(0.6000000000000001), np.float64(0.6000000000000001), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(0.6000000000000001), np.float64(0.8)], 0.9982)\n",
      "([np.float64(0.6000000000000001), np.float64(0.6000000000000001), np.float64(0.6000000000000001)], 0.9995)\n",
      "([np.float64(0.6000000000000001), np.float64(0.6000000000000001), np.float64(0.40000000000000013)], 0.9995)\n",
      "([np.float64(0.6000000000000001), np.float64(0.6000000000000001), np.float64(0.20000000000000018)], 0.9994)\n",
      "([np.float64(0.6000000000000001), np.float64(0.6000000000000001), np.float64(2.220446049250313e-16)], 0.9994)\n",
      "([np.float64(0.6000000000000001), np.float64(0.40000000000000013), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(0.40000000000000013), np.float64(0.8)], 0.9989)\n",
      "([np.float64(0.6000000000000001), np.float64(0.40000000000000013), np.float64(0.6000000000000001)], 0.9998)\n",
      "([np.float64(0.6000000000000001), np.float64(0.40000000000000013), np.float64(0.40000000000000013)], 0.9997)\n",
      "([np.float64(0.6000000000000001), np.float64(0.40000000000000013), np.float64(0.20000000000000018)], 0.9997)\n",
      "([np.float64(0.6000000000000001), np.float64(0.40000000000000013), np.float64(2.220446049250313e-16)], 0.9997)\n",
      "([np.float64(0.6000000000000001), np.float64(0.20000000000000018), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(0.20000000000000018), np.float64(0.8)], 0.9985)\n",
      "([np.float64(0.6000000000000001), np.float64(0.20000000000000018), np.float64(0.6000000000000001)], 0.9997)\n",
      "([np.float64(0.6000000000000001), np.float64(0.20000000000000018), np.float64(0.40000000000000013)], 0.9997)\n",
      "([np.float64(0.6000000000000001), np.float64(0.20000000000000018), np.float64(0.20000000000000018)], 0.9996)\n",
      "([np.float64(0.6000000000000001), np.float64(0.20000000000000018), np.float64(2.220446049250313e-16)], 0.9996)\n",
      "([np.float64(0.6000000000000001), np.float64(2.220446049250313e-16), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.6000000000000001), np.float64(2.220446049250313e-16), np.float64(0.8)], 0.9988)\n",
      "([np.float64(0.6000000000000001), np.float64(2.220446049250313e-16), np.float64(0.6000000000000001)], 0.9997)\n",
      "([np.float64(0.6000000000000001), np.float64(2.220446049250313e-16), np.float64(0.40000000000000013)], 0.9997)\n",
      "([np.float64(0.6000000000000001), np.float64(2.220446049250313e-16), np.float64(0.20000000000000018)], 0.9996)\n",
      "([np.float64(0.6000000000000001), np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16)], 0.9996)\n",
      "([np.float64(0.40000000000000013), np.float64(1.0), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(1.0), np.float64(0.8)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(1.0), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(1.0), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(1.0), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(1.0), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(0.8), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(0.8), np.float64(0.8)], 0.9967)\n",
      "([np.float64(0.40000000000000013), np.float64(0.8), np.float64(0.6000000000000001)], 0.999)\n",
      "([np.float64(0.40000000000000013), np.float64(0.8), np.float64(0.40000000000000013)], 0.9992)\n",
      "([np.float64(0.40000000000000013), np.float64(0.8), np.float64(0.20000000000000018)], 0.9991)\n",
      "([np.float64(0.40000000000000013), np.float64(0.8), np.float64(2.220446049250313e-16)], 0.9991)\n",
      "([np.float64(0.40000000000000013), np.float64(0.6000000000000001), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(0.6000000000000001), np.float64(0.8)], 0.9984)\n",
      "([np.float64(0.40000000000000013), np.float64(0.6000000000000001), np.float64(0.6000000000000001)], 0.9996)\n",
      "([np.float64(0.40000000000000013), np.float64(0.6000000000000001), np.float64(0.40000000000000013)], 0.9995)\n",
      "([np.float64(0.40000000000000013), np.float64(0.6000000000000001), np.float64(0.20000000000000018)], 0.9995)\n",
      "([np.float64(0.40000000000000013), np.float64(0.6000000000000001), np.float64(2.220446049250313e-16)], 0.9996)\n",
      "([np.float64(0.40000000000000013), np.float64(0.40000000000000013), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(0.40000000000000013), np.float64(0.8)], 0.9992)\n",
      "([np.float64(0.40000000000000013), np.float64(0.40000000000000013), np.float64(0.6000000000000001)], 0.9998)\n",
      "([np.float64(0.40000000000000013), np.float64(0.40000000000000013), np.float64(0.40000000000000013)], 0.9997)\n",
      "([np.float64(0.40000000000000013), np.float64(0.40000000000000013), np.float64(0.20000000000000018)], 0.9997)\n",
      "([np.float64(0.40000000000000013), np.float64(0.40000000000000013), np.float64(2.220446049250313e-16)], 0.9996)\n",
      "([np.float64(0.40000000000000013), np.float64(0.20000000000000018), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(0.20000000000000018), np.float64(0.8)], 0.9989)\n",
      "([np.float64(0.40000000000000013), np.float64(0.20000000000000018), np.float64(0.6000000000000001)], 0.9997)\n",
      "([np.float64(0.40000000000000013), np.float64(0.20000000000000018), np.float64(0.40000000000000013)], 0.9997)\n",
      "([np.float64(0.40000000000000013), np.float64(0.20000000000000018), np.float64(0.20000000000000018)], 0.9996)\n",
      "([np.float64(0.40000000000000013), np.float64(0.20000000000000018), np.float64(2.220446049250313e-16)], 0.9996)\n",
      "([np.float64(0.40000000000000013), np.float64(2.220446049250313e-16), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.40000000000000013), np.float64(2.220446049250313e-16), np.float64(0.8)], 0.999)\n",
      "([np.float64(0.40000000000000013), np.float64(2.220446049250313e-16), np.float64(0.6000000000000001)], 0.9998)\n",
      "([np.float64(0.40000000000000013), np.float64(2.220446049250313e-16), np.float64(0.40000000000000013)], 0.9998)\n",
      "([np.float64(0.40000000000000013), np.float64(2.220446049250313e-16), np.float64(0.20000000000000018)], 0.9996)\n",
      "([np.float64(0.40000000000000013), np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16)], 0.9996)\n",
      "([np.float64(0.20000000000000018), np.float64(1.0), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(1.0), np.float64(0.8)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(1.0), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(1.0), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(1.0), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(1.0), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(0.8), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(0.8), np.float64(0.8)], 0.9977)\n",
      "([np.float64(0.20000000000000018), np.float64(0.8), np.float64(0.6000000000000001)], 0.9996)\n",
      "([np.float64(0.20000000000000018), np.float64(0.8), np.float64(0.40000000000000013)], 0.9995)\n",
      "([np.float64(0.20000000000000018), np.float64(0.8), np.float64(0.20000000000000018)], 0.9995)\n",
      "([np.float64(0.20000000000000018), np.float64(0.8), np.float64(2.220446049250313e-16)], 0.9996)\n",
      "([np.float64(0.20000000000000018), np.float64(0.6000000000000001), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(0.6000000000000001), np.float64(0.8)], 0.9986)\n",
      "([np.float64(0.20000000000000018), np.float64(0.6000000000000001), np.float64(0.6000000000000001)], 0.9998)\n",
      "([np.float64(0.20000000000000018), np.float64(0.6000000000000001), np.float64(0.40000000000000013)], 0.9997)\n",
      "([np.float64(0.20000000000000018), np.float64(0.6000000000000001), np.float64(0.20000000000000018)], 0.9997)\n",
      "([np.float64(0.20000000000000018), np.float64(0.6000000000000001), np.float64(2.220446049250313e-16)], 0.9997)\n",
      "([np.float64(0.20000000000000018), np.float64(0.40000000000000013), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(0.40000000000000013), np.float64(0.8)], 0.9994)\n",
      "([np.float64(0.20000000000000018), np.float64(0.40000000000000013), np.float64(0.6000000000000001)], 0.9998)\n",
      "([np.float64(0.20000000000000018), np.float64(0.40000000000000013), np.float64(0.40000000000000013)], 0.9998)\n",
      "([np.float64(0.20000000000000018), np.float64(0.40000000000000013), np.float64(0.20000000000000018)], 0.9999)\n",
      "([np.float64(0.20000000000000018), np.float64(0.40000000000000013), np.float64(2.220446049250313e-16)], 0.9999)\n",
      "([np.float64(0.20000000000000018), np.float64(0.20000000000000018), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(0.20000000000000018), np.float64(0.8)], 0.9993)\n",
      "([np.float64(0.20000000000000018), np.float64(0.20000000000000018), np.float64(0.6000000000000001)], 0.9999)\n",
      "([np.float64(0.20000000000000018), np.float64(0.20000000000000018), np.float64(0.40000000000000013)], 0.9999)\n",
      "([np.float64(0.20000000000000018), np.float64(0.20000000000000018), np.float64(0.20000000000000018)], 0.9999)\n",
      "([np.float64(0.20000000000000018), np.float64(0.20000000000000018), np.float64(2.220446049250313e-16)], 0.9999)\n",
      "([np.float64(0.20000000000000018), np.float64(2.220446049250313e-16), np.float64(1.0)], 0.1023)\n",
      "([np.float64(0.20000000000000018), np.float64(2.220446049250313e-16), np.float64(0.8)], 0.9993)\n",
      "([np.float64(0.20000000000000018), np.float64(2.220446049250313e-16), np.float64(0.6000000000000001)], 0.9999)\n",
      "([np.float64(0.20000000000000018), np.float64(2.220446049250313e-16), np.float64(0.40000000000000013)], 0.9999)\n",
      "([np.float64(0.20000000000000018), np.float64(2.220446049250313e-16), np.float64(0.20000000000000018)], 0.9999)\n",
      "([np.float64(0.20000000000000018), np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(1.0), np.float64(1.0)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(1.0), np.float64(0.8)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(1.0), np.float64(0.6000000000000001)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(1.0), np.float64(0.40000000000000013)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(1.0), np.float64(0.20000000000000018)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(1.0), np.float64(2.220446049250313e-16)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.8), np.float64(1.0)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.8), np.float64(0.8)], 0.9975)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.8), np.float64(0.6000000000000001)], 0.9995)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.8), np.float64(0.40000000000000013)], 0.9995)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.8), np.float64(0.20000000000000018)], 0.9994)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.8), np.float64(2.220446049250313e-16)], 0.9996)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.6000000000000001), np.float64(1.0)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.6000000000000001), np.float64(0.8)], 0.9986)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.6000000000000001), np.float64(0.6000000000000001)], 0.9997)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.6000000000000001), np.float64(0.40000000000000013)], 0.9998)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.6000000000000001), np.float64(0.20000000000000018)], 0.9998)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.6000000000000001), np.float64(2.220446049250313e-16)], 0.9998)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.40000000000000013), np.float64(1.0)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.40000000000000013), np.float64(0.8)], 0.9991)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.40000000000000013), np.float64(0.6000000000000001)], 0.9998)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.40000000000000013), np.float64(0.40000000000000013)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.40000000000000013), np.float64(0.20000000000000018)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.40000000000000013), np.float64(2.220446049250313e-16)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.20000000000000018), np.float64(1.0)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.20000000000000018), np.float64(0.8)], 0.9991)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.20000000000000018), np.float64(0.6000000000000001)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.20000000000000018), np.float64(0.40000000000000013)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.20000000000000018), np.float64(0.20000000000000018)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(0.20000000000000018), np.float64(2.220446049250313e-16)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16), np.float64(1.0)], 0.1023)\n",
      "([np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16), np.float64(0.8)], 0.9992)\n",
      "([np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16), np.float64(0.6000000000000001)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16), np.float64(0.40000000000000013)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16), np.float64(0.20000000000000018)], 0.9999)\n",
      "([np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16), np.float64(2.220446049250313e-16)], 0.9999)\n"
     ]
    }
   ],
   "source": [
    "for i in acc_data:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
