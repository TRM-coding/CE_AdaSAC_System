{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data set\n",
      "create loader\n",
      "construct data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.52s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace\n",
    "\n",
    "from torch import nn\n",
    "from dataloader import load_data\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "train_inputs,train_lables,test_inputs,test_lables=load_data(60000,60000,device=torch.device('cuda:5'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1519498/197572307.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mlp = torch.load('./mlp.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "tensor([[ 0.0021,  0.0352,  0.0155,  ...,  0.0420,  0.0046,  0.0068],\n",
      "        [-0.0354, -0.0184, -0.0227,  ..., -0.0311, -0.0369,  0.0378],\n",
      "        [-0.0101, -0.0039,  0.0147,  ..., -0.0220, -0.0107, -0.0076],\n",
      "        ...,\n",
      "        [-0.0097, -0.0063, -0.0074,  ...,  0.2867, -0.0310, -0.0026],\n",
      "        [-0.0365, -0.0175, -0.0040,  ...,  0.0108,  0.2454,  0.0023],\n",
      "        [ 0.0231,  0.0455,  0.0373,  ...,  0.0011, -0.0090,  0.2486]],\n",
      "       device='cuda:5', grad_fn=<LinalgSvdBackward0>)\n",
      "tensor([[ 0.0592, -0.0850,  0.0028,  ...,  0.0030, -0.0024, -0.0034],\n",
      "        [-0.0401, -0.0265, -0.0410,  ...,  0.0113,  0.0107, -0.0168],\n",
      "        [ 0.0226, -0.0419,  0.0148,  ..., -0.0375, -0.0224, -0.0699],\n",
      "        ...,\n",
      "        [ 0.0056, -0.0447,  0.0185,  ...,  0.0177, -0.0181, -0.0476],\n",
      "        [ 0.0067, -0.0162, -0.0211,  ..., -0.0392, -0.0498, -0.0117],\n",
      "        [-0.0233, -0.0213,  0.0196,  ...,  0.0291, -0.0618, -0.0314]],\n",
      "       device='cuda:5', grad_fn=<LinalgSvdBackward0>)\n",
      "tensor([[-0.0346,  0.0039,  0.0879,  ...,  0.0747,  0.0072,  0.0630],\n",
      "        [-0.0563, -0.0005,  0.0344,  ...,  0.0468,  0.0432,  0.0882],\n",
      "        [ 0.0177,  0.0245, -0.0622,  ..., -0.0019, -0.0283,  0.0264],\n",
      "        ...,\n",
      "        [ 0.0661, -0.0127, -0.0282,  ..., -0.0671,  0.0346, -0.0153],\n",
      "        [ 0.0524,  0.0129,  0.0042,  ..., -0.0326, -0.0019, -0.0060],\n",
      "        [-0.0321,  0.0541, -0.1054,  ..., -0.0136, -0.0456,  0.0838]],\n",
      "       device='cuda:5', grad_fn=<LinalgSvdBackward0>)\n",
      "tensor([[ 0.0354,  0.0438,  0.0344,  ...,  0.0827, -0.0260,  0.0466],\n",
      "        [ 0.0289,  0.0519,  0.1068,  ..., -0.0767, -0.0093,  0.0148],\n",
      "        [ 0.0417,  0.0474,  0.0672,  ...,  0.0182,  0.0130, -0.0006],\n",
      "        ...,\n",
      "        [-0.0425, -0.1003,  0.0131,  ...,  0.1048,  0.0286, -0.1017],\n",
      "        [ 0.0073,  0.0425,  0.0250,  ..., -0.0680, -0.0551,  0.0003],\n",
      "        [ 0.0327, -0.0908,  0.0558,  ..., -0.0430,  0.0174, -0.0836]],\n",
      "       device='cuda:5', grad_fn=<LinalgSvdBackward0>)\n",
      "tensor([[ 0.1390,  0.0256, -0.0469,  ..., -0.0466, -0.1278, -0.0625],\n",
      "        [ 0.0190,  0.1364, -0.1054,  ...,  0.0782, -0.0870,  0.0124],\n",
      "        [ 0.0221, -0.0023, -0.0464,  ..., -0.0392,  0.0458,  0.0589],\n",
      "        ...,\n",
      "        [-0.0207, -0.0925, -0.0506,  ...,  0.1046, -0.0652,  0.1509],\n",
      "        [ 0.0731,  0.0659,  0.0356,  ...,  0.1895,  0.0421, -0.0614],\n",
      "        [ 0.0304,  0.0992,  0.0456,  ...,  0.0484, -0.0527,  0.1188]],\n",
      "       device='cuda:5', grad_fn=<LinalgSvdBackward0>)\n",
      "tensor([[ 0.0487, -0.2071,  0.0056,  ...,  0.0340, -0.1360, -0.0196],\n",
      "        [ 0.1231,  0.0916,  0.0332,  ..., -0.1739, -0.0548,  0.1101],\n",
      "        [ 0.0409,  0.1293,  0.2805,  ...,  0.1690, -0.0848,  0.0771],\n",
      "        ...,\n",
      "        [ 0.0307,  0.1380,  0.1486,  ..., -0.0563,  0.1266,  0.0341],\n",
      "        [ 0.1463,  0.0795,  0.0583,  ...,  0.0931, -0.1598, -0.0423],\n",
      "        [-0.0608, -0.1501, -0.0580,  ...,  0.0279, -0.2260, -0.0764]],\n",
      "       device='cuda:5', grad_fn=<LinalgSvdBackward0>)\n",
      "tensor([[-0.0714, -0.1362, -0.0596,  ...,  0.1472, -0.1729,  0.0548],\n",
      "        [ 0.2384,  0.0413, -0.0723,  ..., -0.0193, -0.1684,  0.3745],\n",
      "        [-0.1655,  0.0323,  0.3632,  ...,  0.0192, -0.1151,  0.0253],\n",
      "        ...,\n",
      "        [ 0.0839,  0.3912, -0.0310,  ...,  0.1999,  0.1380, -0.0948],\n",
      "        [ 0.0453, -0.0734,  0.1055,  ...,  0.2966, -0.1294, -0.1970],\n",
      "        [-0.1285,  0.0152,  0.0602,  ..., -0.1620, -0.1826,  0.0569]],\n",
      "       device='cuda:5', grad_fn=<LinalgSvdBackward0>)\n",
      "tensor([[-0.2021,  0.1244,  0.0252,  0.1055, -0.6567,  0.1942,  0.4890,  0.4687,\n",
      "         -0.0366,  0.0540],\n",
      "        [-0.1938,  0.3913,  0.4831,  0.0053, -0.3210,  0.2389, -0.5102, -0.2537,\n",
      "          0.1177,  0.2780],\n",
      "        [ 0.3365,  0.2277,  0.2066,  0.3745, -0.0688, -0.3420, -0.2105,  0.3386,\n",
      "          0.3803, -0.4761],\n",
      "        [-0.0716, -0.1427, -0.3988, -0.1582,  0.1062,  0.3495, -0.4429,  0.5371,\n",
      "          0.3693,  0.1900],\n",
      "        [ 0.3134,  0.0555, -0.0282,  0.7066,  0.2131,  0.1760,  0.0090,  0.1018,\n",
      "         -0.2639,  0.4921],\n",
      "        [ 0.0599,  0.3563,  0.4270, -0.4158,  0.4444, -0.1103,  0.1714,  0.4551,\n",
      "         -0.1553,  0.2097],\n",
      "        [-0.1633, -0.3507,  0.1057,  0.0336, -0.1339, -0.6601,  0.0111,  0.0440,\n",
      "          0.2941,  0.5439],\n",
      "        [-0.3963,  0.0963,  0.1162,  0.2551,  0.3986,  0.2343,  0.3997, -0.1660,\n",
      "          0.5942, -0.0289],\n",
      "        [ 0.1163, -0.7029,  0.5963,  0.0161,  0.0115,  0.3184, -0.0188,  0.1191,\n",
      "         -0.0262, -0.1409],\n",
      "        [ 0.7101,  0.0536, -0.0312, -0.2912, -0.1729,  0.1762,  0.2652, -0.2275,\n",
      "          0.4120,  0.2332]], device='cuda:5', grad_fn=<LinalgSvdBackward0>)\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "----------------\n",
      "0.9213\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import model\n",
    "# from model import Bias\n",
    "from torch import nn\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "\n",
    "def acc(model):\n",
    "    model.eval()\n",
    "    correct=0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in range(len(test_inputs)):\n",
    "            outputs=model(test_inputs[i])\n",
    "            _,predicted=torch.max(outputs.data,1)\n",
    "            total+=len(predicted)\n",
    "            correct+=(predicted==test_lables[i]).sum().item()\n",
    "    model.train()\n",
    "    return correct/total\n",
    "\n",
    "\n",
    "mlp = torch.load('./mlp.pth')\n",
    "print(len(mlp.layers))\n",
    "device=next(mlp.parameters()).device\n",
    "\n",
    "for k,layer in enumerate(mlp.layers):\n",
    "    if (isinstance(layer, torch.nn.Linear)):\n",
    "        w=layer.weight\n",
    "        b=layer.bias\n",
    "        U,S,V=torch.linalg.svd(w)\n",
    "        print(U)\n",
    "        nU=U.shape[0]\n",
    "        mU=U.shape[1]\n",
    "        nV=V.shape[0]\n",
    "        mV=V.shape[1]\n",
    "        nS=mU\n",
    "        mS=nV\n",
    "        S_diag = torch.zeros(nS,mS)\n",
    "        for i in range(S.shape[0]):\n",
    "            S_diag[i, i] = S[i]\n",
    "        S=S_diag\n",
    "\n",
    "        newlinear1=torch.nn.Linear(nU,mU,bias=False).to(device)\n",
    "        newlinear2=torch.nn.Linear(nS,mS,bias=False).to(device)\n",
    "        newlinear3=torch.nn.Linear(nV,mV,bias=False).to(device)\n",
    "        newlinear1.weight=nn.Parameter(U)\n",
    "        newlinear2.weight=nn.Parameter(S)\n",
    "        newlinear3.weight=nn.Parameter(V)\n",
    "        newbias=model.Bias(b)\n",
    "        newbias=newbias.to(device)\n",
    "        svded=nn.Sequential(newlinear3,newlinear2,newlinear1,newbias)\n",
    "        mlp.layers[k]=svded\n",
    "\n",
    "mlp.eval()\n",
    "mlp.to(device)\n",
    "for layer in mlp.layers:\n",
    "    if(isinstance(layer,torch.nn.Sequential)):\n",
    "        for sublayer in layer:\n",
    "            sublayer.to(device)\n",
    "            if(not isinstance(sublayer,nn.Module)):\n",
    "                print(sublayer.weight.shape)\n",
    "    print('----------------')\n",
    "\n",
    "print(acc(mlp))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
