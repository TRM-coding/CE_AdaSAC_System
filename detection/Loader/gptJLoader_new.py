import torch
import torch.nn as nn
from transformers import AutoTokenizer
from modelscope.utils.hub import snapshot_download
from mymodel_file.gptJ_cloud import gptJ_cloud
from mymodel_file.gptJ_edge import gptJ_edge
import torch.nn.functional as F
from datasets import load_dataset
from transformers import DataCollatorForLanguageModeling
from torch.utils.data import DataLoader
from detection.SVD_model import SVDED_GPTJ_EDGE_Layer

class CloudEdgeCollaborativeGPTJ(nn.Module):
    """
    äº‘è¾¹ååŒGPTJ-6Bæ¨ç†æ¨¡å—
    
    ååŒè®¡ç®—æµç¨‹ï¼š
    1. äº‘ä¾§ï¼šè®¡ç®— Q, K çŸ©é˜µä»¥åŠæ³¨æ„åŠ›æƒé‡ (Q @ K^T)
    2. è¾¹ä¾§ï¼šè®¡ç®— V çŸ©é˜µå’Œåç»­çš„æ³¨æ„åŠ›è®¡ç®—
    3. äº‘ä¾§å°†æ³¨æ„åŠ›æƒé‡ä¼ ç»™è¾¹ä¾§ï¼Œè¾¹ä¾§å®Œæˆå‰©ä½™è®¡ç®—
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    - forward: æ ‡å‡†å‰å‘ä¼ æ’­ï¼Œæ— ç¼“å­˜æœºåˆ¶
    - generate: ç”Ÿæˆæ¨¡å¼ï¼Œå¸¦K/Vç¼“å­˜ä¼˜åŒ–
    """
    
    def __init__(self, model_name='AI-ModelScope/gpt-j-6b', device_cloud='cuda:0', device_edge='cpu'):
        super().__init__()
        
        # ä¸‹è½½æˆ–åŠ è½½æ¨¡å‹
        if not torch.cuda.is_available():
            device_cloud = 'cpu'
            
        print(f"ğŸ“¥ ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹ {model_name}...")
        model_dir = snapshot_download(
            repo_id=model_name,
            cache_dir='./gpt-j-6b'
        )
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œè·¯å¾„: {model_dir}")
        
        # åŠ è½½tokenizer
        print(f"ğŸ”¤ åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½äº‘ç«¯å’Œè¾¹ç«¯æ¨¡å‹
        print(f"â˜ï¸  åŠ è½½äº‘ç«¯æ¨¡å‹åˆ° {device_cloud}...")
        self.cloud = gptJ_cloud(model_name=model_dir).to(device_cloud)
        
        print(f"ğŸ–¥ï¸  åŠ è½½è¾¹ç¼˜æ¨¡å‹åˆ° {device_edge}...")
        self.edge = gptJ_edge(model_name=model_dir).to(device_edge)
        
        # è·å–å…±äº«ç»„ä»¶
        self.embed = self.cloud.model.transformer.wte
        self.ln_f = self.cloud.model.transformer.ln_f
        self.lm_head = self.cloud.model.lm_head
        self.num_layers = len(self.cloud.q_weights)
        
        # ä¿å­˜è®¾å¤‡ä¿¡æ¯
        self.device_cloud = device_cloud
        self.device_edge = device_edge
        
        print(f"ğŸ¯ äº‘è¾¹ååŒæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå…± {self.num_layers} å±‚")
        
    def reset_cache(self):
        """é‡ç½®æ‰€æœ‰ç¼“å­˜"""
        self.cloud.k_cache = [None] * self.num_layers
        self.edge.v_cache = [None] * self.num_layers
    
    def forward(self, input_ids, attention_mask=None):
        """
        æ ‡å‡†å‰å‘ä¼ æ’­ï¼ˆæ— ç¼“å­˜ï¼‰
        
        Args:
            input_ids: [batch_size, seq_len] tokenåºåˆ—
            attention_mask: å¯é€‰çš„æ³¨æ„åŠ›æ©ç 
            
        Returns:
            logits: [batch_size, seq_len, vocab_size] è¾“å‡ºlogits
        """
        device = input_ids.device
        batch_size, seq_len = input_ids.shape
        
        # ç§»åŠ¨åµŒå…¥å±‚åˆ°è¾“å…¥è®¾å¤‡
        self.embed = self.embed.to(device)
        self.ln_f = self.ln_f.to(device)
        self.lm_head = self.lm_head.to(device)
        
        # TokenåµŒå…¥
        x = self.embed(input_ids)  # [batch_size, seq_len, hidden_size]
        
        # é€å±‚ååŒè®¡ç®—
        for layer_idx in range(self.num_layers):
            # 1. äº‘ä¾§è®¡ç®—ï¼šQ, KçŸ©é˜µå’Œæ³¨æ„åŠ›æƒé‡
            q, k, attn_weights = self.cloud.forward_no_cache(x, layer_idx)
            
            # 2. å°†æ³¨æ„åŠ›æƒé‡ä¼ è¾“åˆ°è¾¹ä¾§ (æ¨¡æ‹Ÿç½‘ç»œä¼ è¾“)
            attn_weights_edge = attn_weights.to(self.device_edge)
            x_edge = x.to(self.device_edge)
            
            # 3. è¾¹ä¾§è®¡ç®—ï¼šVçŸ©é˜µå’Œåç»­æ“ä½œ
            v, x_out = self.edge.forward_no_cache(x_edge, layer_idx, attn_weights_edge)
            
            # 4. å°†ç»“æœä¼ å›äº‘ä¾§ (æˆ–ä¿æŒåœ¨è¾¹ä¾§ï¼Œæ ¹æ®ä¸‹ä¸€å±‚çš„éœ€è¦)
            x = x_out.to(device)
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–å’Œè¾“å‡ºæŠ•å½±
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        return logits
    
    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, top_k=50, do_sample=True):
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰
        
        Args:
            input_ids: [batch_size, seq_len] åˆå§‹tokenåºåˆ—
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_k: top-ké‡‡æ ·
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            
        Returns:
            generated_ids: [batch_size, seq_len + max_new_tokens] ç”Ÿæˆçš„å®Œæ•´åºåˆ—
        """
        device = input_ids.device
        batch_size, initial_seq_len = input_ids.shape
        
        # é‡ç½®ç¼“å­˜
        self.reset_cache()
        
        # ç§»åŠ¨åµŒå…¥å±‚åˆ°æ­£ç¡®è®¾å¤‡
        self.embed = self.embed.to(device)
        self.ln_f = self.ln_f.to(device)
        self.lm_head = self.lm_head.to(device)
        
        generated_ids = input_ids.clone()
        
        # æ­£ç¡®çš„ç¼“å­˜åˆå§‹åŒ–ï¼šé€ä¸ªtokenå¤„ç†å®Œæ•´çš„prompt
        with torch.no_grad():
            for i in range(initial_seq_len):
                current_token = input_ids[:, i:i+1]  # [batch_size, 1]
                x = self.embed(current_token)
                
                # é€å±‚å¤„ç†ï¼Œæ­£ç¡®æ›´æ–°éšè—çŠ¶æ€
                for layer_idx in range(self.num_layers):
                    # 1. äº‘ä¾§ï¼šä½¿ç”¨ç¼“å­˜è®¡ç®—Q, Kå’Œæ³¨æ„åŠ›æƒé‡
                    q, k_all, attn_weights = self.cloud.forward_cache(x, layer_idx)
                    
                    # 2. ä¼ è¾“åˆ°è¾¹ä¾§
                    x_edge = x.to(self.device_edge)
                    attn_weights_to_edge = attn_weights.to(self.device_edge)
                    
                    # 3. è¾¹ä¾§ï¼šä½¿ç”¨ç¼“å­˜è®¡ç®—Vå’Œåç»­æ“ä½œ
                    v_all, x_out = self.edge.forward_cache(x_edge, layer_idx, attn_weights_to_edge)
                    
                    # 4. ä¼ å›äº‘ä¾§ç”¨äºä¸‹ä¸€å±‚
                    x = x_out.to(device)
        
        # é€ä¸ªç”Ÿæˆæ–°token
        for step in range(max_new_tokens):
            with torch.no_grad():
                # åªå¯¹æœ€åä¸€ä¸ªtokenè¿›è¡Œå‰å‘ä¼ æ’­
                current_token = generated_ids[:, -1:]  # [batch_size, 1]
                logits = self._forward_with_cache(current_token)
                
                # åªå–æœ€åä¸€ä¸ªtokençš„logits
                next_token_logits = logits[:, -1, :] / temperature  # [batch_size, vocab_size]
                
                # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
                if do_sample:
                    if top_k > 0:
                        # Top-ké‡‡æ ·
                        indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                        next_token_logits[indices_to_remove] = float('-inf')
                    
                    probs = F.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    # è´ªå¿ƒè§£ç 
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                
                # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
                generated_ids = torch.cat([generated_ids, next_token], dim=-1)
                
                # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æŸç¬¦
                if next_token.item() == self.tokenizer.eos_token_id:
                    break
        
        return generated_ids
    
    def _forward_with_cache(self, input_ids):
        """
        å¸¦ç¼“å­˜çš„å‰å‘ä¼ æ’­ï¼ˆç”¨äºç”Ÿæˆï¼‰
        
        Args:
            input_ids: [batch_size, seq_len] å½“å‰è¾“å…¥token
            
        Returns:
            logits: [batch_size, seq_len, vocab_size]
        """
        device = input_ids.device
        x = self.embed(input_ids)
        
        for layer_idx in range(self.num_layers):
            # 1. äº‘ä¾§ï¼šä½¿ç”¨ç¼“å­˜è®¡ç®—Q, Kå’Œæ³¨æ„åŠ›æƒé‡
            q, k_all, attn_weights = self.cloud.forward_cache(x, layer_idx)
            
            # 2. ä¼ è¾“åˆ°è¾¹ä¾§çš„æ•°æ®ä¼˜åŒ–ï¼š
            # å¯¹äºç”Ÿæˆï¼Œæˆ‘ä»¬åªéœ€è¦æœ€æ–°tokenä¸æ‰€æœ‰å†å²tokençš„æ³¨æ„åŠ›æƒé‡
            # attn_weights shape: [batch, num_heads, seq_q, seq_k]
            x_edge = x.to(self.device_edge)
            attn_weights_to_edge = attn_weights.to(self.device_edge)
            
            # 3. è¾¹ä¾§ï¼šä½¿ç”¨ç¼“å­˜è®¡ç®—Vå’Œåç»­æ“ä½œ
            v_all, x_out = self.edge.forward_cache(x_edge, layer_idx, attn_weights_to_edge)
            
            # 4. ä¼ å›äº‘ä¾§ç”¨äºä¸‹ä¸€å±‚
            x = x_out.to(device)
        
        # æœ€ç»ˆè¾“å‡º
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        return logits
    
    def generate_text(self, prompt, max_new_tokens=50, temperature=1.0, top_k=50, do_sample=True):
        """
        æ–‡æœ¬ç”Ÿæˆçš„ä¾¿æ·æ¥å£
        
        Args:
            prompt: è¾“å…¥æ–‡æœ¬æç¤º
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_k: top-ké‡‡æ ·
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            
        Returns:
            generated_text: ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬
        """
        # ç¼–ç è¾“å…¥
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
        
        # ç§»åŠ¨åˆ°äº‘ä¾§è®¾å¤‡
        input_ids = input_ids.to(self.device_cloud)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = self.generate(
                input_ids, 
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_k=top_k,
                do_sample=do_sample
            )
        
        # è§£ç 
        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        
        return generated_text
    
    def get_transfer_stats(self):
        """
        è·å–æ•°æ®ä¼ è¾“ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåˆ†æç½‘ç»œå¼€é”€ï¼‰
        
        Returns:
            dict: åŒ…å«ä¼ è¾“æ•°æ®é‡çš„ç»Ÿè®¡ä¿¡æ¯
        """
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„ä¼ è¾“é‡ç»Ÿè®¡
        # ä¸»è¦ä¼ è¾“ï¼šæ³¨æ„åŠ›æƒé‡ä»äº‘åˆ°è¾¹ï¼Œæœ€ç»ˆè¾“å‡ºä»è¾¹åˆ°äº‘
        attention_transfer_size = 0
        output_transfer_size = 0
        
        return {
            "attention_transfer_mb": attention_transfer_size / (1024**2),
            "output_transfer_mb": output_transfer_size / (1024**2),
            "total_transfer_mb": (attention_transfer_size + output_transfer_size) / (1024**2)
        }

from tqdm import tqdm
import math

class EVALER():
    def load_and_tokenize_dataset(self,cache_dir: str, tokenizer, batch_size: int = 1):
        """
        Loads and tokenizes the MiniPile dataset.

        Args:
            cache_dir: Directory where MiniPile is cached/downloaded.
            tokenizer: Tokenizer for tokenizing the dataset.
            batch_size: Batch size for evaluation.

        Returns:
            A DataLoader for the tokenized dataset.
        """
        # Load dataset
        ds = load_dataset("JeanKaddour/minipile", split="validation", cache_dir=cache_dir)

        # Tokenize dataset
        def tokenize_fn(examples):
            return tokenizer(examples['text'], padding=True, truncation=True, max_length=512)
        
        tokenized = ds.map(tokenize_fn, batched=True, remove_columns=["text"])

        # Group the dataset into blocks of block_size (use consistent max_length)
        block_size = 512  # Use the same as tokenization max_length
        def group_texts(examples):
            all_ids = sum(examples["input_ids"], [])
            total_len = (len(all_ids) // block_size) * block_size
            blocks = [all_ids[i:i + block_size] for i in range(0, total_len, block_size)]
            return {"input_ids": blocks}

        lm_dataset = tokenized.map(group_texts, batched=True, remove_columns=["attention_mask"])

        # DataLoader setup
        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
        dataloader = DataLoader(lm_dataset, batch_size=batch_size, collate_fn=data_collator)

        return dataloader


    def evaluate_minipile_gptj(self,model, batch_size: int = 1, cache_dir: str = "./minipile_cache", Dataloader=None) -> dict:
        """
        Evaluates a GPTJ-6B model instance on the MiniPile dataset.

        Args:
            model: A transformers.GPTJForCausalLM instance.
            batch_size: Batch size for evaluation.
            cache_dir: Directory where MiniPile is cached/downloaded.

        Returns:
            A dict with keys:
                - "avg_loss": Average cross-entropy loss.
                - "perplexity": Exponential of the average loss.
        """
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model.eval()

        # Load and tokenize dataset
        tokenizer = model.tokenizer  # already initialized in the pipeline
        dataloader = None
        if Dataloader is None:
            dataloader = self.load_and_tokenize_dataset(cache_dir, tokenizer, batch_size)
        else:
            dataloader = Dataloader

        # Initialize loss function with ignore_index=-100 to skip padding tokens
        criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=-100)

        # Evaluation loop
        total_loss = 0.0
        total_batches = 0

        # model.eval()
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(dataloader, desc="Evaluating")):
                # æ‹¿åˆ°å®Œæ•´çš„ input_ids, attention_mask, å’Œå·²ç»è¢« collator è®¾å¥½ -100 çš„ labels
                input_ids    = batch['input_ids'].to(device)       # [B, T]
                attention_mask = batch['attention_mask'].to(device)# [B, T]
                labels       = batch['labels'].to(device)          # [B, T], pad å·²ç»æ˜¯ -100

              

                with torch.no_grad():
                    outputs = model(input_ids=input_ids)
                    logits  = outputs                     # [B, T, V]

                
                # æ‰‹åŠ¨ shiftï¼šlogits ä¸¢æ‰æœ€åä¸€ä½ï¼Œlabels ä¸¢æ‰ç¬¬ä¸€ä½
                shift_logits = logits[:, :-1, :].contiguous()    # [B, T-1, V]
                shift_labels = labels[:, 1:].contiguous()        # [B, T-1]

                # è®¡ç®—äº¤å‰ç†µ lossï¼Œignore_index=-100 ä¼šè·³è¿‡æ‰€æœ‰ pad ä½ç½®
                loss = criterion(
                    shift_logits.view(-1, shift_logits.size(-1)),  # [(B*(T-1)), V]
                    shift_labels.view(-1)                          # [(B*(T-1))]
                )
                
               
                total_loss   += loss.item()
                total_batches+= 1


            avg_loss = total_loss / total_batches
            perplexity = math.exp(avg_loss)

        return {"avg_loss": avg_loss, "perplexity": perplexity}


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    model_name = 'AI-ModelScope/gpt-j-6b'
    device_cloud = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    device_edge = 'cuda:0'
    
    # åˆ›å»ºäº‘è¾¹ååŒæ¨¡å‹
    collaborative_model = CloudEdgeCollaborativeGPTJ(
        model_name=model_name,
        device_cloud=device_cloud,
        device_edge=device_edge
    )
    
    # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
    prompt = "Once upon a time, in a distant galaxy"
    print(f"ğŸ”¸ è¾“å…¥æç¤º: {prompt}")
    
    generated_text = collaborative_model.generate_text(
        prompt, 
        max_new_tokens=30,
        temperature=0.8,
        top_k=50
    )
    
    print(f"ğŸ”¸ ç”Ÿæˆæ–‡æœ¬: {generated_text}")
    
    # æµ‹è¯•æ ‡å‡†å‰å‘ä¼ æ’­
    input_ids = collaborative_model.tokenizer.encode(prompt, return_tensors='pt')
    input_ids = input_ids.to(device_cloud)
    
    with torch.no_grad():
        logits = collaborative_model.forward(input_ids)
        print(f"ğŸ”¸ Forwardè¾“å‡ºå½¢çŠ¶: {logits.shape}")

    eval=EVALER()

    dataloader=eval.load_and_tokenize_dataset(cache_dir='./minipile_cache',tokenizer=collaborative_model.tokenizer)
    eval.evaluate_minipile_gptj(collaborative_model,Dataloader=dataloader)
