{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ddea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load fused SVD extension: Ninja is required to load C++ extensions\n",
      "初始化云侧模型 (设备: cuda:0)...\n",
      "Downloading model AI-ModelScope/gpt-j-6b using ModelScope...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 21:20:01,202 - modelscope - WARNING - Model revision not specified, use revision: v1.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./gpt-j-6b/AI-ModelScope/gpt-j-6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./gpt-j-6b/AI-ModelScope/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化边侧模型 (设备: cuda:0)...\n",
      "Downloading model AI-ModelScope/gpt-j-6b using ModelScope...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 21:20:50,584 - modelscope - WARNING - Model revision not specified, use revision: v1.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./gpt-j-6b/AI-ModelScope/gpt-j-6b\n",
      "开始生成，初始prompt: 'The future of artificial intelligence is'\n",
      "目标生成长度: 20 tokens\n",
      "初始token数: 6\n",
      "使用SVD压缩: False\n",
      "生成进度: 0/20\n",
      "  Step 0: token_id=257, token=' a'\n",
      "  Step 1: token_id=7243, token=' topic'\n",
      "  Step 2: token_id=326, token=' that'\n",
      "  Step 3: token_id=468, token=' has'\n",
      "  Step 4: token_id=587, token=' been'\n",
      "生成进度: 5/20\n",
      "  Step 5: token_id=50180, token=' hotly'\n",
      "  Step 6: token_id=24594, token=' debated'\n",
      "  Step 7: token_id=329, token=' for'\n",
      "  Step 8: token_id=4647, token=' decades'\n",
      "  Step 9: token_id=13, token='.'\n",
      "生成进度: 10/20\n",
      "生成进度: 15/20\n",
      "\n",
      "生成完成!\n",
      "总时间: 0.000s\n",
      "生成的token数: 20\n",
      "平均每token时间: 0.000s\n",
      "压缩信息: {'compressed': False}\n",
      "生成结果: The future of artificial intelligence is a topic that has been hotly debated for decades. The debate has been fueled by the fact that AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since JeanKaddour/minipile couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at minipile_cache/JeanKaddour___minipile/default/0.0.0/18ad1b0c701eaa0de03d3cecfdd769cbc70ffbd0 (last modified on Tue Jul 29 20:23:40 2025).\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "from detection.Loader.mymodel_file.gptJ_edge import gptJ_edge_layer\n",
    "from detection.Loader.mymodel_file.gptJ_cloud import gptJ_cloud\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from modelscope.utils.hub import snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "from detection.SVD_model import SVDED_GPTJ_EDGE_Layer\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "datafolder=\"./GPTJ_SVD_DATA\"\n",
    "with open(f\"{datafolder}/cloud_flops_dict.pickle\",\"rb\") as f:\n",
    "    cloud_flops=pickle.load(f)\n",
    "\n",
    "datafolder=\"./GPTJ_SVD_DATA\"\n",
    "cloud_flops=0\n",
    "net_dict=0\n",
    "with open(f\"{datafolder}/cloud_flops_dict.pickle\",\"rb\") as f:\n",
    "    cloud_flops=pickle.load(f)\n",
    "\n",
    "with open(f\"{datafolder}/net_dict.pickle\",\"rb\") as f:\n",
    "    net_dict=pickle.load(f)\n",
    "\n",
    "\n",
    "class gptJ_edge(nn.Module):\n",
    "    def __init__(self, model_name='AI-ModelScope/gpt-j-6b',svd=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 如果是 HuggingFace 仓库名，使用 ModelScope 下载\n",
    "        if not os.path.exists(model_name):\n",
    "            print(f\"Downloading model {model_name} using ModelScope...\")\n",
    "            model_path = snapshot_download(\n",
    "                repo_id=model_name,\n",
    "                cache_dir='./gpt-j-6b'\n",
    "            )\n",
    "        else:\n",
    "            model_path = model_name\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.num_layers = 28\n",
    "\n",
    "    \n",
    "    def forward_no_cache(self, x, layer_idx, attn_weights):\n",
    "        return self.layers[layer_idx].forward_no_cache(x, attn_weights)\n",
    "    \n",
    "    # 在 gptJ_edge_layer 类中添加 clear 方法\n",
    "    def clear(self):\n",
    "        for layeri in self.layers:\n",
    "            if isinstance(layeri,gptJ_edge_layer):\n",
    "                del layeri\n",
    "                continue\n",
    "            del layeri.v_svd['U']\n",
    "            del layeri.v_svd['V']\n",
    "            del layeri.v_svd['bias']\n",
    "            del layeri.out_proj_svd['U']\n",
    "            del layeri.out_proj_svd['V']\n",
    "            del layeri.out_proj_svd['bias']\n",
    "            del layeri.fc_in_svd['U']\n",
    "            del layeri.fc_in_svd['V']\n",
    "            del layeri.fc_in_svd['bias']\n",
    "            del layeri.fc_out_svd['U']           \n",
    "            del layeri.fc_out_svd['V']        \n",
    "            del layeri.fc_out_svd['bias']\n",
    "            del layeri\n",
    "        del self.layers\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        self.layers=nn.ModuleList()\n",
    "    \n",
    "    def add_layer(self,layeri,device):\n",
    "        layer=copy.deepcopy(layeri)\n",
    "        \n",
    "        if isinstance(layer,SVDED_GPTJ_EDGE_Layer):\n",
    "            layer.v_svd['U']=layer.v_svd['U'].to(device=device,)\n",
    "            layer.v_svd['V']=layer.v_svd['V'].to(device=device,)\n",
    "            layer.v_svd['bias']=layer.v_svd['bias'].to(device=device,  )\n",
    "            layer.out_proj_svd['U']=layer.out_proj_svd['U'].to(device=device,  )\n",
    "            layer.out_proj_svd['V']=layer.out_proj_svd['V'].to(device=device,  )\n",
    "            layer.out_proj_svd['bias']=layer.out_proj_svd['bias'].to(device=device,  )\n",
    "            layer.fc_in_svd['U']=layer.fc_in_svd['U'].to(device=device,  )\n",
    "            layer.fc_in_svd['V']=layer.fc_in_svd['V'].to(device=device,  )\n",
    "            layer.fc_in_svd['bias']=layer.fc_in_svd['bias'].to(device=device,  )\n",
    "            layer.fc_out_svd['U']=layer.fc_out_svd['U'].to(device=device,  )            \n",
    "            layer.fc_out_svd['V']=layer.fc_out_svd['V'].to(device=device,  )            \n",
    "            layer.fc_out_svd['bias']=layer.fc_out_svd['bias'].to(device=device,  )\n",
    "        \n",
    "        # 将整个层移动到设备，并确保数据类型\n",
    "        layer = layer.to(device=device,  )\n",
    "        self.layers.append(layer)\n",
    "        gc.collect()\n",
    "        return\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "class GPTJCloudEdgeCollaborator(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-J 云边协同模型\n",
    "    云侧：完成Q、K的计算和attention权重计算\n",
    "    边侧：完成V的计算和最终的attention输出\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='AI-ModelScope/gpt-j-6b', device_cloud='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device_cloud = device_cloud\n",
    "        self.device_edge = device_cloud\n",
    "        self.flops_edge=0\n",
    "        self.flops_cloud=0\n",
    "        \n",
    "        # 初始化云侧和边侧模型\n",
    "        print(f\"初始化云侧模型 (设备: {device_cloud})...\")\n",
    "        self.cloud = gptJ_cloud(model_name=model_name)\n",
    "        self.cloud =self.cloud.to(device_cloud)\n",
    "        \n",
    "        print(f\"初始化边侧模型 (设备: {device_cloud})...\")\n",
    "        self.edge = gptJ_edge(model_name=model_name,svd=True,).to(device_cloud)\n",
    "        \n",
    "        # 获取共享的组件（embedding和输出层）\n",
    "        self.embed = self.cloud.model.transformer.wte.to(device_cloud)\n",
    "        self.ln_f = self.cloud.model.transformer.ln_f.to(device_cloud)\n",
    "        self.lm_head = self.cloud.model.lm_head.to(device_cloud)\n",
    "        \n",
    "        # 模型配置\n",
    "        self.num_layers = len(self.cloud.q_weights)\n",
    "        self.vocab_size = self.cloud.model.config.vocab_size\n",
    "        \n",
    "        # 初始化tokenizer\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        except:\n",
    "            # 如果直接加载失败，尝试从本地路径加载\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('./gpt-j-6b/AI-ModelScope/gpt-j-6b')\n",
    "            \n",
    "        # 设置pad_token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # 标记是否使用SVD\n",
    "        self.use_svd = False\n",
    "        self.svd_layers = None\n",
    "        self.origin_layers= []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None,reduce_rate=0,need_embedding=True,flops_eval=False):\n",
    "        \"\"\"\n",
    "        前向传播用于数据集评估\n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len] token ids\n",
    "            attention_mask: [batch_size, seq_len] attention mask (1=valid, 0=padding)\n",
    "        Returns:\n",
    "            logits: [batch_size, seq_len, vocab_size] 预测logits\n",
    "        \"\"\"\n",
    "        # 1. Embedding\n",
    "        x=0\n",
    "        batch_size=0\n",
    "        seq_len=0\n",
    "        if need_embedding:\n",
    "            x = self.embed(input_ids.to(self.device_cloud))  # [B, T, D]\n",
    "        \n",
    "            batch_size, seq_len = input_ids.shape\n",
    "        else:\n",
    "            batch_size=input_ids.shape[0]\n",
    "            seq_len=input_ids.shape[1]\n",
    "            x=input_ids\n",
    "        \n",
    "        # 2. 如果没有提供attention_mask，根据pad_token_id生成\n",
    "        temp_=torch.ones(input_ids.shape[0],input_ids.shape[1])\n",
    "        if attention_mask is None:\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                attention_mask = (temp_ != self.tokenizer.pad_token_id).long()\n",
    "            else:\n",
    "                attention_mask = torch.ones(input_ids.shape[0],input_ids.shape[1])\n",
    "        \n",
    "        # 创建position_ids\n",
    "        position_ids = torch.arange(seq_len, device=self.device_cloud).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        prof_cloud = profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            record_shapes=True,\n",
    "            with_flops=True,\n",
    "        )\n",
    "        prof_edge = profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            record_shapes=True,\n",
    "            with_flops=True,\n",
    "        )\n",
    "        self.flops_edge=0\n",
    "        self.flops_cloud=0\n",
    "        # 3. 逐层处理\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            # 云侧：计算Q、K和attention权重（传入attention_mask和position_ids）\n",
    "            \n",
    "            if(flops_eval):\n",
    "                prof_cloud.start()\n",
    "            q, k, attn_weights = self.cloud.forward_no_cache(\n",
    "                    x, layer_idx, position_ids, attention_mask.to(self.device_cloud)\n",
    "            )\n",
    "            if flops_eval:\n",
    "                prof_cloud.stop()\n",
    "           \n",
    "            # 将数据传输到边侧设备\n",
    "            x_edge = x.to(self.device_edge)\n",
    "            attn_weights_edge = attn_weights.to(self.device_edge)\n",
    "            \n",
    "\n",
    "            \n",
    "            _, x_edge = self.edge.forward_no_cache(x_edge, layer_idx, attn_weights_edge)\n",
    "            self.flops_edge+=self.edge.layers[layer_idx].flops\n",
    "    \n",
    "\n",
    "            # 将结果传回云侧\n",
    "            x = x_edge.to(self.device_cloud)\n",
    "        \n",
    "        # 4. 最终的Layer Norm和LM Head\n",
    "        # if flops_eval:\n",
    "        #     self.flops_cloud = sum(evt.flops for evt in prof_cloud.key_averages())\n",
    "        #     self.flops_edge =sum(evt.flops for evt in prof_edge.key_averages())\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, prompt, max_length=50, temperature=1.0, top_p=0.9, do_sample=True):\n",
    "        \"\"\"文本生成方法 - 修复版本\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        # 编码输入\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')[0].tolist()\n",
    "        outputs = input_ids.copy()\n",
    "        \n",
    "        print(f\"开始生成，初始prompt: '{prompt}'\")\n",
    "        print(f\"目标生成长度: {max_length} tokens\")\n",
    "        print(f\"初始token数: {len(input_ids)}\")\n",
    "        print(f\"使用SVD压缩: {self.use_svd}\")\n",
    "        \n",
    "        # 统计时间\n",
    "        cloud_time = 0\n",
    "        edge_time = 0\n",
    "        transfer_time = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 逐token生成\n",
    "            for step in range(max_length):\n",
    "                if step % 5 == 0:\n",
    "                    print(f\"生成进度: {step}/{max_length}\")\n",
    "                \n",
    "                # 处理完整的序列\n",
    "                current_ids = torch.tensor([outputs]).to(self.device_cloud)  # [1, current_seq_len]\n",
    "                x = self.embed(current_ids)  # [1, current_seq_len, hidden_size]\n",
    "                \n",
    "                # 创建position_ids和attention_mask\n",
    "                seq_len = len(outputs)\n",
    "                position_ids = torch.arange(seq_len, device=self.device_cloud).unsqueeze(0)  # [1, seq_len]\n",
    "                \n",
    "                # 创建attention_mask（生成时所有token都是有效的）\n",
    "                attention_mask = torch.ones_like(current_ids)\n",
    "                \n",
    "                # 逐层处理\n",
    "                for layer_idx in range(self.num_layers):\n",
    "                    # 云侧计算（传入position_ids和attention_mask）\n",
    "              \n",
    "                    q, k, attn_weights = self.cloud.forward_no_cache(\n",
    "                        x, layer_idx, position_ids, attention_mask\n",
    "                    )\n",
    "                   \n",
    "                    \n",
    "                    # 数据传输到边侧\n",
    "                 \n",
    "                    x_edge = x.to(self.device_edge)\n",
    "                    attn_weights_edge = attn_weights.to(self.device_edge)\n",
    "                   \n",
    "                    \n",
    "                    # 边侧计算\n",
    "                 \n",
    "                    # if self.use_svd:\n",
    "                    #     # 使用SVD压缩层\n",
    "                    #     x_edge = self.svd_layers[layer_idx].forward_no_cache(x_edge, attn_weights_edge)\n",
    "                    # else:\n",
    "                    #     # 使用原始层\n",
    "                    #     _, x_edge = self.edge.forward_no_cache(x_edge, layer_idx, attn_weights_edge)\n",
    "                    _, x_edge = self.edge.forward_no_cache(x_edge,layer_idx,attn_weights_edge)\n",
    "                    \n",
    "                    x = x_edge\n",
    "                    \n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # 最终处理\n",
    "                x = self.ln_f(x)\n",
    "                logits = self.lm_head(x)  # [1, current_seq_len, vocab_size]\n",
    "                \n",
    "                # 只使用最后一个位置的logits进行采样\n",
    "                next_token_logits = logits[0, -1, :]  # [vocab_size]\n",
    "                \n",
    "                # 采样下一个token\n",
    "                if do_sample:\n",
    "                    # 应用temperature\n",
    "                    next_token_logits = next_token_logits / temperature\n",
    "                    \n",
    "                    # Top-p采样\n",
    "                    if top_p < 1.0:\n",
    "                        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                        \n",
    "                        # 移除累积概率超过top_p的token\n",
    "                        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                        if len(sorted_indices_to_remove) > 1:\n",
    "                            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "                        sorted_indices_to_remove[0] = False\n",
    "                        \n",
    "                        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                        next_token_logits[indices_to_remove] = float('-inf')\n",
    "                    \n",
    "                    # 从分布中采样\n",
    "                    probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                    next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "                else:\n",
    "                    # 贪心解码\n",
    "                    next_token_id = torch.argmax(next_token_logits, dim=-1).item()\n",
    "                \n",
    "                outputs.append(next_token_id)\n",
    "                \n",
    "                # 调试信息：显示生成的token\n",
    "                if step < 10:\n",
    "                    token_text = self.tokenizer.decode([next_token_id])\n",
    "                    print(f\"  Step {step}: token_id={next_token_id}, token='{token_text}'\")\n",
    "                \n",
    "                # 检查是否遇到结束token\n",
    "                if next_token_id == self.tokenizer.eos_token_id:\n",
    "                    print(\"遇到结束token，停止生成\")\n",
    "                    break\n",
    "        \n",
    "        # 生成完成的处理代码保持不变...\n",
    "        generated_text = self.tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # 输出统计信息\n",
    "        total_time = cloud_time + edge_time + transfer_time\n",
    "        generated_tokens = len(outputs) - len(input_ids)\n",
    "        \n",
    "        print(f\"\\n生成完成!\")\n",
    "        print(f\"总时间: {total_time:.3f}s\")\n",
    "        if total_time > 0:\n",
    "            print(f\"云侧时间: {cloud_time:.3f}s ({cloud_time/total_time*100:.1f}%)\")\n",
    "            print(f\"边侧时间: {edge_time:.3f}s ({edge_time/total_time*100:.1f}%)\")\n",
    "            print(f\"传输时间: {transfer_time:.3f}s ({transfer_time/total_time*100:.1f}%)\")\n",
    "        print(f\"生成的token数: {generated_tokens}\")\n",
    "        if generated_tokens > 0:\n",
    "            print(f\"平均每token时间: {total_time/generated_tokens:.3f}s\")\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def forward_with_cache(self, input_ids, use_cache=True):\n",
    "        \"\"\"\n",
    "        带缓存的前向传播（用于生成时的优化）\n",
    "        注意：这个方法暂时未实现，因为要求忽略缓存策略\n",
    "        \"\"\"\n",
    "        return self.forward(input_ids)\n",
    "    \n",
    "    def reset_cache(self):\n",
    "        \"\"\"重置所有缓存\"\"\"\n",
    "        # 由于我们忽略缓存策略，这个方法为空\n",
    "        pass\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"获取模型信息\"\"\"\n",
    "        return {\n",
    "            'num_layers': self.num_layers,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'cloud_device': self.device_cloud,\n",
    "            'edge_device': self.device_edge,\n",
    "            'model_name': 'GPT-J Cloud-Edge Collaborator',\n",
    "            'use_svd': self.use_svd,\n",
    "            'svd_enabled_layers': self.num_layers if self.use_svd else 0\n",
    "        }\n",
    "\n",
    "    def get_compression_info(self):\n",
    "        \"\"\"获取压缩信息\"\"\"\n",
    "        if not self.use_svd:\n",
    "            return {\"compressed\": False}\n",
    "        \n",
    "        compression_info = {\n",
    "            \"compressed\": True,\n",
    "            \"total_layers\": self.num_layers,\n",
    "            \"layer_details\": []\n",
    "        }\n",
    "        \n",
    "        for i, layer in enumerate(self.edge.layers):\n",
    "            layer_info = {\n",
    "                \"layer_idx\": i,\n",
    "                # \"reduce_rate\": layer.reduce_rate,\n",
    "                # 可以添加更多SVD相关信息\n",
    "            }\n",
    "            compression_info[\"layer_details\"].append(layer_info)\n",
    "        \n",
    "        return compression_info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "def load_and_tokenize_dataset(cache_dir: str='./minipile_cache', tokenizer=None, batch_size: int = 1):\n",
    "    \"\"\"\n",
    "    Loads and tokenizes the MiniPile dataset.\n",
    "\n",
    "    Args:\n",
    "        cache_dir: Directory where MiniPile is cached/downloaded.\n",
    "        tokenizer: Tokenizer for tokenizing the dataset.\n",
    "        batch_size: Batch size for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        A DataLoader for the tokenized dataset.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    ds = load_dataset(\"JeanKaddour/minipile\", split=\"validation\", cache_dir=cache_dir)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(examples['text'], padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    tokenized = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    # Group the dataset into blocks of block_size (use consistent max_length)\n",
    "    block_size = 512  # Use the same as tokenization max_length\n",
    "    def group_texts(examples):\n",
    "        all_ids = sum(examples[\"input_ids\"], [])\n",
    "        total_len = (len(all_ids) // block_size) * block_size\n",
    "        blocks = [all_ids[i:i + block_size] for i in range(0, total_len, block_size)]\n",
    "        return {\"input_ids\": blocks}\n",
    "\n",
    "    lm_dataset = tokenized.map(group_texts, batched=True, remove_columns=[\"attention_mask\"])\n",
    "\n",
    "    # DataLoader setup\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    dataloader = DataLoader(lm_dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch import nn\n",
    "svd_layers={}\n",
    "def evaluate_minipile_gptj(model, batch_size: int = 1, cache_dir: str = \"./minipile_cache\", Dataloader=None) -> dict:\n",
    "   \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "   \n",
    "    tokenizer = model.tokenizer  # already initialized in the pipeline\n",
    "    dataloader = None\n",
    "    if Dataloader is None:\n",
    "        dataloader = load_and_tokenize_dataset(cache_dir, tokenizer, batch_size)\n",
    "    else:\n",
    "        dataloader = Dataloader\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=-100)\n",
    "\n",
    "    # Evaluation loop\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    avg_loss=0\n",
    "    perplexity=0\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            # 拿到完整的 input_ids, attention_mask, 和已经被 collator 设好 -100 的 labels\n",
    "            input_ids    = batch['input_ids'].to(device)       # [B, T]\n",
    "            attention_mask = batch['attention_mask'].to(device)# [B, T]\n",
    "            labels       = batch['labels'].to(device)          # [B, T], pad 已经是 -100\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids)\n",
    "                logits  = outputs                     # [B, T, V]\n",
    "\n",
    "\n",
    "            # 手动 shift：logits 丢掉最后一位，labels 丢掉第一位\n",
    "            shift_logits = logits[:, :-1, :].contiguous()    # [B, T-1, V]\n",
    "            shift_labels = labels[:, 1:].contiguous()        # [B, T-1]\n",
    "\n",
    "            # shift_logits=logits\n",
    "            # labels=labels\n",
    "\n",
    "            # 计算交叉熵 loss，ignore_index=-100 会跳过所有 pad 位置\n",
    "            loss = criterion(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),  # [(B*(T-1)), V]\n",
    "                shift_labels.view(-1)                          # [(B*(T-1))]\n",
    "            )\n",
    "            \n",
    "            # Debug: 打印loss信息\n",
    "            if batch_idx < 3:\n",
    "                print(f\"Batch {batch_idx} loss: {loss.item():.4f}\")\n",
    "                \n",
    "            total_loss   += loss.item()\n",
    "            total_batches+= 1\n",
    "\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / total_batches\n",
    "        perplexity = math.exp(avg_loss)\n",
    "\n",
    "    return {\"avg_loss\": avg_loss, \"perplexity\": perplexity}\n",
    "\n",
    "def load_svd_layer(layer_idx,reduce_rate):\n",
    "    # print(f\"./GPTJ_SVD_DATA/gptj_svd_layer{layer_idx}_reduce_rate{reduce_rate}_svd.pth\")\n",
    "    gc.collect()\n",
    "    if((layer_idx,reduce_rate) in svd_layers):\n",
    "        return svd_layers[(layer_idx,reduce_rate)]\n",
    "    if os.path.exists(f\"./GPTJ_SVD_DATA/gptj_svd_layer{layer_idx}_reduce_rate{reduce_rate}_origin.pth\"):\n",
    "        newlayer=torch.load(f\"./GPTJ_SVD_DATA/gptj_svd_layer{layer_idx}_reduce_rate{reduce_rate}_origin.pth\",weights_only=False,map_location='cpu')\n",
    "        if hasattr(newlayer,\"original_layer\"):\n",
    "            del newlayer.original_layer\n",
    "        newlayer=newlayer.half()\n",
    "        svd_layers[(layer_idx,reduce_rate)]=newlayer\n",
    "        return newlayer\n",
    "    elif os.path.exists(f\"./GPTJ_SVD_DATA/gptj_svd_layer{layer_idx}_reduce_rate{reduce_rate}_svd.pth\"):\n",
    "        newlayer=torch.load(f\"./GPTJ_SVD_DATA/gptj_svd_layer{layer_idx}_reduce_rate{reduce_rate}_svd.pth\",weights_only=False,map_location='cpu')\n",
    "        if hasattr(newlayer,\"original_layer\"):\n",
    "            del newlayer.original_layer\n",
    "        newlayer=newlayer.half()\n",
    "        svd_layers[(layer_idx,reduce_rate)]=newlayer\n",
    "        return newlayer\n",
    "    else :\n",
    "        return None\n",
    "\n",
    "def get_model(model:GPTJCloudEdgeCollaborator,reduce_list:list):\n",
    "    try:\n",
    "        model.edge.clear()\n",
    "        device=model.device_cloud\n",
    "        layer_idx=0\n",
    "        for rate in reduce_list:\n",
    "            newlayer=load_svd_layer(layer_idx=layer_idx,reduce_rate=rate)\n",
    "            model.edge.add_layer(newlayer,device)\n",
    "            layer_idx=layer_idx+1\n",
    "    except Exception as e:\n",
    "        print(\"Exceptions in get_model:\")\n",
    "        print(e)\n",
    "        # traceback.print_exc()\n",
    "    # model.edge.clear()\n",
    "\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 方法1：创建普通模型后手动应用压缩\n",
    "    model=GPTJCloudEdgeCollaborator(device_cloud=f'cuda:{0}')\n",
    "    \n",
    "    # loss_spi=get_loss(collaboration,batch_input,batch_output)\n",
    "    \n",
    "    # 定义每层的压缩率（28层，每层压缩率不同）\\\n",
    "    #alpha=0.9\n",
    "    # reduce_rates=[0.1,0.2,0.1,0.1,0.3,0.1,0.4,0.5,0.1,0.2,0.5,0.2,0.1,0.3,0.5,0.1,0.1,0.2,0.4,0.5,0.1,0.1,0.3,0.1,0.1,0.4,0.1,0.2]\n",
    "    reduce_rates=[0.0 for _ in range(28)]\n",
    "    get_model(model,reduce_rates)\n",
    "    # 应用SVD压缩\n",
    "    # model.apply_svd_compression(reduce_rates, svd_device='cuda:0')\n",
    "    \n",
    "\n",
    "    \n",
    "    # 生成文本测试\n",
    "    prompt = \"The future of artificial intelligence is\"\n",
    "    generated_text = model.generate(\n",
    "        prompt=prompt,\n",
    "        max_length=20,\n",
    "        temperature=0.7,\n",
    "        top_p=0.8,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    print(f\"压缩信息: {model.get_compression_info()}\")\n",
    "    print(f\"生成结果: {generated_text}\")\n",
    "\n",
    "    dataloader=load_and_tokenize_dataset(tokenizer=model.tokenizer)\n",
    "    # loss=evaluate_minipile_gptj(model=model,Dataloader=dataloader)\n",
    "    # print(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea55564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 1/500 [00:00<02:50,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 loss: 2.5703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 2/500 [00:00<02:54,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 loss: 2.7285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 3/500 [00:00<02:41,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 loss: 4.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [02:15<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_loss': 3.4496484375, 'perplexity': 31.489319898513042}\n",
      "flops_edge: 7323779100.0\n",
      "flops_cloud: 1879908380.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reduce_rates=[0.1,0.2,0.1,0.5,0.5,0.4,0.4,0.5,0.4,0.6,0.5,0.7,0.5,0.4,0.5,0.5,0.4,0.2,0.4,0.7,0.4,0.1,0.3,0.7,0.4,0.4,0.6,0.2]\n",
    "# reduce_rates=[0.0 for _ in range(28)]\n",
    "get_model(model,reduce_rates)\n",
    "loss=evaluate_minipile_gptj(model=model,Dataloader=dataloader)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "datafolder=\"./GPTJ_SVD_DATA\"\n",
    "with open(f\"{datafolder}/cloud_flops_dict.pickle\",\"rb\") as f:\n",
    "    cloud_flops=pickle.load(f)\n",
    "def count_flops(collaboration_model:GPTJCloudEdgeCollaborator):\n",
    "    cloud=0\n",
    "    edge=0\n",
    "    cnt=0\n",
    "    for layer in collaboration_model.edge.layers:\n",
    "        if hasattr(layer,'flops'):\n",
    "            edge+=layer.flops\n",
    "            # split+=0\n",
    "        else:\n",
    "            edge+=0\n",
    "        cnt=cnt+1\n",
    "    cnt=0\n",
    "    for layer,flops in cloud_flops.items():\n",
    "        cloud+=flops\n",
    "    return cloud,edge\n",
    "cloud_flops,edgeflops=count_flops(model)\n",
    "# split=count_flops(model)\n",
    "# print(\"split_edge:\",split)\n",
    "\n",
    "\n",
    "# first_batch=0\n",
    "\n",
    "# for batch_idx,batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "#     first_batch=batch\n",
    "#     if batch_idx>=1:\n",
    "#         break\n",
    "# first_batch = next(iter(dataloader))\n",
    "#     # 拿到完整的 input_ids, attention_mask, 和已经被 collator 设好 -100 的 labels\n",
    "# input_ids    = first_batch['input_ids'].to('cuda:0')       \n",
    "#         # [B, T], pad 已经是 -100\n",
    "\n",
    "#     # input=torch.randint(1,100)\n",
    "# model.forward(input_ids=input_ids,flops_eval=True)\n",
    "print(\"flops_edge:\",edgeflops)\n",
    "print(\"flops_cloud:\",cloud_flops)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
