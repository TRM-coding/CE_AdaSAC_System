import torch
import torch.nn as nn
import time
import psutil
import os
import tracemalloc
from transformers import AutoTokenizer
from modelscope.utils.hub import snapshot_download
from detection.Loader.mymodel_file.gptJ_cloud import gptJ_cloud
from detection.Loader.mymodel_file.gptJ_edge import gptJ_edge
from detection.SVD_model import SVDED_GPTJ_EDGE_Layer

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§ç±»ï¼Œç”¨äºè®°å½•CPUæ—¶é—´å’Œå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    def __init__(self):
        self.process = psutil.Process(os.getpid())
        self.reset_stats()
        
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡æ•°æ®"""
        self.cloud_gpu_times = []
        self.edge_cpu_times = []
        self.network_times = []
        self.memory_snapshots = []
        self.token_count = 0
        self.layer_calls = 0
        
        # è¯¦ç»†çš„è®¡æ—¶ç»Ÿè®¡
        self.cloud_total_time = 0.0
        self.edge_total_time = 0.0
        self.network_total_time = 0.0
        
        # å†…å­˜ç»Ÿè®¡
        self.initial_memory = self.get_memory_mb()
        self.peak_memory = self.initial_memory
        
    def get_memory_mb(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        return self.process.memory_info().rss / 1024 / 1024
    
    def get_cpu_percent(self):
        """è·å–CPUä½¿ç”¨ç‡"""
        return self.process.cpu_percent()
    
    def start_memory_tracking(self):
        """å¼€å§‹å†…å­˜è·Ÿè¸ª"""
        tracemalloc.start()
        
    def stop_memory_tracking(self):
        """åœæ­¢å†…å­˜è·Ÿè¸ªå¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯"""
        if tracemalloc.is_tracing():
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            return current / 1024 / 1024, peak / 1024 / 1024  # è½¬æ¢ä¸ºMB
        return 0, 0
    
    def record_cloud_time(self, time_taken):
        """è®°å½•äº‘ç«¯GPUæ—¶é—´"""
        self.cloud_gpu_times.append(time_taken)
        self.cloud_total_time += time_taken
        
    def record_edge_time(self, time_taken):
        """è®°å½•è¾¹ç¼˜CPUæ—¶é—´"""
        self.edge_cpu_times.append(time_taken)
        self.edge_total_time += time_taken
        
    def record_network_time(self, time_taken):
        """è®°å½•ç½‘ç»œä¼ è¾“æ—¶é—´"""
        self.network_times.append(time_taken)
        self.network_total_time += time_taken
        
    def record_memory_snapshot(self, phase=""):
        """è®°å½•å†…å­˜å¿«ç…§"""
        current_memory = self.get_memory_mb()
        self.memory_snapshots.append({
            'phase': phase,
            'memory_mb': current_memory,
            'timestamp': time.time()
        })
        self.peak_memory = max(self.peak_memory, current_memory)
        
    def increment_counters(self):
        """å¢åŠ è®¡æ•°å™¨"""
        self.layer_calls += 1
        
    def increment_token_count(self):
        """å¢åŠ tokenè®¡æ•°"""
        self.token_count += 1
        
    def print_detailed_report(self):
        """æ‰“å°è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print(f"ğŸ” è¯¦ç»†æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print(f"{'='*70}")
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   ğŸ”¢ å¤„ç†çš„Tokenæ•°é‡: {self.token_count}")
        print(f"   ğŸ”¢ æ€»å±‚è°ƒç”¨æ¬¡æ•°: {self.layer_calls}")
        print(f"   ğŸ”¢ å¹³å‡æ¯tokenå±‚è°ƒç”¨: {self.layer_calls/max(1, self.token_count):.1f}")
        
        # æ—¶é—´ç»Ÿè®¡
        print(f"\nâ±ï¸  æ—¶é—´ç»Ÿè®¡ (æ€»è®¡):")
        print(f"   â˜ï¸  GPUäº‘ç«¯æ€»æ—¶é—´: {self.cloud_total_time:.4f}s")
        print(f"   ğŸ–¥ï¸  CPUè¾¹ç¼˜æ€»æ—¶é—´: {self.edge_total_time:.4f}s")
        print(f"   ğŸŒ ç½‘ç»œä¼ è¾“æ€»æ—¶é—´: {self.network_total_time:.4f}s")
        print(f"   ğŸ”„ æ€»å¤„ç†æ—¶é—´: {self.cloud_total_time + self.edge_total_time + self.network_total_time:.4f}s")
        
        # å¹³å‡æ—¶é—´ç»Ÿè®¡
        if self.token_count > 0:
            print(f"\nâ±ï¸  å¹³å‡æ¯Tokenæ—¶é—´:")
            print(f"   â˜ï¸  GPUäº‘ç«¯å¹³å‡: {self.cloud_total_time/self.token_count:.4f}s")
            print(f"   ğŸ–¥ï¸  CPUè¾¹ç¼˜å¹³å‡: {self.edge_total_time/self.token_count:.4f}s")
            print(f"   ğŸŒ ç½‘ç»œä¼ è¾“å¹³å‡: {self.network_total_time/self.token_count:.4f}s")
            print(f"   ğŸ”„ æ€»å¹³å‡: {(self.cloud_total_time + self.edge_total_time + self.network_total_time)/self.token_count:.4f}s")
        
        # å†…å­˜ç»Ÿè®¡
        current_memory = self.get_memory_mb()
        memory_diff = current_memory - self.initial_memory
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨ç»Ÿè®¡:")
        print(f"   ğŸ“ˆ åˆå§‹å†…å­˜: {self.initial_memory:.2f}MB")
        print(f"   ğŸ“Š å½“å‰å†…å­˜: {current_memory:.2f}MB")
        print(f"   ğŸ“ˆ å³°å€¼å†…å­˜: {self.peak_memory:.2f}MB")
        print(f"   ğŸ“Š å†…å­˜å˜åŒ–: {memory_diff:+.2f}MB")
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = self.get_cpu_percent()
        print(f"   ğŸ”¥ å½“å‰CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")
        
        # è·å–å†…å­˜è·Ÿè¸ªä¿¡æ¯
        if hasattr(self, '_tracemalloc_peak'):
            print(f"   ğŸ” å†…å­˜è·Ÿè¸ªå³°å€¼: {self._tracemalloc_peak:.2f}MB")
        
        # æ—¶é—´åˆ†å¸ƒåˆ†æ
        if len(self.cloud_gpu_times) > 0:
            print(f"\nğŸ“ˆ GPUæ—¶é—´åˆ†å¸ƒ:")
            print(f"   æœ€å°: {min(self.cloud_gpu_times):.4f}s")
            print(f"   æœ€å¤§: {max(self.cloud_gpu_times):.4f}s")
            print(f"   å¹³å‡: {sum(self.cloud_gpu_times)/len(self.cloud_gpu_times):.4f}s")
            
        if len(self.edge_cpu_times) > 0:
            print(f"\nğŸ“ˆ CPUæ—¶é—´åˆ†å¸ƒ:")
            print(f"   æœ€å°: {min(self.edge_cpu_times):.4f}s")
            print(f"   æœ€å¤§: {max(self.edge_cpu_times):.4f}s")
            print(f"   å¹³å‡: {sum(self.edge_cpu_times)/len(self.edge_cpu_times):.4f}s")
        
        # æ€§èƒ½æ¯”è¾ƒ
        if self.cloud_total_time > 0 and self.edge_total_time > 0:
            ratio = self.edge_total_time / self.cloud_total_time
            print(f"\nğŸ” æ€§èƒ½æ¯”è¾ƒ:")
            print(f"   CPU/GPUæ—¶é—´æ¯”: {ratio:.2f}x")
            if ratio > 1:
                print(f"   ğŸ’¡ CPUæ¯”GPUæ…¢ {ratio:.1f} å€")
            else:
                print(f"   ğŸ’¡ CPUæ¯”GPUå¿« {1/ratio:.1f} å€")
        
        print(f"{'='*70}")
        
    def print_memory_timeline(self):
        """æ‰“å°å†…å­˜ä½¿ç”¨æ—¶é—´çº¿"""
        if len(self.memory_snapshots) > 0:
            print(f"\nğŸ“Š å†…å­˜ä½¿ç”¨æ—¶é—´çº¿:")
            for i, snapshot in enumerate(self.memory_snapshots):
                print(f"   {i+1}. {snapshot['phase']}: {snapshot['memory_mb']:.2f}MB")
                
    def get_summary_stats(self):
        """è¿”å›æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'cloud_total_time': self.cloud_total_time,
            'edge_total_time': self.edge_total_time,
            'network_total_time': self.network_total_time,
            'token_count': self.token_count,
            'layer_calls': self.layer_calls,
            'memory_usage_mb': self.get_memory_mb(),
            'memory_peak_mb': self.peak_memory,
            'memory_diff_mb': self.get_memory_mb() - self.initial_memory
        }

class SVD_GPTJ_Edge_Model(nn.Module):
    """åŒ…å«æ‰€æœ‰SVDå±‚çš„å®Œæ•´edgeæ¨¡å‹ï¼Œå…¼å®¹åŸå§‹edgeæ¨¡å‹æ¥å£"""
    def __init__(self, original_edge, svd_reduce_rate, device='cpu', svd_device='cpu'):
        super().__init__()
        self.device = device
        self.svd_device = svd_device
        self.num_layers = original_edge.num_layers
        self.max_ctx = original_edge.max_ctx
        self.v_cache = [None] * self.num_layers
        
        print(f"ğŸ”„ å¼€å§‹SVDåˆ†è§£å¤„ç†ï¼Œå‹ç¼©ç‡: {svd_reduce_rate}")
        print(f"ğŸ“Š æ€»å…±éœ€è¦å¤„ç† {self.num_layers} å±‚...")
        print(f"âš¡ SVDåˆ†è§£è®¾å¤‡: {svd_device}, è¿è¡Œè®¾å¤‡: {device}")
        
        # ç”¨SVDå‹ç¼©çš„å±‚æ›¿æ¢åŸå§‹edgeå±‚
        self.svd_layers = nn.ModuleList()
        for i in range(self.num_layers):
            print(f"  å¤„ç†ç¬¬ {i+1}/{self.num_layers} å±‚: ", end="")
            original_edge_layer = original_edge.layers[i]
            if(i>=4):
                self.svd_layers.append(original_edge_layer)
                continue
            if(i%2):
                # å¥‡æ•°å±‚è·³è¿‡å‹ç¼©
                svd_layer = SVDED_GPTJ_EDGE_Layer(
                    gptj_edge_layer=original_edge_layer,
                    reduce_rate=svd_reduce_rate,
                    device=device,
                    svd_device=svd_device
                )
                print("è·³è¿‡å‹ç¼© (å¥‡æ•°å±‚)")
                self.svd_layers.append(svd_layer)
            else:
                # å¶æ•°å±‚è¿›è¡ŒSVDå‹ç¼©
                print(f"æ­£åœ¨è¿›è¡ŒSVDåˆ†è§£ (å‹ç¼©ç‡: {svd_reduce_rate})...")
                
                svd_start_time = time.time()
                svd_layer = SVDED_GPTJ_EDGE_Layer(
                    gptj_edge_layer=original_edge_layer,
                    reduce_rate=svd_reduce_rate,
                    device=device,
                    svd_device=svd_device
                )
                svd_end_time = time.time()
                print(f"    âœ… ç¬¬ {i+1} å±‚SVDåˆ†è§£å®Œæˆ (è€—æ—¶: {svd_end_time - svd_start_time:.2f}ç§’)")
                self.svd_layers.append(svd_layer)
        
        print(f"ğŸ‰ æ‰€æœ‰å±‚çš„SVDåˆ†è§£å¤„ç†å®Œæˆï¼")
    
    def forward_cache(self, x, layer_idx, attn_weights):
        """
        å…¼å®¹åŸå§‹edgeæ¨¡å‹çš„forward_cacheæ¥å£
        Args:
            x: è¾“å…¥tensor
            layer_idx: å±‚ç´¢å¼•
            attn_weights: æ³¨æ„åŠ›æƒé‡
        Returns:
            tuple: (v_cache, output_x) - ä¸åŸå§‹edgeæ¨¡å‹ç›¸åŒçš„è¿”å›æ ¼å¼
        """
        # ä½¿ç”¨SVDå‹ç¼©çš„å±‚è¿›è¡Œå‰å‘ä¼ æ’­
        # tim1=time.time()
        self.v_cache[layer_idx], output_x = self.svd_layers[layer_idx].forward_cache(
            x, self.v_cache[layer_idx], attn_weights
        )
        # tim2=time.time()
        # print(f"layer_{layer_idx}_forward_time:",tim2-tim1)
        
        # åº”ç”¨sliding windowåˆ°ç¼“å­˜
        if self.v_cache[layer_idx] is not None and self.v_cache[layer_idx].size(1) > self.max_ctx:
            self.v_cache[layer_idx] = self.v_cache[layer_idx][:, -self.max_ctx:, :]
        # tim3=time.time()
        # print(f"layer_{layer_idx}_memory_time:",tim3-tim2)

        return self.v_cache[layer_idx], output_x

class GPTJPipeline:
    def __init__(self, model_name='AI-ModelScope/gpt-j-6b', device_cloud='cuda:0', device_edge='cpu', svd_reduce_rate=0.5, use_compile=True):
        print(f"ğŸš€ åˆå§‹åŒ–GPTJPipeline...")
        print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
        print(f"   - æ¨¡å‹: {model_name}")
        print(f"   - äº‘ç«¯è®¾å¤‡: {device_cloud}")
        print(f"   - è¾¹ç¼˜è®¾å¤‡: {device_edge}")
        print(f"   - SVDå‹ç¼©ç‡: {svd_reduce_rate}")
        
        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        self.performance_monitor = PerformanceMonitor()
        
        # ä½¿ç”¨ ModelScope ä¸‹è½½æ¨¡å‹
        print(f"ğŸ“¥ ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹ {model_name}...")
        model_dir = snapshot_download(
            repo_id=model_name,
            cache_dir='./gpt-j-6b'
        )
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œè·¯å¾„: {model_dir}")
        
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„åŠ è½½ tokenizer
        print(f"ğŸ”¤ åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        
        # è®¾ç½® pad_token ä¸º eos_tokenï¼ˆGPT-J æ²¡æœ‰ pad_tokenï¼‰
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print(f"â˜ï¸  åŠ è½½äº‘ç«¯æ¨¡å‹åˆ° {device_cloud}...")
        self.cloud       = gptJ_cloud(model_name=model_dir).to(device_cloud)
        print(f"ğŸ–¥ï¸  åŠ è½½è¾¹ç¼˜æ¨¡å‹åˆ°CPU...")
        # å¼ºåˆ¶ edge æ”¾åœ¨ CPU
        original_edge    = gptJ_edge (model_name=model_dir).to('cpu')
        self.embed       = self.cloud.model.transformer.wte
        self.ln_f        = self.cloud.model.transformer.ln_f
        self.lm_head     = self.cloud.model.lm_head
        self.num_layers  = len(self.cloud.q_weights)
        
        print(f"ğŸ“Š æ¨¡å‹å…±æœ‰ {self.num_layers} å±‚")
        
        # SVDå‹ç¼©å‚æ•°
        self.svd_reduce_rate = svd_reduce_rate
        self.use_compile = use_compile
        
        # åˆ›å»ºæ•´ä¸ªSVD edgeæ¨¡å‹
        print(f"ğŸ”§ åˆ›å»ºSVDè¾¹ç¼˜æ¨¡å‹...")
        # å¦‚æœæœ‰GPUï¼Œå…ˆåœ¨GPUä¸Šè¿›è¡ŒSVDåˆ†è§£ï¼Œç„¶åç§»åˆ°CPU
        svd_device = device_cloud if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ”§ SVDåˆ†è§£å°†åœ¨ {svd_device} ä¸Šè¿›è¡Œ...")
        
        self.edge = SVD_GPTJ_Edge_Model(
            original_edge=original_edge,
            svd_reduce_rate=svd_reduce_rate,
            device='cpu',  # æœ€ç»ˆè¿è¡Œåœ¨CPUä¸Š
            svd_device=svd_device  # ä½†SVDåˆ†è§£åœ¨GPUä¸Šè¿›è¡Œ
        )
        
        print(f"âœ… GPTJPipelineåˆå§‹åŒ–å®Œæˆï¼")
        print(f"ğŸ¯ å‡†å¤‡å¼€å§‹æ¨ç†ï¼ŒSVDå‹ç¼©ç‡: {self.svd_reduce_rate}")
    
    def generate(self, prompt, max_length=50, temperature=1.0, top_k=50):
        print(f"ğŸ”„ å¼€å§‹æ–‡æœ¬ç”Ÿæˆ...")
        print(f"ğŸ“ è¾“å…¥æç¤º: '{prompt}'")
        print(f"âš™ï¸  ç”Ÿæˆå‚æ•°: max_length={max_length}, temperature={temperature}, top_k={top_k}")
        
        # é‡ç½®æ€§èƒ½ç›‘æ§å™¨å¹¶å¼€å§‹è·Ÿè¸ª
        self.performance_monitor.reset_stats()
        self.performance_monitor.start_memory_tracking()
        self.performance_monitor.record_memory_snapshot("ç”Ÿæˆå¼€å§‹")
        
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')[0].tolist()
        outputs   = input_ids.copy()

        # reset caches for a fresh generation
        print(f"ğŸ—‚ï¸  æ¸…ç©ºç¼“å­˜...")
        for i in range(self.num_layers):
            self.cloud.k_cache[i] = None
            self.edge.v_cache[i] = None

        # ç»Ÿè®¡å˜é‡
        bandwidth = 10  # MB/s

        # ä¸Šä¸‹æ–‡çª—å£å¤§å°
        max_ctx = self.cloud.max_ctx

        print(f"ğŸ”¥ é¢„çƒ­é˜¶æ®µï¼šå¤„ç† {len(input_ids)} ä¸ªæç¤ºtoken...")
        self.performance_monitor.record_memory_snapshot("é¢„çƒ­é˜¶æ®µå¼€å§‹")
        
        # é¢„çƒ­ç¼“å­˜ï¼šå°† prompt ä¸­æ¯ä¸ª token èµ°ä¸€æ¬¡ forward_cache
        for pos, token_id in enumerate(input_ids):
            print(f"  å¤„ç†æç¤ºtoken {pos+1}/{len(input_ids)}")
            self.performance_monitor.increment_token_count()
            
            # clamp ä½ç½®ï¼Œé˜²æ­¢è¶Šç•Œ
            pos_clamped = pos if pos < max_ctx else max_ctx - 1
            cur_id = torch.tensor([[token_id]]).to(self.embed.weight.device)
            
            # GPT-J æ²¡æœ‰ä½ç½®embeddingï¼Œç›´æ¥ä½¿ç”¨ token embedding
            x = self.embed(cur_id)
            
            # é€å±‚å¤„ç† - äº‘ç«¯è®¡ç®—QKï¼Œè¾¹ç«¯è®¡ç®—V
            for layer_idx in range(self.num_layers):
                # äº‘ç«¯è®¡ç®—ï¼šåœ¨GPUä¸Šè®¡ç®—QKå’Œæ³¨æ„åŠ›æƒé‡ï¼Œä¿æŒKç¼“å­˜åœ¨GPU
                if hasattr(torch.cuda, 'synchronize'):
                    torch.cuda.synchronize()
                t0 = time.time()
                _, _, attn_weights = self.cloud.forward_cache(x, layer_idx)
                if hasattr(torch.cuda, 'synchronize'):
                    torch.cuda.synchronize()
                cloud_time = time.time() - t0
                self.performance_monitor.record_cloud_time(cloud_time)
                
                # ç½‘ç»œä¼ è¾“ï¼šåªä¼ è¾“æ³¨æ„åŠ›æƒé‡åˆ°è¾¹ç«¯
                attn_cpu = attn_weights.to('cpu')
                elements = attn_cpu.numel() * attn_cpu.element_size()  # B
                net_time = elements / bandwidth / 1024 / 1024  # s
                self.performance_monitor.record_network_time(net_time)
                
                # è¾¹ç«¯è®¡ç®—ï¼šåœ¨CPUä¸Šè®¡ç®—Vå’Œè¾“å‡ºï¼Œä¿æŒVç¼“å­˜åœ¨CPU
                x_cpu = x.to('cpu')
                t1 = time.time()
                _, x_cpu = self.edge.forward_cache(x_cpu, layer_idx, attn_cpu)
                edge_time = time.time() - t1
                print(f"edge_time_{layer_idx}:",edge_time)
                self.performance_monitor.record_edge_time(edge_time)
                
                # ç½‘ç»œä¼ è¾“ï¼šå°†å¤„ç†åçš„xä¼ å›äº‘ç«¯
                x = x_cpu.to(self.embed.weight.device)
                elements = x.numel() * x.element_size()  # B
                net_time = elements / bandwidth / 1024 / 1024
                self.performance_monitor.record_network_time(net_time)
                
                self.performance_monitor.increment_counters()
            
            self.performance_monitor.record_memory_snapshot(f"Token {pos+1} å¤„ç†å®Œæˆ")

        print(f"ğŸ¯ ç”Ÿæˆé˜¶æ®µï¼šå¼€å§‹ç”Ÿæˆæ–°token...")
        self.performance_monitor.record_memory_snapshot("ç”Ÿæˆé˜¶æ®µå¼€å§‹")
        
        # çœŸå®ç”Ÿæˆé˜¶æ®µ
        for token_idx in range(max_length):
            if token_idx % 5 == 0:  # æ¯5ä¸ªtokenæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                print(f"  ç”Ÿæˆè¿›åº¦: {token_idx}/{max_length}")
                self.performance_monitor.record_memory_snapshot(f"ç”ŸæˆToken {token_idx}")
                
            self.performance_monitor.increment_token_count()
            cur_id = torch.tensor([[outputs[-1]]]).to(self.embed.weight.device)
            x = self.embed(cur_id)
            
            # é€å±‚å¤„ç† - äº‘ç«¯è®¡ç®—QKï¼Œè¾¹ç«¯è®¡ç®—V
            for layer_idx in range(self.num_layers):
                # äº‘ç«¯è®¡ç®—ï¼šåœ¨GPUä¸Šè®¡ç®—QKå’Œæ³¨æ„åŠ›æƒé‡ï¼Œä¿æŒKç¼“å­˜åœ¨GPU
                if hasattr(torch.cuda, 'synchronize'):
                    torch.cuda.synchronize()
                t0 = time.time()
                _, _, attn_weights = self.cloud.forward_cache(x, layer_idx)
                if hasattr(torch.cuda, 'synchronize'):
                    torch.cuda.synchronize()
                cloud_time = time.time() - t0
                self.performance_monitor.record_cloud_time(cloud_time)

                # ç½‘ç»œä¼ è¾“ï¼šåªä¼ è¾“æ³¨æ„åŠ›æƒé‡åˆ°è¾¹ç«¯
                attn_cpu = attn_weights.to('cpu')
                elements = attn_cpu.numel() * attn_cpu.element_size()  # B
                net_time = elements / bandwidth / 1024 / 1024
                self.performance_monitor.record_network_time(net_time)
                
                # è¾¹ç«¯è®¡ç®—ï¼šåœ¨CPUä¸Šè®¡ç®—Vå’Œè¾“å‡ºï¼Œä¿æŒVç¼“å­˜åœ¨CPU
                x_cpu = x.to('cpu')
                t1 = time.time()
                _, x_cpu = self.edge.forward_cache(x_cpu, layer_idx, attn_cpu)
                edge_time = time.time() - t1
                self.performance_monitor.record_edge_time(edge_time)
                
                # ç½‘ç»œä¼ è¾“ï¼šå°†å¤„ç†åçš„xä¼ å›äº‘ç«¯
                x = x_cpu.to(self.embed.weight.device)
                elements = x.numel() * x.element_size()  # B
                net_time = elements / bandwidth / 1024 / 1024
                self.performance_monitor.record_network_time(net_time)
                
                self.performance_monitor.increment_counters()
            
            # final normalization and LM head to get logits
            x = self.ln_f(x)
            logits = self.lm_head(x)
            
            # ç”¨ top-k + æ¸©åº¦é‡‡æ ·ä»£æ›¿è´ªå¿ƒ argmax
            next_logits = logits[:, -1, :] / temperature
            topk_vals, topk_idx = torch.topk(next_logits, k=top_k, dim=-1)
            probs = torch.softmax(topk_vals, dim=-1)
            next_id = topk_idx[0, torch.multinomial(probs, num_samples=1).item()].item()
            outputs.append(next_id)
            
            if next_id == self.tokenizer.eos_token_id:
                print(f"  é‡åˆ°ç»“æŸç¬¦ï¼Œæå‰ç»“æŸç”Ÿæˆ")
                break

        self.performance_monitor.record_memory_snapshot("ç”Ÿæˆå®Œæˆ")
        
        # åœæ­¢å†…å­˜è·Ÿè¸ª
        tracemalloc_current, tracemalloc_peak = self.performance_monitor.stop_memory_tracking()
        self.performance_monitor._tracemalloc_peak = tracemalloc_peak
        
        # æ‰“å°è¯¦ç»†æ€§èƒ½æŠ¥å‘Š
        self.performance_monitor.print_detailed_report()
        self.performance_monitor.print_memory_timeline()
            
        return self.tokenizer.decode(outputs, clean_up_tokenization_spaces=True)

if __name__ == "__main__":
    # æŠŠ intra-opï¼ˆå•ä¸ªç®—å­å†…éƒ¨å¹¶è¡Œï¼‰çº¿ç¨‹æ•°è°ƒåˆ° 56
    # torch.set_num_threads(56)
    # æŠŠ inter-opï¼ˆä¸åŒç®—å­ä¹‹é—´å¹¶è¡Œï¼‰çº¿ç¨‹æ•°ä¹Ÿè°ƒå¤§
    # torch.set_num_interop_threads(56)
    print("num_threads: %d" % torch.get_num_threads())
    # torch.set_num_interop_threads(8)
    model_name = 'AI-ModelScope/gpt-j-6b'
    device_cloud = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    device_edge = 'cpu'
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"ğŸ® æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œå°†ä½¿ç”¨GPUè¿›è¡Œäº‘ç«¯è®¡ç®—")
        print(f"ğŸ”§ GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    else:
        print(f"âš ï¸  æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œäº‘ç«¯è®¡ç®—")
    
    # æµ‹è¯•ä¸åŒçš„SVDå‹ç¼©ç‡
    svd_rates = [0.8]
    
    for svd_rate in svd_rates:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª æµ‹è¯•SVDå‹ç¼©ç‡: {svd_rate}")
        print(f"{'='*60}")
        
        try:
            print(f"ğŸ”§ åˆå§‹åŒ–æ€§èƒ½ç›‘æ§...")
            pipeline = GPTJPipeline(
                model_name=model_name, 
                device_cloud=device_cloud, 
                device_edge=device_edge,
                svd_reduce_rate=svd_rate
            )
            
            prompt = "Once upon a time"
            print(f"\nğŸ’¬ æç¤ºè¯: '{prompt}'")
            
            print(f"â±ï¸  å¼€å§‹ç”Ÿæˆæ–‡æœ¬...")
            overall_start_time = time.time()
            generated_text = pipeline.generate(prompt, max_length=20)
            overall_end_time = time.time()
            
            print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡æœ¬:")
            print(f"   {generated_text}")
            print(f"â±ï¸  æ€»ç”Ÿæˆæ—¶é—´: {overall_end_time - overall_start_time:.2f}ç§’")
            
            # è·å–æ€§èƒ½æ‘˜è¦
            stats = pipeline.performance_monitor.get_summary_stats()
            print(f"\nğŸ“ˆ æ€§èƒ½æ‘˜è¦:")
            print(f"   ğŸ”¢ å¤„ç†Tokenæ•°: {stats['token_count']}")
            print(f"   â˜ï¸  GPUæ€»æ—¶é—´: {stats['cloud_total_time']:.4f}s")
            print(f"   ğŸ–¥ï¸  CPUæ€»æ—¶é—´: {stats['edge_total_time']:.4f}s")
            print(f"   ğŸŒ ç½‘ç»œæ€»æ—¶é—´: {stats['network_total_time']:.4f}s")
            print(f"   ğŸ’¾ å†…å­˜å˜åŒ–: {stats['memory_diff_mb']:+.2f}MB")
            print(f"   ğŸ“Š å³°å€¼å†…å­˜: {stats['memory_peak_mb']:.2f}MB")
            
            # ä¿å­˜æ€§èƒ½æ•°æ®åˆ°æ–‡ä»¶
            with open(f"performance_svd_{svd_rate}.txt", "w") as f:
                f.write(f"SVDå‹ç¼©ç‡: {svd_rate}\n")
                f.write(f"å¤„ç†Tokenæ•°: {stats['token_count']}\n")
                f.write(f"GPUæ€»æ—¶é—´: {stats['cloud_total_time']:.4f}s\n")
                f.write(f"CPUæ€»æ—¶é—´: {stats['edge_total_time']:.4f}s\n")
                f.write(f"ç½‘ç»œæ€»æ—¶é—´: {stats['network_total_time']:.4f}s\n")
                f.write(f"å†…å­˜å˜åŒ–: {stats['memory_diff_mb']:+.2f}MB\n")
                f.write(f"å³°å€¼å†…å­˜: {stats['memory_peak_mb']:.2f}MB\n")
                f.write(f"æ€»ç”Ÿæˆæ—¶é—´: {overall_end_time - overall_start_time:.2f}ç§’\n")
                f.write(f"ç”Ÿæˆæ–‡æœ¬: {generated_text}\n")
            
            print(f"ğŸ’¾ æ€§èƒ½æ•°æ®å·²ä¿å­˜åˆ° performance_svd_{svd_rate}.txt")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        
        print(f"{'='*60}")
        print(f"ğŸ SVDå‹ç¼©ç‡ {svd_rate} æµ‹è¯•å®Œæˆ")
        print(f"{'='*60}")
    
    print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
